[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Water and Economy Xiaojing Ni",
    "section": "",
    "text": "Georgetown University | Washington, D.C. Master of Science in Data Science and Analytics | Aug 2021 - Dec 2023\nMississippi State University | Mississippi State, MS PhD in Biological Engineering | Aug 2014 - Aug 2018"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Introduction",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Water quality time series analysis — a time series case study of Toledo Ohio, U.S.",
    "section": "",
    "text": "Figure 1: Lake Erie algae bloom outbreaks\n\n\nLake Erie’s harmful algae blooms (HABs, Figure 1) have been an annual threat to surrounding communities and businesses who worry about whether algae will: 1) pollute their drinking water; 2) harm the region’s vital tourism economy; and 3) prevent residents and visitors from recreational opportunities such as boating, swimming, and visiting shorelines. In fact, the city of Toledo Ohio, US (Figure 2) had to shut down their drinking water supply due to HABs detected in Lake Erie in the summer of 2014, which affected more than a half million residents in Toledo.\nSediment with the nutrients attached to it is considered as one of the major contributor to HABs. When sediment is eroded from the surrounding landscape and enters a stream, it can carry with it attached nutrients that can contribute to excess nutrients in the water. This can lead to algal blooms, reduced dissolved oxygen levels, and other negative impacts on aquatic ecosystems.\n\n\n\nFigure 2: Toledo, Ohio, U.S."
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Three sources of data are collected in this study, including stream quantity and quality data from United States Geological Survey (USGS), climate information from National Oceanic and Atmospheric Administration (NOAA), and stock price data from Yahoo finance."
  },
  {
    "objectID": "data_sources.html#running-code",
    "href": "data_sources.html#running-code",
    "title": "Data Sources",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Economy\n\nThe U.S. Housing Market\n\n\n                   \n\nSource\n\n\nAgriculture Gross Output\n\n\nCode\n# read data and convert it into ts\n## import data\nagg_output <- read.csv(\"./data/agg_gross_2005-2022.csv\",header = FALSE)\ncolnames(agg_output) <- c('Year','Quater','Year_Quater','[Billions of dollars] Seasonally adjusted at annual rates')\n\n# set year_quater as year quater\nagg_output$Year_Quater <- as.yearqtr(agg_output$Year_Quater)\nstr(agg_output)\n\n# extract year_quater and gross output columns\nagg_df <- data.frame(agg_output$Year_Quater,agg_output$`[Billions of dollars] Seasonally adjusted at annual rates`)\ncolnames(agg_df) <- c(\"Year_Quater\",\"rate\")\nstr(agg_df)\n\n# convert to time series\nagg_ts <- read.zoo(agg_df)\nhead(agg_ts)\n\n\n\n\nCode\n## ggplot\nagg_fig<- ggplot(agg_df, aes(x=Year_Quater)) +\n  geom_line(aes(y=rate))+\n   labs(\n    title = \"Gross Output by Agriculture, Forestry, Fishing, and Hunting\",\n    subtitle = \"From 2005-2022\",\n    x = \"Quarter\",\n    y = \"[Billions of dollars] Seasonally adjusted at annual rates\")+\n    guides(colour=guide_legend(title=\"Tech Companies\")) \n\n\nggplotly(agg_fig) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\nFigure 1: Gross Output by Agriculture, Forestry, Fishing, and Hunting\n\n\n\nThe Gross Output by Agriculture, Forestry, Fishing, and Hunting in Figure 1 shows that there was a reclining aboud 2008, which is caused by the Greate Regression in U.S. economy around 2008. The U.S.Housing Market has a reclining around 2011-2012, which caused by the Subprime mortgage crisis that contributed to the 2007–2008 financial crisis.\n\n\n\nWater\n\nStream discharge visualization\n\n\nCode\n## import and processing data\ndischarge <- read.csv(\"./data/discharge.csv\")\ndischarge <- discharge[discharge$X.!= \"#\",]\ndischarge <- discharge[3:dim(discharge)[1],]\ncolnames(discharge) <- c(\"prefix\", \"station\",\"date\",\"discharge\",\"flag\")\ndischarge = discharge[c(\"prefix\", \"station\",\"date\",\"discharge\",\"flag\")]\ndischarge$date<-as.Date(discharge$date,\"%m/%d/%y\")\ndischarge$date <- as.Date(ifelse(discharge$date > Sys.Date(), \n  format(discharge$date, \"19%y-%m-%d\"), \n  format(discharge$date)))\nhead(discharge)\ndim(discharge)\nstr(discharge)\n\n# select data after 1970\ndischarge = discharge[discharge$date >= \"1970-01-01\", ]\nhead(discharge)\ndim(discharge)\nstr(discharge)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\ndischarge$discharge <- na_ma(discharge$discharge, k = 4, weighting = \"exponential\")\n\n## sanity check\nstr(discharge)\nsum(is.na(discharge))\n\n\n\n\nCode\np <- plot_ly(discharge, type = 'scatter', mode = 'lines')%>%\n  add_trace(x = ~date, y = ~discharge, name=\"discharge\")%>%\n  layout(title='Stream discharge at Waterville, OH station',\n         xaxis = list(rangeslider = list(visible = T),\n                      rangeselector=list(\n                        buttons=list(\n                          list(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                          list(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n                          list(count=16, label=\"YTD\", step=\"year\", stepmode=\"todate\")\n                          )\n                        )))\n\np <- p %>%\n  layout(\n    xaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff',\n                 title = \"Date\"),\n    yaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff',\n                 title = \"Discharge, ft<sup>3</sup>/s\"),\n    plot_bgcolor='#e5ecf6', margin = 0.2, width = 900)\np\n\n\n\n\n\nFigure 2: Stream discharge at Waterville, OH station\n\n\n\nStream flow has clear periodicity according to Figure 2. There are some high value year, for example 2015. This is caused by the 2014–2016 El Niño event."
  },
  {
    "objectID": "data_viz.html#running-code",
    "href": "data_viz.html#running-code",
    "title": "Data Visualization",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "deep_learning.html#running-code",
    "href": "deep_learning.html#running-code",
    "title": "Deep Learning for TS",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "financial.html#running-code",
    "href": "financial.html#running-code",
    "title": "Financial Time Series Models",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\ntrend_discharge = ma(discharge_ts, order = 365, centre = T)\nplot(discharge_ts, main=\"Stream flow discharge at Waterville, OH station\",ylab=expression(paste(\"Discharge, \",ft^3,\"/s\",sep=\"\")))\nlines(trend_discharge, col = 2, lwd = 4)\nlegend(14000, 115000, legend=c(\"Observation\", \"Trend\"),\n       col=c(\"black\", \"red\"), pch = c(1, NA),lty=1)\n\n\n\n\n\nFigure 1: Stream flow discharge annual trend\n\n\n\n\nFigure 1 shows the daily stream flow trend obtained by moving average method with order of 365 (annual). There is no clear trend based on this figure.\n\n\nCode\n## detrend\nx = seq(0,20000,20000/5)\ny = seq(1970,2020,10)\ndetrend_discharge = discharge_ts/trend_discharge\nplot(as.ts(detrend_discharge),xaxt='n',main=\"Detrend Discharge\",ylab=expression(paste(\"Discharge, \",ft^3,\"/s\",sep=\"\")))\naxis(side = 1, at = x, labels = y)\n\n\n\n\n\nFigure 2: Stream flow discharge detrended series\n\n\n\n\n\n\n\n\n\nCode\n## Average the Seasonality\nm_discharge = t(matrix(data = detrend_discharge, nrow = 365))\n\nseasonal_discharge = colMeans(m_discharge, na.rm = T)\nplot(as.ts(rep(seasonal_discharge,50)),main=\"Stream Flow Discharge Average Seasonality\",ylab=expression(paste(\"Discharge, \",ft^3,\"/s\",sep=\"\")),xaxt='n')\naxis(side = 1, at = x,labels = y)\n\n\n\n\n\nFigure 3: Stream flow discharge average seasonality\n\n\n\n\nFigure 2 shows the detrened series. There is clear seasonality left in the series. Both multiplicative and additive have non-stationary residuals. Thus, more analysis need to be done other then detrending.\n\n\n\n\n\n\nCode\n### can get monthly data\n# Get mean value for each month\nmean_data <- discharge_df %>% \n  mutate(month = month(discharge.date), year = year(discharge.date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(discharge.discharge))\n\ndischarge_month<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n  \n\n\np <-gglagplot(discharge_ts, do.lines=FALSE)+\nggtitle(paste(\"Lag plots for daily stream flow\"))\nprint(p)\n\np1 <-gglagplot(discharge_month, do.lines=FALSE)+\nggtitle(paste(\"Lag plots for monthly stream flow\"))\nprint(p1)\n\n\n\n\n\n(a) Daily\n\n\n\n\n\n\n\n(b) Monthly\n\n\n\nFigure 4: Lag plots for stream flow\n\n\nFigure 4 shows the lag plot for daily and monthly stream flow discharge. For daily stream flow discharge (Figure 4 (a)), it shows the most related time is lag1, which makes sense that the next day stream flow discharge highly depends on the previous day. For monthly stream flow discharge (Figure 4 (b)), it seems there is no clear pattern.\n\n\nCode\ndischarge_month2022 <- window(discharge_month,start=1980,end=2022)\ndecompose_discharge <- decompose(as.ts(discharge_month2022), \"additive\")\nplot(decompose_discharge)\n\n\n\n\n\nFigure 5: Decomposition of stream flow\n\n\n\n\nFigure 5 is the decomposition for monthly stream flow discharge. we can see the residual is more stationary compared to detrended daily stream flow discharge in Figure 2. Next, we will use ACF and PACF plot to see if the residual is stationary.\n\n\n\n\n\nCode\n## ACF\n\nggAcf(discharge_month,400)\nggPacf(discharge_month,400)\n\n\n\n\n\n\n\n\n(a) ACF plot for monthly streamflow 1970-2022\n\n\n\n\n\n\n\n(b) PACF plot for monthly streamflow 1970-2022\n\n\n\n\nFigure 6: ACF and PACF plots\n\n\n\n\n\nCode\n################ ADF Test #############\ntseries::adf.test(discharge_month2022)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  discharge_month2022\nDickey-Fuller = -10.302, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\nFigure 6 shows that the monthly stream flow discharge is not stationary. Augmented Dickey-Fuller Test confirms this. Next, we will use detrened series to see if the residual is stationary. Figure 7 shows the detrended series still have unsolved pattern left. Other analysis need to be done further.\n\n\nCode\n## ACF\ndetrend <- discharge_month2022/decompose_discharge$trend\nggAcf(detrend,400)\nggPacf(detrend,400)\n\n################ ADF Test #############\ntseries::adf.test(window(detrend, 1981,2020))\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  window(detrend, 1981, 2020)\nDickey-Fuller = -14.646, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n(a) ACF plot for detrended monthly streamflow 1970-2022\n\n\n\n\n\n\n\n\n\n(b) PACF plot for detrended monthly streamflow 1970-2022\n\n\n\n\nFigure 7: ACF and PACF plots\n\n\n\n\n\n\n\n\nCode\n## daily plot\ndischarge_daily <- ts(discharge$discharge,star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\np <- autoplot(discharge_daily, series=\"Observation\") +\n  autolayer(ma(discharge_daily, 30), series=\"30-MA (Monthly)\") + \n  autolayer(ma(discharge_daily, 365), series=\"365-MA (Annual)\") + \n  autolayer(ma(discharge_daily, 1460), series=\"1460-MA (Four-year)\") +\n  xlab(\"Year\") + ylab(expression(paste(\"Discharge, \",ft^3,\"/s\",sep=\"\"))) +\n  ggtitle(\"daily stream flow\") +\n  scale_colour_manual(values=c(\"Observation\"=\"grey\",\"1460-MA (Four-year)\"=\"green\",\"365-MA (Annual)\"=\"red\", \"30-MA (Monthly)\"=\"blue\"),\n                      breaks=c(\"Observation\",\"1460-MA (Four-year)\",\"365-MA (Annual)\", \"30-MA (Monthly)\"))\np\n\n\n\n\n\nFigure 8: Moving average smoothing for daily stream flow\n\n\n\n\n\n\nCode\n### can get monthly data\n# Get mean value for each month\nmean_data <- discharge %>% \n  mutate(month = month(date), year = year(date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(as.numeric(as.vector(discharge)),na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nCode\ndischarge_monthly<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\np <- autoplot(discharge_monthly, series=\"Observation\") +\n  autolayer(ma(discharge_monthly, 12), series=\"12-MA (Annual)\") + \n  autolayer(ma(discharge_monthly, 48), series=\"48-MA (Four-year)\") +\n  xlab(\"Year\") + ylab(expression(paste(\"Discharge, \",ft^3,\"/s\",sep=\"\"))) +\n  ggtitle(\"daily stream flow\") +\n  scale_colour_manual(values=c(\"Observation\"=\"grey\",\"48-MA (Four-year)\"=\"green\",\"12-MA (Annual)\"=\"red\"),\n                      breaks=c(\"Observation\",\"48-MA (Four-year)\",\"12-MA (Annual)\"))\np\n\n\n\n\n\nFigure 9: Moving average smoothing for monthly stream flow\n\n\n\n\nThe annual moving average shows periodicity. Thus, a larger window, four-year window, is used to explore the pattern of the streamflow. we can see the four-year moving average is more stationary compared to others."
  },
  {
    "objectID": "eda.html#running-code",
    "href": "eda.html#running-code",
    "title": "Exploratory Data Analysis",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "The main objective of this study is to investigate the water quality index, such as stream flow discharge, turbidity, and sediment, as time series data. More specifically shown in Figure 1, various time series models were used to simulate stream flow discharge, i.e. ARMA, ARIMA, SARIMA, VAR, turbidity, i.e. SARIMAX, and sediment, i.e. VAR. In addition, the climate data, such as precipitation and temperature were also used to predict the water quality index. In the “Financial time series model” session, GARCH model are used to analyze water treatment company, Xylem, stock prices."
  },
  {
    "objectID": "conclusions.html#running-code",
    "href": "conclusions.html#running-code",
    "title": "Conclusions",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "spectral_filtering.html",
    "href": "spectral_filtering.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "spectral_filtering.html#running-code",
    "href": "spectral_filtering.html#running-code",
    "title": "Spectral Analysis and Filtering",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "arma_arima_sarima.html",
    "href": "arma_arima_sarima.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In the EDA session, we explore the ACF and PACF for detrended daily stream flow, however, the Augmented Dickey-Fuller Test show that after decomposition, the randomness is still non-stationary."
  },
  {
    "objectID": "arma_arima_sarima.html#running-code",
    "href": "arma_arima_sarima.html#running-code",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "data_viz.html#bitcoin-plot-using-plotly",
    "href": "data_viz.html#bitcoin-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\nOR you can obtain a single stock price\n\n\nCode\n#bitc_ALL <- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2020-10-01\",src=\"yahoo\")\nbitc <- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2021-09-15\",src=\"yahoo\") \nhead(bitc)\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\n\nCode\nstart(bitc)\n\n\n[1] \"2021-09-15\"\n\n\nCode\nend(bitc)\n\n\n[1] \"2023-01-13\"\n\n\n\n\nCode\nbitc=data.frame(bitc)\nbitc <- data.frame(bitc,rownames(bitc))\nhead(bitc)\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\nCode\ncolnames(bitc)[7] = \"date\"\nhead(bitc)\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\nCode\nbitc$date<-as.Date(bitc$date,\"%Y-%m-%d\")\nstr(bitc)\n\n\n'data.frame':   336 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n\nCode\n## ggplot\nbitc %>%\n  ggplot()+\n  geom_line(aes(y=BTC.Adjusted,x=date),color=\"blue\")\n\n\n\n\n\nCode\n## plotly\nfig <- plot_ly(bitc, x = ~date, y = ~BTC.Adjusted, type = 'scatter', mode = 'lines')\n\nfig <- fig %>% layout(title = \"Basic line Plot\")\nfig\n\n\n\n\n\n\n\n\nCode\n#plotly\n# candlestick plot\n\ndf <- tail(bitc, 30)\n\nfigc <- df %>% plot_ly(x = ~date, type=\"candlestick\",\n          open = ~BTC.Open, close = ~BTC.Close,\n          high = ~BTC.High, low = ~BTC.Low) \nfigc <- figc %>% layout(title = \"Basic Candlestick Chart\")\n\nfigc"
  },
  {
    "objectID": "data_viz.html#climate-plot-using-plotly",
    "href": "data_viz.html#climate-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Climate plot using plotly",
    "text": "Climate plot using plotly\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\n\n[1] \"2021-09-15\"\n\n\n[1] \"2023-01-13\"\n\n\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\n'data.frame':   336 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ..."
  },
  {
    "objectID": "data_viz.html#climate",
    "href": "data_viz.html#climate",
    "title": "Data Visualization",
    "section": "Climate",
    "text": "Climate\n\nPrecipitation\n\n\nCode\n## read data and aggregate\ndf <- data.frame(matrix(ncol = 6, nrow = 0))\ncolnames(df) <- c(\"station\",\"name\",\"date\",\"prcp\",\"tmax\",\"tmin\")\nfor (i in 1:3){\n  file_name <- paste0(\"climate\",i,\".csv\")\n  df_curr <- read.csv(paste0(\"./data/\",file_name), header = T)\n  colnames(df_curr) <- c(\"station\",\"name\",\"date\",\"prcp\",\"tmax\",\"tmin\")\n  df <- rbind(df_curr,df)\n}\n\n## group by date: each day, we have one average prcp, tmax, and tmin based on all stations in Toledo\n\nclimate <- df %>%\n  group_by(date) %>%\n  summarise_at(vars(prcp,tmax,tmin),  mean, na.rm = TRUE)\n\n\nclimate$date<-as.Date(climate$date,\"%Y-%m-%d\")\n## sanity check\n# any(duplicated(climate$date))\n# min(climate$date);max(climate$date)\n\n### climate for turbidity model, 2021-11-24 to 2022-12-31\n\n# select data after 2021-11-24 and before 2022-12-31\nclimate_trub <- climate[(climate$date >= \"2021-11-24\") & (climate$date <= \"2022-12-31\"), ]\n\n## interpolate the missing data using moving average\nclimate_trub$prcp <- na_ma(climate_trub$prcp, k = 4, weighting = \"exponential\")\nclimate_trub$tmax <- na_ma(climate_trub$tmax, k = 4, weighting = \"exponential\")\nclimate_trub$tmin <- na_ma(climate_trub$tmin, k = 4, weighting = \"exponential\")\n\n## sanity check\n#str(climate_trub)\n#sum(is.na(climate_trub))\n#min(climate_trub$date);max(climate_trub$date)\n\n## prcp \nprcp_trub_df <- data.frame(climate_trub$date,climate_trub$prcp)\ncolnames(prcp_trub_df) <- c(\"Date\",\"Precipitation\")\n\n## prepare data for candlestick plot\nprcp_summary <- prcp_trub_df %>%                               \n  group_by(Date) %>% \n  summarize(min = min(Precipitation),\n            q1 = quantile(Precipitation, 0.25),\n            median = median(Precipitation),\n            mean = mean(Precipitation),\n            q3 = quantile(Precipitation, 0.75),\n            max = max(Precipitation))\n\np2 <- plot_ly(prcp_summary,\n              x = ~Date,\n              y = ~mean,\n              type = 'scatter', mode = 'lines')\np2 <- p2 %>% layout(title = \"Daily Precipitation Time Series In Study Area\",\n      xaxis = list(rangeslider = list(visible = T)),\n       yaxis = list(title = 'Average precipitation (in)'))\np2\n\n\n\n\n\nFigure 1: Average precipitation in study area\n\n\n\nFigure 1 shows the time series plot of the average precipitation (in liquid form) in the study area. The summer and fall are the season with high precipitation. However, this is not necessary indicate that winter and spring have less total precipitation, as the snow in the winter may also contribute to the total precipitation. There might be periodicity in the precipitation pattern, which will need more analysis to confirm.\n\n\nTemperature\nFigure 2 shows the daily maximum and minimum temperature in study area. The temperature has a generally increasing trend. Both maximum and minimum temperature have similar trends.\n\n\nCode\n## t\nt_trub_df <- data.frame(climate_trub$date,climate_trub$tmax, climate_trub$tmin)\n\ncolnames(t_trub_df) <- c(\"Date\",\"Tmax\",\"Tmin\")\n\n\n\np3 <- plot_ly(t_trub_df, type = 'scatter', mode = 'lines')%>%\n  add_trace(x = ~Date, y = ~Tmax, name = \"Max\")%>%\n  add_trace(x = ~Date, y = ~Tmin, name = \"Min\")%>%\n  layout(title='Max and Min Temperature for Study area',\n         xaxis = list(rangeslider = list(visible = T),\n                      rangeselector=list(\n                        buttons=list(\n                          list(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n                          list(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\")\n                        ))))\n\np3 <- p3 %>%\n  layout(\n    xaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff'),\n    yaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff',\n                 title = 'Temperature, F'),\n    plot_bgcolor='#e5ecf6', margin = 0.2, width = 900)\np3\n\n\n\n\n\nFigure 2: Maximum and minimum temperature in study area"
  },
  {
    "objectID": "data_sources.html#stream-quantity-and-quality",
    "href": "data_sources.html#stream-quantity-and-quality",
    "title": "Data Sources",
    "section": "Stream quantity and quality",
    "text": "Stream quantity and quality\nThe daily summary of stream quantity and quality are obtained from USGS at USGS 04193500 Maumee River at Waterville OH station (Figure 1). Figure 2 shows the download page. In this study, streamflow discharge from 1940 to 2022 was downloaded as shown in Figure 3.\n\n\n\nFigure 1: USGS gaging station location\n\n\n\n\n\nFigure 2: USGS data download page\n\n\n\n\n\nFigure 3: USGS stream quantity and quality data example"
  },
  {
    "objectID": "data_sources.html#climate-data",
    "href": "data_sources.html#climate-data",
    "title": "Data Sources",
    "section": "Climate data",
    "text": "Climate data\nThe daily summary of climate data, including precipitation, maximum temperature, and minimum temperature, are obtained from NOAA at Hydrologic Unit Code (HUC) 04100009 watershed where the water drained from to the 04193500 gaging station. Figure 4 shows the download page. In this study, climate information from 1940 to 2022 was downloaded as shown in Table 1.\n\n\n\nFigure 4: NOAA data download page\n\n\n\n\n\n\n\nCode\nlibrary(knitr)\nclimate <- read.csv(\"./data/climate1.csv\",header=TRUE)\nclimate <- drop_na(climate)\nkable(head(climate))\n\n\n\n\nTable 1: Cliamte data example\n\n\n\n\n\n\n\n\n\n\nSTATION\nNAME\nDATE\nPRCP\nTMAX\nTMIN\n\n\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-01\n0.26\n47\n28\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-02\n0.00\n47\n34\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-03\n0.04\n50\n31\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-04\n0.00\n46\n30\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-05\n0.92\n55\n33\n\n\nUSW00094830\nTOLEDO EXPRESS AIRPORT, OH US\n1955-01-06\n0.08\n37\n30"
  },
  {
    "objectID": "data_sources.html#economic-data",
    "href": "data_sources.html#economic-data",
    "title": "Data Sources",
    "section": "Economic data",
    "text": "Economic data\nThe quarterly economic data, including GPD by industry for agriculture, forestry, fishing, and hunting from 2005 to 2022 , maximum temperature, and minimum temperature, are obtained from BEA. Figure 5 shows the download page.\n\n\n\nFigure 5: BEA download page"
  },
  {
    "objectID": "data_sources.html#zillow-research-data",
    "href": "data_sources.html#zillow-research-data",
    "title": "Data Sources",
    "section": "Zillow research data",
    "text": "Zillow research data\nThe monthly Zillow Observed Rent Index (ZORI) for All Homes Plus Multifamily Time Series and Zillow Home Value Index (ZHVI) Single Family Homes Time Series by zipcod were downloaded from Zillow and filtered by city name of “Toledo”. Table 2 shows the data example.\n\n\nCode\nlibrary(knitr)\nzhvi <- read.csv(\"./data/Zip_zhvi_uc_sfr_tier_0.33_0.67_sm_sa_month.csv\",header=TRUE)\nzhvi <- zhvi[zhvi$City==\"Toledo\",]\nkable(head(zhvi))\n\n\n\n\nTable 2: Zillow research data example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\nX2000.01.31\nX2000.02.29\nX2000.03.31\nX2000.04.30\nX2000.05.31\nX2000.06.30\nX2000.07.31\nX2000.08.31\nX2000.09.30\nX2000.10.31\nX2000.11.30\nX2000.12.31\nX2001.01.31\nX2001.02.28\nX2001.03.31\nX2001.04.30\nX2001.05.31\nX2001.06.30\nX2001.07.31\nX2001.08.31\nX2001.09.30\nX2001.10.31\nX2001.11.30\nX2001.12.31\nX2002.01.31\nX2002.02.28\nX2002.03.31\nX2002.04.30\nX2002.05.31\nX2002.06.30\nX2002.07.31\nX2002.08.31\nX2002.09.30\nX2002.10.31\nX2002.11.30\nX2002.12.31\nX2003.01.31\nX2003.02.28\nX2003.03.31\nX2003.04.30\nX2003.05.31\nX2003.06.30\nX2003.07.31\nX2003.08.31\nX2003.09.30\nX2003.10.31\nX2003.11.30\nX2003.12.31\nX2004.01.31\nX2004.02.29\nX2004.03.31\nX2004.04.30\nX2004.05.31\nX2004.06.30\nX2004.07.31\nX2004.08.31\nX2004.09.30\nX2004.10.31\nX2004.11.30\nX2004.12.31\nX2005.01.31\nX2005.02.28\nX2005.03.31\nX2005.04.30\nX2005.05.31\nX2005.06.30\nX2005.07.31\nX2005.08.31\nX2005.09.30\nX2005.10.31\nX2005.11.30\nX2005.12.31\nX2006.01.31\nX2006.02.28\nX2006.03.31\nX2006.04.30\nX2006.05.31\nX2006.06.30\nX2006.07.31\nX2006.08.31\nX2006.09.30\nX2006.10.31\nX2006.11.30\nX2006.12.31\nX2007.01.31\nX2007.02.28\nX2007.03.31\nX2007.04.30\nX2007.05.31\nX2007.06.30\nX2007.07.31\nX2007.08.31\nX2007.09.30\nX2007.10.31\nX2007.11.30\nX2007.12.31\nX2008.01.31\nX2008.02.29\nX2008.03.31\nX2008.04.30\nX2008.05.31\nX2008.06.30\nX2008.07.31\nX2008.08.31\nX2008.09.30\nX2008.10.31\nX2008.11.30\nX2008.12.31\nX2009.01.31\nX2009.02.28\nX2009.03.31\nX2009.04.30\nX2009.05.31\nX2009.06.30\nX2009.07.31\nX2009.08.31\nX2009.09.30\nX2009.10.31\nX2009.11.30\nX2009.12.31\nX2010.01.31\nX2010.02.28\nX2010.03.31\nX2010.04.30\nX2010.05.31\nX2010.06.30\nX2010.07.31\nX2010.08.31\nX2010.09.30\nX2010.10.31\nX2010.11.30\nX2010.12.31\nX2011.01.31\nX2011.02.28\nX2011.03.31\nX2011.04.30\nX2011.05.31\nX2011.06.30\nX2011.07.31\nX2011.08.31\nX2011.09.30\nX2011.10.31\nX2011.11.30\nX2011.12.31\nX2012.01.31\nX2012.02.29\nX2012.03.31\nX2012.04.30\nX2012.05.31\nX2012.06.30\nX2012.07.31\nX2012.08.31\nX2012.09.30\nX2012.10.31\nX2012.11.30\nX2012.12.31\nX2013.01.31\nX2013.02.28\nX2013.03.31\nX2013.04.30\nX2013.05.31\nX2013.06.30\nX2013.07.31\nX2013.08.31\nX2013.09.30\nX2013.10.31\nX2013.11.30\nX2013.12.31\nX2014.01.31\nX2014.02.28\nX2014.03.31\nX2014.04.30\nX2014.05.31\nX2014.06.30\nX2014.07.31\nX2014.08.31\nX2014.09.30\nX2014.10.31\nX2014.11.30\nX2014.12.31\nX2015.01.31\nX2015.02.28\nX2015.03.31\nX2015.04.30\nX2015.05.31\nX2015.06.30\nX2015.07.31\nX2015.08.31\nX2015.09.30\nX2015.10.31\nX2015.11.30\nX2015.12.31\nX2016.01.31\nX2016.02.29\nX2016.03.31\nX2016.04.30\nX2016.05.31\nX2016.06.30\nX2016.07.31\nX2016.08.31\nX2016.09.30\nX2016.10.31\nX2016.11.30\nX2016.12.31\nX2017.01.31\nX2017.02.28\nX2017.03.31\nX2017.04.30\nX2017.05.31\nX2017.06.30\nX2017.07.31\nX2017.08.31\nX2017.09.30\nX2017.10.31\nX2017.11.30\nX2017.12.31\nX2018.01.31\nX2018.02.28\nX2018.03.31\nX2018.04.30\nX2018.05.31\nX2018.06.30\nX2018.07.31\nX2018.08.31\nX2018.09.30\nX2018.10.31\nX2018.11.30\nX2018.12.31\nX2019.01.31\nX2019.02.28\nX2019.03.31\nX2019.04.30\nX2019.05.31\nX2019.06.30\nX2019.07.31\nX2019.08.31\nX2019.09.30\nX2019.10.31\nX2019.11.30\nX2019.12.31\nX2020.01.31\nX2020.02.29\nX2020.03.31\nX2020.04.30\nX2020.05.31\nX2020.06.30\nX2020.07.31\nX2020.08.31\nX2020.09.30\nX2020.10.31\nX2020.11.30\nX2020.12.31\nX2021.01.31\nX2021.02.28\nX2021.03.31\nX2021.04.30\nX2021.05.31\nX2021.06.30\nX2021.07.31\nX2021.08.31\nX2021.09.30\nX2021.10.31\nX2021.11.30\nX2021.12.31\nX2022.01.31\nX2022.02.28\nX2022.03.31\nX2022.04.30\nX2022.05.31\nX2022.06.30\nX2022.07.31\nX2022.08.31\nX2022.09.30\nX2022.10.31\nX2022.11.30\nX2022.12.31\n\n\n\n\n1886\n76774\n1889\n43615\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n95109\n95431\n95514\n95878\n96298\n96884\n97492\n98271\n99066\n99898\n100646\n101204\n101732\n101968\n102472\n102907\n103371\n103723\n104088\n104228\n104510\n104675\n105003\n105360\n105685\n104796\n104922\n105107\n106439\n106737\n106996\n107331\n107619\n107989\n108488\n109110\n109898\n110789\n111189\n111483\n111642\n112099\n112615\n113159\n113664\n114230\n114534\n114626\n114600\n114770\n115181\n115654\n116565\n117498\n118419\n118837\n119072\n119075\n119105\n119299\n119628\n120136\n120988\n121583\n121714\n120962\n120572\n120749\n121667\n122371\n122977\n122900\n122999\n122873\n123059\n123116\n123444\n123987\n124419\n124298\n123684\n123119\n122782\n122945\n123301\n123736\n123829\n123434\n122934\n122065\n121176\n120109\n118904\n117673\n116266\n115364\n114535\n112921\n111220\n110581\n111031\n111803\n111899\n112086\n111777\n111552\n110599\n109606\n108413\n107509\n106030\n105468\n105177\n105710\n105104\n104708\n104317\n103793\n103530\n103578\n103430\n103442\n103617\n103890\n103714\n103080\n103261\n103239\n102948\n102182\n101020\n99452\n97886\n97022\n97506\n97715\n97154\n96011\n94788\n94254\n93528\n92821\n91666\n90752\n89665\n89304\n89538\n89981\n90211\n90316\n90775\n90865\n90673\n90261\n90227\n90046\n90195\n89926\n89120\n88300\n87854\n87989\n88407\n89086\n89512\n92010\n94489\n97397\n95376\n92570\n89423\n88741\n88383\n88100\n87774\n87865\n88340\n88718\n89079\n89354\n90536\n91205\n91736\n91338\n91807\n91835\n92442\n92176\n92602\n92910\n93823\n94498\n95042\n95671\n96585\n97352\n97841\n98104\n98072\n97932\n97641\n97686\n97814\n98241\n99104\n99684\n99942\n99783\n99883\n100212\n100604\n101044\n101717\n102329\n102578\n102513\n102535\n103628\n104608\n105796\n106394\n107111\n107159\n107168\n106727\n106660\n106859\n107897\n108556\n108820\n108513\n108799\n109247\n109856\n111238\n112247\n113258\n113311\n113568\n113701\n113777\n114673\n116200\n117921\n118721\n119160\n120074\n121683\n123388\n124612\n125349\n125574\n126977\n128741\n131429\n134617\n136701\n138838\n139305\n141073\n141945\n142649\n142999\n143629\n145174\n147071\n149069\n150897\n152833\n154746\n156523\n157280\n157491\n157018\n157267\n156938\n\n\n2685\n76772\n2689\n43613\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n68212\n68424\n68574\n69101\n69763\n70525\n71096\n71503\n71935\n72339\n72889\n73447\n74126\n74648\n75203\n75750\n76306\n76796\n77150\n77584\n78080\n78677\n79114\n79466\n79654\n78998\n78872\n78681\n79541\n79804\n80196\n80268\n80441\n80628\n81055\n81421\n81904\n82396\n82838\n83238\n83318\n83498\n83666\n84238\n84711\n85267\n85667\n85967\n86129\n86368\n86574\n86826\n87027\n87238\n87488\n87690\n88107\n88441\n88855\n89281\n89733\n90164\n90588\n90956\n91312\n91557\n91679\n91575\n91427\n91411\n91515\n91430\n91263\n91044\n91088\n91145\n91398\n91521\n91730\n91746\n91714\n91486\n91253\n91067\n91065\n91209\n91338\n91330\n91096\n90539\n89972\n89326\n88689\n87942\n87076\n86658\n86445\n85527\n84433\n84099\n84342\n84791\n84627\n85020\n84785\n84224\n83089\n81994\n80773\n79782\n78244\n77906\n77791\n78635\n77869\n77518\n76933\n77063\n76796\n76972\n76859\n77084\n77097\n77241\n76776\n76463\n76536\n76612\n76441\n75857\n75273\n74345\n73388\n72509\n72366\n71968\n71674\n70980\n70269\n69294\n68815\n68631\n68792\n68509\n67981\n67223\n66965\n67011\n67561\n67684\n67796\n67431\n67077\n66457\n66004\n65469\n65319\n65114\n64947\n64693\n64435\n64446\n64762\n65318\n65732\n68035\n70444\n72934\n73331\n73299\n72731\n72285\n72107\n72425\n72632\n72707\n73046\n73306\n73596\n73753\n74215\n74780\n75155\n75344\n75743\n76046\n76197\n75895\n75864\n76176\n77143\n77990\n78615\n79201\n80063\n80862\n81148\n80912\n80732\n80819\n81042\n81343\n81365\n81594\n81837\n82203\n82624\n82947\n83165\n83363\n83654\n84179\n84771\n85084\n85324\n85486\n85830\n86829\n87796\n89061\n89713\n90279\n90126\n89999\n89374\n89072\n88844\n89385\n90271\n90819\n91108\n91328\n91958\n92775\n94373\n95597\n96541\n96761\n96835\n97032\n96958\n97682\n98875\n100097\n100709\n101287\n102708\n105008\n107201\n108692\n109340\n109273\n110299\n111578\n113617\n116160\n118222\n120858\n122185\n124519\n125619\n126074\n126201\n126809\n128246\n129813\n131456\n133170\n135107\n136843\n138449\n139014\n139132\n138742\n139007\n138477\n\n\n3381\n76773\n3386\n43614\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n99858\n100458\n100850\n101729\n102701\n103761\n104817\n105553\n106195\n106902\n107709\n108161\n108457\n108538\n108876\n109115\n109636\n110128\n110956\n111709\n112455\n112901\n113176\n113723\n114289\n113645\n113945\n114158\n115360\n115315\n115222\n115255\n115663\n116245\n117097\n117596\n117965\n118338\n118770\n119264\n119519\n119975\n120710\n121503\n121845\n121929\n122088\n122250\n122525\n122687\n122920\n123076\n123397\n123970\n124514\n124786\n125101\n125333\n125804\n126229\n126752\n127278\n127841\n128104\n128004\n127670\n127726\n127924\n128120\n128240\n128583\n128903\n129227\n129183\n129402\n129508\n129777\n129898\n129774\n129469\n128804\n128024\n127467\n127178\n127173\n127160\n126985\n126697\n126242\n125946\n125542\n125076\n124550\n124117\n123323\n122666\n122082\n120951\n119131\n118382\n118378\n119169\n119110\n119593\n119454\n119126\n118192\n117148\n116173\n115556\n114513\n114296\n114089\n114613\n113574\n112748\n111896\n111481\n111143\n111320\n111613\n111709\n111952\n111786\n111621\n110727\n110674\n110507\n110355\n109685\n108618\n107083\n105516\n104190\n103911\n103659\n103396\n103076\n102597\n102200\n101409\n100561\n99620\n98658\n97784\n97383\n97539\n97893\n98115\n98221\n98476\n98523\n98357\n97765\n97202\n96504\n96318\n96108\n95887\n95739\n95879\n96191\n96782\n97323\n97963\n98417\n99188\n100140\n100847\n101107\n100779\n100809\n100725\n101192\n101176\n101105\n100981\n101113\n101559\n102314\n103570\n104764\n105528\n105735\n106729\n107335\n108042\n107404\n107461\n107636\n108751\n109737\n110374\n111246\n112415\n113578\n113772\n113284\n112797\n112809\n112800\n113007\n113084\n113914\n114751\n115463\n116082\n116113\n116407\n116279\n116514\n116837\n117437\n118359\n118935\n119240\n118965\n119906\n120846\n122368\n122813\n123311\n123440\n123567\n123314\n123447\n123855\n124735\n125294\n125438\n125655\n126245\n127346\n128403\n129714\n130378\n130944\n131286\n131637\n131992\n131493\n132205\n133548\n135255\n135903\n136509\n137627\n139857\n141910\n143520\n144321\n144092\n145312\n146710\n149597\n152850\n155053\n157467\n158285\n160634\n161676\n162615\n162922\n163662\n165178\n167062\n168809\n170503\n172347\n174202\n176110\n177215\n178003\n178063\n178865\n178176\n\n\n3590\n76771\n3595\n43612\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n59781\n60101\n60310\n60827\n61277\n61815\n62251\n62560\n62789\n63064\n63472\n63821\n64136\n64300\n64642\n65016\n65570\n66075\n66610\n67310\n67966\n68539\n68852\n69132\n69349\n68771\n68873\n69043\n70025\n70250\n70431\n70702\n70992\n71354\n71635\n71897\n72196\n72563\n72912\n73210\n73313\n73431\n73568\n73779\n74052\n74189\n74342\n74418\n74661\n74983\n75333\n75634\n75822\n76111\n76351\n76654\n76826\n77058\n77217\n77527\n77753\n77948\n78091\n78312\n78764\n79017\n79079\n78827\n78780\n78781\n78864\n78791\n78855\n78967\n79155\n79340\n79417\n79547\n79555\n79559\n79406\n79163\n78830\n78499\n78332\n78420\n78449\n78018\n77494\n76853\n76626\n76213\n75917\n75357\n74761\n74395\n74163\n73202\n71691\n71134\n71243\n71592\n71242\n71332\n71120\n70894\n70318\n69750\n68731\n67528\n66221\n65638\n65208\n65252\n64547\n64322\n63888\n63505\n63107\n62990\n63036\n62835\n62467\n62221\n61889\n61663\n61405\n61173\n60916\n60394\n59548\n58250\n56979\n55894\n55829\n55429\n55199\n54304\n53632\n52906\n52315\n52017\n51749\n51320\n50715\n50304\n50602\n51034\n51320\n51284\n51267\n50660\n49870\n48879\n48447\n47888\n47272\n46633\n46123\n46283\n46693\n47026\n46978\n46529\n45918\n45318\n44987\n45219\n45735\n46272\n46355\n46621\n46720\n47004\n47130\n47188\n47586\n47504\n47427\n47194\n47277\n47546\n47766\n47664\n47769\n47734\n48045\n47984\n48161\n48447\n48840\n49237\n49525\n50006\n50717\n51290\n51603\n51623\n51647\n51899\n51862\n52270\n52335\n52667\n52933\n53186\n53451\n53078\n53054\n53119\n53411\n54091\n54680\n55325\n55460\n55653\n55901\n56538\n56692\n57060\n57175\n57834\n58333\n58859\n58812\n58931\n58953\n59235\n59519\n59721\n60176\n60290\n60497\n60597\n61250\n61974\n62698\n63041\n63456\n64060\n64425\n64913\n65457\n66115\n66251\n66600\n67222\n68801\n70592\n72395\n73668\n74103\n75095\n76392\n78087\n79821\n80829\n82358\n83251\n85106\n86063\n86941\n87665\n88398\n89576\n90656\n92018\n93191\n94394\n95363\n96545\n97256\n97939\n98131\n98808\n99087\n\n\n4570\n76764\n4575\n43605\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n46369\n46437\n46701\n46965\n47113\n47195\n47376\n47504\n47682\n48014\n48552\n48912\n49230\n49655\n49909\n50264\n50518\n51034\n51449\n52140\n52805\n53319\n53442\n53658\n53731\n53304\n53282\n53578\n54485\n55010\n55295\n55549\n55646\n55740\n56132\n56422\n56860\n56927\n57209\n57217\n57362\n57415\n57717\n57922\n58080\n57947\n56990\n56080\n55428\n55803\n55911\n55938\n56036\n56342\n56739\n56703\n56785\n56996\n57885\n58612\n59259\n59233\n59607\n59732\n59818\n59542\n59437\n59251\n59078\n59003\n59423\n59784\n60100\n60496\n60486\n60447\n60158\n60644\n61242\n61750\n61835\n61626\n61212\n60990\n61017\n61377\n61537\n61572\n61049\n60314\n59697\n59249\n58890\n58306\n57490\n56970\n56225\n55267\n53502\n52732\n52700\n53194\n52494\n52104\n51247\n50128\n48527\n47249\n46484\n46001\n45573\n44694\n43665\n42166\n40641\n38215\n36216\n34892\n33648\n32795\n31509\n31030\n30411\n30858\n31284\n31572\n31382\n31400\n31748\n31954\n31735\n30795\n29494\n28829\n28148\n27700\n26627\n26776\n26859\n26865\n26477\n26371\n26470\n26438\n26097\n25843\n25247\n24967\n24487\n23998\n23272\n22562\n22007\n21573\n21413\n21307\n21271\n21026\n20549\n19819\n19228\n19219\n19390\n20088\n20435\n20896\n21061\n21540\n22162\n22460\n22550\n22310\n22453\n22206\n21862\n21152\n21040\n21047\n21229\n21152\n21268\n21243\n21499\n21539\n21587\n21825\n22274\n22720\n23000\n23176\n23474\n23491\n23768\n24008\n24708\n25197\n25828\n25591\n25533\n25446\n25461\n25490\n25242\n25236\n25106\n25136\n25076\n24967\n25138\n25390\n25445\n25276\n25261\n25582\n25816\n25801\n25587\n25301\n25256\n24981\n24877\n24527\n24718\n24745\n24689\n24384\n24264\n24516\n24660\n24872\n25138\n25509\n25673\n25786\n26182\n26530\n26734\n26845\n27261\n27547\n27731\n27687\n27956\n28383\n28855\n29341\n29983\n30626\n31182\n31230\n31735\n31945\n32764\n33402\n34385\n35447\n36416\n37917\n39121\n40848\n41700\n42502\n42939\n43568\n44241\n45097\n45863\n46541\n46873\n47161\n47633\n48081\n48729\n48991\n49101\n48912\n\n\n4825\n76765\n4834\n43606\nzip\nOH\nOH\nToledo\nToledo, OH\nLucas County\n91464\n91665\n91956\n92483\n93206\n93771\n94307\n94884\n95555\n96171\n96609\n97150\n97835\n98477\n98935\n99457\n99970\n100674\n101354\n102082\n102672\n103325\n103844\n104307\n104458\n103529\n103709\n103834\n105082\n105226\n105375\n105833\n106403\n107280\n107916\n108464\n108867\n109389\n109815\n110310\n110506\n110867\n111134\n111385\n111580\n111719\n111919\n112022\n112224\n112588\n113056\n113460\n113719\n114025\n114473\n115007\n115506\n115798\n116077\n116317\n116748\n117115\n117571\n117768\n117924\n117852\n117817\n117682\n117573\n117527\n117619\n117456\n117267\n117038\n117077\n117450\n117933\n118330\n118416\n118205\n117983\n117535\n117125\n116701\n116530\n116657\n116800\n116597\n116037\n115254\n114241\n113086\n112447\n112228\n111979\n111996\n112043\n111382\n110119\n109679\n110126\n110934\n111321\n112022\n112039\n111522\n110334\n108740\n107120\n105255\n103316\n102591\n102617\n103312\n102716\n102295\n101806\n101346\n101009\n101140\n101160\n101431\n101741\n102199\n101984\n101348\n101372\n101377\n101412\n100917\n100101\n98499\n96541\n94972\n95018\n94933\n94689\n94110\n93701\n93452\n93068\n92783\n92497\n91963\n91407\n90906\n90779\n90916\n91329\n91942\n92514\n92443\n91621\n90754\n90344\n90150\n90009\n89862\n89800\n89820\n89763\n89649\n89771\n90103\n90603\nNA\n96266\n99416\nNA\n94406\n90940\n90939\n91361\n92210\n92454\n92487\n92601\n92682\n92934\n93252\n94332\n95309\n95957\n95865\n96083\n96291\n96496\n96100\n95898\n95967\n96665\n97348\n98132\n99104\n100260\n101068\n101164\n100897\n100674\n100807\n100982\n100928\n100554\n100483\n101051\n101822\n102360\n102347\n102391\n102712\n102933\n103554\n104420\n105386\n105957\n106216\n106778\n107890\n108634\n109497\n110252\n111193\n111545\n111566\n111044\n110910\n110910\n111740\n112617\n113123\n113668\n114211\n115167\n116039\n117284\n118195\n118996\n119497\n119928\n120304\n120347\n121209\n122396\n123743\n124178\n124935\n126278\n128784\n131037\n132459\n133194\n132968\n134678\n136344\n139149\n141869\n143892\n146493\n148018\n150346\n151202\n151209\n151124\n151733\n153220\n154911\n156373\n157784\n159616\n161648\n163625\n164529\n165048\n165050\n165896\n165519"
  },
  {
    "objectID": "eda.html#zillow-observed-rent-index-zori",
    "href": "eda.html#zillow-observed-rent-index-zori",
    "title": "Exploratory Data Analysis",
    "section": "Zillow Observed Rent Index (ZORI)",
    "text": "Zillow Observed Rent Index (ZORI)"
  },
  {
    "objectID": "arma_arima_sarima.html#log-transform",
    "href": "arma_arima_sarima.html#log-transform",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Log-transform",
    "text": "Log-transform\n\n\n\n\n\nCode\nautoplot(discharge_ts, main=\"Stream flow discharge at Waterville, OH station\")\nautoplot(log(discharge_ts), main=\"Stream flow discharge at Waterville, OH station\")\n\n\n\n\n\n\n\n\n(a) Original stream flow\n\n\n\n\n\n\n\n(b) Log-transformed stream flow\n\n\n\n\nFigure 1: Log transform of the data\n\n\n\nFigure 1 shows that the data need to be log-transformed. Thus, compared to the analysis in the EDA session, log-transformed data is used here.\n\nACF and PACF\n\n\nCode\n### can get monthly data\n# Get mean value for each month\ndischarge_df[\"log_discharge\"] <- log(discharge_df[\"discharge.discharge\"])\n\nmean_data <- discharge_df %>% \n  mutate(month = month(discharge.date), year = year(discharge.date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(log_discharge))\n\ndischarge_month<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndischarge_month2022 <- window(discharge_month,start=1980,end=2022)\ndecompose_discharge <- decompose(as.ts(discharge_month2022), \"additive\")\nplot(decompose_discharge)\n\n\n\n\n\nFigure 2: Decomposition of monthly stream flow\n\n\n\n\n\n\nCode\n## ACF\n\nggAcf(discharge_month,400)\nggPacf(discharge_month,400)\n\n\n\n\n\n\n\n\n(a) ACF plot for monthly streamflow 1970-2022\n\n\n\n\n\n\n\n(b) PACF plot for monthly streamflow 1970-2022\n\n\n\n\nFigure 3: ACF and PACF plots\n\n\n\n\n\nCode\n################ ADF Test #############\ntseries::adf.test(discharge_month2022)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  discharge_month2022\nDickey-Fuller = -9.366, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe monthly loged data is still not stationary."
  },
  {
    "objectID": "arma_arima_sarima.html#difference",
    "href": "arma_arima_sarima.html#difference",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Difference",
    "text": "Difference\n\n\nCode\n######### see the differenced series #########\n# first order\ndf.ts.discharge<- discharge_month %>% diff() \ndf.ts.discharge%>% ggtsdisplay()\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 4: First order differencing of monthly stream flow\n\n\n\n\n\n\nCode\n## second order\ndf.ts.discharge %>% diff() %>% diff() %>% ggtsdisplay()\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5: Second order differencing of monthly stream flow\n\n\n\n\n\n\nCode\n## second order\ndf.ts.discharge %>% diff() %>% diff() %>% diff() %>% ggtsdisplay()\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 6: Third order differencing of monthly stream flow\n\n\n\n\n\n\nCode\n################ ADF Test #############\ndf.ts.discharge1980 <- window(discharge_month,start=1980,end=2021)\ntseries::adf.test(df.ts.discharge1980)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df.ts.discharge1980\nDickey-Fuller = -9.13, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\nFigure 5 shows that the second order differencing may be needed for this data. Figure 4 is not enough, while Figure 6 does not improve much compared to the second order differencing. However, the ADF test show that the time series still not stationary. There might be some seasonal pattern need to be considered. We will go with the second order for now.\nFor the MA model parameter, q, ACF plot shows that it can be 1 or 2. And the AR model parameter, p, from PACF plot shows that it can be up to 7."
  },
  {
    "objectID": "arma_arima_sarima.html#arima-model",
    "href": "arma_arima_sarima.html#arima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ARIMA model",
    "text": "ARIMA model\n\n\nCode\ni=1\nd=1\ntemp= data.frame()\nls=matrix(rep(NA,6*40),nrow=40)\n\n\nfor (p in 1:8)#2 p=0,1,2,3,4,5,6,7\n{\n  for(q in 1:3)#2 q = 0,1,2\n  {\n    for(d in 1:2)#2 d=1,2\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        skip_to_next <- FALSE\n            model<- tryCatch({\n    Arima(df.ts.discharge1980,order=c(p-1,d,q-1),include.drift=FALSE) \n        \n  }, error = function(e) { skip_to_next <<- TRUE})\nif(skip_to_next) { next }\n            ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n        \n      }\n      \n    }\n  }\n}\n\n\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n1357.648\n1361.846\n1357.656\n\n\n0\n2\n0\n1769.527\n1773.724\n1769.535\n\n\n0\n1\n1\n1347.007\n1355.404\n1347.031\n\n\n0\n2\n1\n1364.089\n1372.482\n1364.114\n\n\n0\n1\n2\n1348.832\n1361.427\n1348.881\n\n\n0\n2\n2\n1353.817\n1366.406\n1353.866\n\n\n1\n1\n0\n1346.593\n1354.989\n1346.617\n\n\n1\n2\n0\n1564.562\n1572.955\n1564.586\n\n\n1\n1\n1\n1348.447\n1361.042\n1348.496\n\n\n1\n2\n1\n1353.359\n1365.948\n1353.408\n\n\n1\n1\n2\n1349.204\n1365.997\n1349.286\n\n\n1\n2\n2\n1355.176\n1371.961\n1355.258\n\n\n2\n1\n0\n1348.500\n1361.095\n1348.549\n\n\n2\n2\n0\n1508.410\n1521.000\n1508.460\n\n\n2\n1\n1\n1349.326\n1366.120\n1349.409\n\n\n2\n2\n1\n1355.237\n1372.022\n1355.319\n\n\n2\n1\n2\n1263.736\n1284.728\n1263.859\n\n\n2\n2\n2\n1356.069\n1377.051\n1356.192\n\n\n3\n1\n0\n1349.660\n1366.453\n1349.742\n\n\n3\n2\n0\n1489.302\n1506.087\n1489.384\n\n\n3\n1\n1\n1241.035\n1262.028\n1241.159\n\n\n3\n2\n1\n1356.477\n1377.459\n1356.600\n\n\n3\n1\n2\n1209.143\n1234.334\n1209.316\n\n\n3\n2\n2\n1358.036\n1383.215\n1358.210\n\n\n4\n1\n0\n1346.226\n1367.218\n1346.349\n\n\n4\n2\n0\n1484.882\n1505.864\n1485.005\n\n\n4\n1\n1\n1219.709\n1244.900\n1219.883\n\n\n4\n2\n1\n1353.250\n1378.429\n1353.424\n\n\n4\n1\n2\n1204.395\n1233.784\n1204.626\n\n\n4\n2\n2\n1236.064\n1265.439\n1236.296\n\n\n5\n1\n0\n1329.687\n1354.878\n1329.860\n\n\n5\n2\n0\n1479.310\n1504.489\n1479.484\n\n\n5\n1\n1\n1203.909\n1233.299\n1204.141\n\n\n5\n2\n1\n1337.097\n1366.472\n1337.329\n\n\n5\n1\n2\n1201.744\n1235.332\n1202.042\n\n\n6\n1\n0\n1309.947\n1339.336\n1310.178\n\n\n6\n2\n0\n1477.736\n1507.111\n1477.968\n\n\n6\n1\n1\n1200.849\n1234.437\n1201.147\n\n\n7\n1\n0\n1266.364\n1299.951\n1266.662\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n38 6 1 1 1200.849 1234.437 1201.147\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n38 6 1 1 1200.849 1234.437 1201.147\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n33 5 1 1 1203.909 1233.299 1204.141\n\n\n\n\nCode\n######### compare 2 models\n\nmodel_output <- capture.output(sarima(df.ts.discharge1980,6,1,1))\ncat(model_output[53:81], model_output[length(model_output)], sep = \"\\n\") \n\n\nCoefficients:\n         ar1     ar2      ar3      ar4      ar5      ar6      ma1  constant\n      0.5203  0.1032  -0.0665  -0.0945  -0.1351  -0.1040  -1.0000     3e-04\ns.e.  0.0448  0.0504   0.0505   0.0505   0.0505   0.0451   0.0067     3e-04\n\nsigma^2 estimated as 0.6407:  log likelihood = -591.99,  aic = 1201.99\n\n$degrees_of_freedom\n[1] 484\n\n$ttable\n         Estimate     SE   t.value p.value\nar1        0.5203 0.0448   11.6064  0.0000\nar2        0.1032 0.0504    2.0486  0.0410\nar3       -0.0665 0.0505   -1.3159  0.1888\nar4       -0.0945 0.0505   -1.8719  0.0618\nar5       -0.1351 0.0505   -2.6766  0.0077\nar6       -0.1040 0.0451   -2.3046  0.0216\nma1       -1.0000 0.0067 -148.3871  0.0000\nconstant   0.0003 0.0003    0.9217  0.3572\n\n$AIC\n[1] 2.443064\n\n$AICc\n[1] 2.44367\n\n$BIC\n[1] 2.519866\n\n\n\n\n\nFigure 7: Model 1 diagnosis (6,1,1)\n\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(df.ts.discharge1980,5,1,1))\ncat(model_output2[47:74], model_output2[length(model_output2)], sep = \"\\n\")\n\n\nCoefficients:\n         ar1     ar2      ar3      ar4      ar5      ma1  constant\n      0.5407  0.1139  -0.0596  -0.1062  -0.1910  -1.0000     3e-04\ns.e.  0.0442  0.0504   0.0507   0.0505   0.0445   0.0063     4e-04\n\nsigma^2 estimated as 0.648:  log likelihood = -594.63,  aic = 1205.27\n\n$degrees_of_freedom\n[1] 485\n\n$ttable\n         Estimate     SE   t.value p.value\nar1        0.5407 0.0442   12.2310  0.0000\nar2        0.1139 0.0504    2.2583  0.0244\nar3       -0.0596 0.0507   -1.1756  0.2403\nar4       -0.1062 0.0505   -2.1033  0.0360\nar5       -0.1910 0.0445   -4.2929  0.0000\nma1       -1.0000 0.0063 -159.2364  0.0000\nconstant   0.0003 0.0004    0.7963  0.4263\n\n$AIC\n[1] 2.44973\n\n$AICc\n[1] 2.4502\n\n$BIC\n[1] 2.517998\n\n\n\n\n\nFigure 8: Model 2 diagnosis (5,1,1)\n\n\n\n\nFrom the results and diagnosis (Figure 7 and Figure 8), ARIMA(5,1,1) and ARIMA(6,1,1) are with lowest AIC, AICc, or BIC. From the diagnosis, both model works well. ARIMA(5,1,1) has less parameters, thus, ARIMA(5,1,1) is chosen as the best model with the equation as follow: \\[\nx_t = 0.5407*x_{t-1}+0.1139*x_{t-2}-0.0596*x_{t-3}-0.1062*x_{t-4}-0.1910*x_{t-5}-\\omega_{t-1}+\\omega_t\n\\]\n\nauto.arima()\nUsing auto.arima() gives results as below. The model is different from the chosen model.The difference comes from that auto.arima() uses different cariteria to chose the best model, i.e.a combination of unit root tests, minimization of the AIC and MLE (source).\n\n\nCode\nauto.arima(df.ts.discharge1980,seasonal = FALSE)\n\n\nSeries: df.ts.discharge1980 \nARIMA(2,0,2) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ma1     ma2    mean\n      1.6474  -0.9023  -1.2396  0.5716  7.8237\ns.e.  0.0318   0.0325   0.0754  0.0682  0.0466\n\nsigma^2 = 0.6339:  log likelihood = -585.33\nAIC=1182.65   AICc=1182.83   BIC=1207.86\n\n\nCode\nmodel_output3 <- capture.output(sarima(df.ts.discharge1980,2,0,2))\ncat(model_output3[60:85], model_output3[length(model_output3)], sep = \"\\n\")\n\n\nCoefficients:\n         ar1      ar2      ma1     ma2   xmean\n      1.6474  -0.9023  -1.2396  0.5716  7.8237\ns.e.  0.0318   0.0325   0.0754  0.0682  0.0466\n\nsigma^2 estimated as 0.6274:  log likelihood = -585.33,  aic = 1182.65\n\n$degrees_of_freedom\n[1] 488\n\n$ttable\n      Estimate     SE  t.value p.value\nar1     1.6474 0.0318  51.8379       0\nar2    -0.9023 0.0325 -27.7802       0\nma1    -1.2396 0.0754 -16.4371       0\nma2     0.5716 0.0682   8.3841       0\nxmean   7.8237 0.0466 168.0616       0\n\n$AIC\n[1] 2.398894\n\n$AICc\n[1] 2.399144\n\n$BIC\n[1] 2.450016\n\n\n\n\n\nFigure 9: Model 2 diagnosis (2,0,2)\n\n\n\n\n\n\nChosen model RMSE\nThe chosen model is ARIMA(5,1,1).\n\n\nCode\nmodel <- sarima(df.ts.discharge1980,5,1,1)\n\n\ninitial  value -0.043422 \niter   2 value -0.077676\niter   3 value -0.086688\niter   4 value -0.090021\niter   5 value -0.095887\niter   6 value -0.154643\niter   7 value -0.181523\niter   8 value -0.195392\niter   9 value -0.196261\niter  10 value -0.200343\niter  11 value -0.200970\niter  12 value -0.203454\niter  13 value -0.203627\niter  14 value -0.204076\niter  15 value -0.204154\niter  16 value -0.204176\niter  17 value -0.204177\niter  18 value -0.204178\niter  19 value -0.204178\niter  19 value -0.204178\niter  19 value -0.204178\nfinal  value -0.204178 \nconverged\ninitial  value -0.204173 \niter   2 value -0.204677\niter   3 value -0.207122\niter   4 value -0.209444\niter   5 value -0.209952\niter   6 value -0.210167\niter   7 value -0.210276\niter   8 value -0.210329\niter   9 value -0.210333\niter  10 value -0.210334\niter  11 value -0.210334\niter  12 value -0.210334\niter  12 value -0.210334\niter  12 value -0.210334\nfinal  value -0.210334 \nconverged\n\n\n\n\n\nCode\n(rmse_arima511 <- sqrt(mean(model$fit$sigma2)))\n\n\n[1] 0.8049785\n\n\n\n\nForecast\nHere, using the chosen model ARIMA(5,1,1) to forecast the next 10 months. Figure 10 shows the forecasting plot.\n\n\nCode\n###### Forecasting ###########\ndf.ts.discharge1980 %>%\n  Arima(order=c(5,1,1),include.drift = FALSE) %>%\n  forecast %>%\n  autoplot() +\n  ylab(\"Log discharge\") + xlab(\"Time\") +\n  theme(aspect.ratio = 0.4)\n\n\n\n\n\nFigure 10: Next 10 month forecasting\n\n\n\n\n\n\nComparing with benchmark models\nIn this session, we compared ARIMA model with benchmark models of mean, naive and seasonal naive methods. From the diagnosis in Figure 11, we can see the seasonal naive method is better than the other two. From Summary of ARIMA model and the Seasonal naive model, we can see the chosen ARIMA(5,1,1) has lower RMSE and MAE. However, the seasonal naive method captures better seasonal trend in Figure 12. And the diagnosis plot (Figure 11 (c)) also better than the chosen model in Figure 8.\n\n\nCode\n####################### Benchmark methods ###############################\nf1<-meanf(df.ts.discharge1980, h=10) #mean\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\nf2<-naive(df.ts.discharge1980, h=10) # naive method\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\nf3<-snaive(df.ts.discharge1980, h=10) #seasonal naive method\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1216.2, df = 24, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 157.81, df = 24, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 358.05, df = 24, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\n(a) Mean method\n\n\n\n\n\n\n\n\n\n(b) Naive method\n\n\n\n\n\n\n\n(c) Seasonal naive method\n\n\n\n\nFigure 11: Benchmark models diagnosis\n\n\n\n\n\n\n\n\nCode\nautoplot(df.ts.discharge1980) +\n  autolayer(meanf(df.ts.discharge1980, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(df.ts.discharge1980, h=10),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(df.ts.discharge1980, h=10),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for monthly stream flow\") +\n  xlab(\"Year\") + ylab(\"Log Discharge\") +\n  guides(colour=guide_legend(title=\"Forecast\")) + \n  autolayer(for_sarima$pred,series=\"ARIMA(5,1,1)\") +\n  theme(aspect.ratio = 0.4)\n\n#seasonal Naive is good\n\n\n\n\n\nFigure 12: Benchmark model forecasting\n\n\n\n\n\n\n\nSummary of ARIMA model\n\n\nCode\nsummary(fit)\n\n\nSeries: df.ts.discharge1980 \nARIMA(5,1,1) \n\nCoefficients:\n         ar1     ar2      ar3      ar4      ar5      ma1\n      0.5418  0.1138  -0.0597  -0.1059  -0.1895  -1.0000\ns.e.  0.0442  0.0505   0.0507   0.0505   0.0445   0.0092\n\nsigma^2 = 0.6569:  log likelihood = -594.95\nAIC=1203.91   AICc=1204.14   BIC=1233.3\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.004363739 0.8046979 0.6385257 -1.062096 8.377412 0.6903305\n                    ACF1\nTraining set -0.01913352\n\n\nSummary of Seasonal naive model\n\n\nCode\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = df.ts.discharge1980, h = 10) \n\nResidual sd: 1.1724 \n\nError measures:\n                       ME     RMSE       MAE       MPE     MAPE MASE      ACF1\nTraining set -0.006577768 1.172446 0.9249566 -1.322599 12.26289    1 0.4562968\n\nForecasts:\n         Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\nFeb 2021       9.075799 7.573249 10.578349 6.777847 11.373752\nMar 2021       9.419143 7.916592 10.921693 7.121190 11.717095\nApr 2021       8.459100 6.956550  9.961651 6.161148 10.757053\nMay 2021       8.599828 7.097278 10.102379 6.301876 10.897781\nJun 2021       7.310991 5.808440  8.813541 5.013038  9.608943\nJul 2021       6.594944 5.092394  8.097495 4.296992  8.892897\nAug 2021       6.224302 4.721751  7.726852 3.926349  8.522254\nSep 2021       6.243943 4.741393  7.746494 3.945991  8.541896\nOct 2021       6.453248 4.950697  7.955798 4.155295  8.751200\nNov 2021       7.269523 5.766973  8.772074 4.971571  9.567476\n\n\nThis is an indication of seasonal effect need to be considered in the model."
  },
  {
    "objectID": "arma_arima_sarima.html#forecast",
    "href": "arma_arima_sarima.html#forecast",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecast",
    "text": "Forecast\nHere, using the chosen model ARIMA(5,1,1) to forecast the next 10 months. Figure 10 shows the forecasting plot.\n\n\nCode\n###### Forecasting ###########\ndf.ts.discharge1980 %>%\n  Arima(order=c(5,1,1),include.drift = FALSE) %>%\n  forecast %>%\n  autoplot() +\n  ylab(\"Log discharge\") + xlab(\"Time\") +\n  theme(aspect.ratio = 0.4)\n\n\n\n\n\nFigure 10: Next 10 month forecasting"
  },
  {
    "objectID": "arma_arima_sarima.html#comparing-with-benchmark-models",
    "href": "arma_arima_sarima.html#comparing-with-benchmark-models",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Comparing with benchmark models",
    "text": "Comparing with benchmark models\nIn this session, we compared ARIMA model with benchmark models of mean, naive and seasonal naive methods. From the diagnosis in Figure 11, we can see the seasonal naive method is better than the other two. From Summary of ARIMA model and the Seasonal naive model, we can see the chosen ARIMA(5,1,1) has lower RMSE and MAE. However, the seasonal naive method captures better seaonal trend in Figure 12. And the diagnosis plot (Figure 11 (c)) also better than the chosen model in Figure 8.\n\n\nCode\n####################### Benchmark methods ###############################\nf1<-meanf(df.ts.discharge1980, h=10) #mean\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\nf2<-naive(df.ts.discharge1980, h=10) # naive method\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\nf3<-snaive(df.ts.discharge1980, h=10) #seasonal naive method\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1216.2, df = 23, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 24\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 157.81, df = 24, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 358.05, df = 24, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\n(a) Mean method\n\n\n\n\n\n\n\n\n\n(b) Naive method\n\n\n\n\n\n\n\n(c) Seasonal naive method\n\n\n\n\nFigure 11: Benchmark models diagnosis\n\n\n\n\n\n\n\n\nCode\nautoplot(df.ts.discharge1980) +\n  autolayer(meanf(df.ts.discharge1980, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(df.ts.discharge1980, h=10),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(df.ts.discharge1980, h=10),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for monthly stream flow\") +\n  xlab(\"Year\") + ylab(\"Log Discharge\") +\n  guides(colour=guide_legend(title=\"Forecast\")) + \n  autolayer(for_sarima$pred,series=\"ARIMA(5,1,1)\") +\n  theme(aspect.ratio = 0.4)\n\n#seasonal Naive is good\n\n\n\n\n\nFigure 12: Benchmark model forecasting\n\n\n\n\n\n\n\nSummary of ARIMA model\n\n\nCode\nsummary(fit)\n\n\nSeries: df.ts.discharge1980 \nARIMA(5,1,1) \n\nCoefficients:\n         ar1     ar2      ar3      ar4      ar5      ma1\n      0.5418  0.1138  -0.0597  -0.1059  -0.1895  -1.0000\ns.e.  0.0442  0.0505   0.0507   0.0505   0.0445   0.0092\n\nsigma^2 = 0.6569:  log likelihood = -594.95\nAIC=1203.91   AICc=1204.14   BIC=1233.3\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.004363739 0.8046979 0.6385257 -1.062096 8.377412 0.6903305\n                    ACF1\nTraining set -0.01913352\n\n\nSummary of Seasonal naive model\n\n\nCode\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = df.ts.discharge1980, h = 10) \n\nResidual sd: 1.1724 \n\nError measures:\n                       ME     RMSE       MAE       MPE     MAPE MASE      ACF1\nTraining set -0.006577768 1.172446 0.9249566 -1.322599 12.26289    1 0.4562968\n\nForecasts:\n         Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\nFeb 2021       9.075799 7.573249 10.578349 6.777847 11.373752\nMar 2021       9.419143 7.916592 10.921693 7.121190 11.717095\nApr 2021       8.459100 6.956550  9.961651 6.161148 10.757053\nMay 2021       8.599828 7.097278 10.102379 6.301876 10.897781\nJun 2021       7.310991 5.808440  8.813541 5.013038  9.608943\nJul 2021       6.594944 5.092394  8.097495 4.296992  8.892897\nAug 2021       6.224302 4.721751  7.726852 3.926349  8.522254\nSep 2021       6.243943 4.741393  7.746494 3.945991  8.541896\nOct 2021       6.453248 4.950697  7.955798 4.155295  8.751200\nNov 2021       7.269523 5.766973  8.772074 4.971571  9.567476"
  },
  {
    "objectID": "arma_arima_sarima.html#auto.arima",
    "href": "arma_arima_sarima.html#auto.arima",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "auto.arima()",
    "text": "auto.arima()\nUsing auto.arima() gives results as below. The model is different from the chosen model.The difference comes from that auto.arima() uses different cariteria to chose the best model, i.e.a combination of unit root tests, minimization of the AIC and MLE (source).\n\n\nCode\nauto.arima(df.ts.discharge1980,seasonal = FALSE)\n\n\nSeries: df.ts.discharge1980 \nARIMA(2,0,2) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ma1     ma2    mean\n      1.6474  -0.9023  -1.2396  0.5716  7.8237\ns.e.  0.0318   0.0325   0.0754  0.0682  0.0466\n\nsigma^2 = 0.6339:  log likelihood = -585.33\nAIC=1182.65   AICc=1182.83   BIC=1207.86\n\n\nCode\nmodel_output3 <- capture.output(sarima(df.ts.discharge1980,2,0,2))\ncat(model_output3[60:85], model_output3[length(model_output3)], sep = \"\\n\")\n\n\nCoefficients:\n         ar1      ar2      ma1     ma2   xmean\n      1.6474  -0.9023  -1.2396  0.5716  7.8237\ns.e.  0.0318   0.0325   0.0754  0.0682  0.0466\n\nsigma^2 estimated as 0.6274:  log likelihood = -585.33,  aic = 1182.65\n\n$degrees_of_freedom\n[1] 488\n\n$ttable\n      Estimate     SE  t.value p.value\nar1     1.6474 0.0318  51.8379       0\nar2    -0.9023 0.0325 -27.7802       0\nma1    -1.2396 0.0754 -16.4371       0\nma2     0.5716 0.0682   8.3841       0\nxmean   7.8237 0.0466 168.0616       0\n\n$AIC\n[1] 2.398894\n\n$AICc\n[1] 2.399144\n\n$BIC\n[1] 2.450016\n\n\n\n\n\nFigure 9: Model 2 diagnosis (2,0,2)"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html",
    "href": "ARIMAX_SARIMAX_VAR.html",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "",
    "text": "The goal of this project is to explore the relationship among water quality (i.e. sediment, turbidity) and quantity variables (i.e. stream discharge) and climate variables such as precipitation and temperature.\nFor SARIMAX model, the dependent variables in the SARIMAX model is turbidity, which can be affected by stream discharge (Pejman et al. (2009),Mukundan et al. (2013)) and climate variables (Wang (2014), Roosmalen, Christensen, and Sonnenborg (2007)).\nFor VAR model, the chosen variables are stream discharge, precipitation, and sediment. Due to the water cycle effect, stream discharge and precipitation have a relationship on both sides. Sediment is usually considered flow with stream flow and caused by high precipitation (Foster and Meyer (1977)). However, due to the water cycle effect and time series factor, there are researches indicate that high turbidity and sediment is an indication of soil erosion (De Vente et al. (2013)) and can cause later higher stream discharge. Thus, for VAR model, the three variables are used to figure out which process is more significant."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#running-code",
    "href": "ARIMAX_SARIMAX_VAR.html#running-code",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "arma_arima_sarima.html#sarima-model",
    "href": "arma_arima_sarima.html#sarima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA model",
    "text": "SARIMA model\nIn the last session, ARIMA model was not out performed the benchmark models. This is an indication of that seasonal effect need to be considered in the model. Thus, in this session, SARIMA model is applied to model stream flow time series. From Figure 5, the p1=1, p2=8, q1=1,q2= 3, d1=1,d2=2,Q=1,2, P=1,2.\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,d1,d2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(d2)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*250),nrow=250)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(d in d1:d2){\n        \n        for(P in P1:P2)\n        {\n          for(Q in Q1:Q2)\n          {\n            if(p+d+q+P+Q+D<=12)\n            {\n              skip_to_next <- FALSE\n              model<- tryCatch({\n      Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n    }, error = function(e) { skip_to_next <<- TRUE})\n  if(skip_to_next) { next }\n              ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n              i=i+1\n              #print(i)\n              }\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\n# q=0,1,2,3,4,5,6,7; Q=1,2 and PACF plot: p=0,1,2; P=1,2, D=1 and d=0,1,2\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=8,d1=1,d2=3,P1=1,P2=3,Q1=1,Q2=3,data=df.ts.discharge1980)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n1554.711\n1558.885\n1554.720\n\n\n0\n1\n0\n0\n1\n1\n1270.686\n1279.033\n1270.711\n\n\n0\n1\n0\n0\n1\n2\n1272.633\n1285.154\n1272.683\n\n\n0\n1\n0\n1\n1\n0\n1430.233\n1438.580\n1430.258\n\n\n0\n1\n0\n1\n1\n1\n1272.634\n1285.156\n1272.685\n\n\n0\n1\n0\n1\n1\n2\n1273.807\n1290.502\n1273.891\n\n\n0\n1\n0\n2\n1\n0\n1367.976\n1380.497\n1368.026\n\n\n0\n1\n0\n2\n1\n1\n1274.519\n1291.214\n1274.603\n\n\n0\n1\n0\n2\n1\n2\n1276.416\n1297.285\n1276.542\n\n\n0\n2\n0\n0\n1\n0\n2030.439\n2034.611\n2030.448\n\n\n0\n2\n0\n0\n1\n1\n1759.319\n1767.662\n1759.344\n\n\n0\n2\n0\n0\n1\n2\n1761.006\n1773.522\n1761.057\n\n\n0\n2\n0\n1\n1\n0\n1917.611\n1925.954\n1917.636\n\n\n0\n2\n0\n1\n1\n1\n1761.031\n1773.546\n1761.082\n\n\n0\n2\n0\n1\n1\n2\n1761.734\n1778.421\n1761.819\n\n\n0\n2\n0\n2\n1\n0\n1841.816\n1854.331\n1841.867\n\n\n0\n2\n0\n2\n1\n1\n1762.431\n1779.118\n1762.515\n\n\n0\n2\n0\n2\n1\n2\n1763.684\n1784.543\n1763.811\n\n\n0\n3\n0\n0\n1\n0\n2579.920\n2584.089\n2579.928\n\n\n0\n3\n0\n0\n1\n1\n2316.845\n2325.184\n2316.870\n\n\n0\n3\n0\n0\n1\n2\n2318.307\n2330.816\n2318.357\n\n\n0\n3\n0\n1\n1\n0\n2473.464\n2481.803\n2473.489\n\n\n0\n3\n0\n1\n1\n1\n2318.364\n2330.873\n2318.415\n\n\n0\n3\n0\n1\n1\n2\n2318.281\n2334.960\n2318.366\n\n\n0\n3\n0\n2\n1\n0\n2388.689\n2401.198\n2388.739\n\n\n0\n3\n0\n2\n1\n1\n2319.315\n2335.994\n2319.400\n\n\n0\n3\n0\n2\n1\n2\n2319.809\n2340.657\n2319.936\n\n\n0\n1\n1\n0\n1\n0\n1466.645\n1474.993\n1466.670\n\n\n0\n1\n1\n0\n1\n1\n1167.104\n1179.625\n1167.155\n\n\n0\n1\n1\n0\n1\n2\n1168.861\n1185.556\n1168.945\n\n\n0\n1\n1\n1\n1\n0\n1327.195\n1339.716\n1327.246\n\n\n0\n1\n1\n1\n1\n1\n1168.861\n1185.556\n1168.945\n\n\n0\n1\n1\n1\n1\n2\n1170.867\n1191.736\n1170.993\n\n\n0\n1\n1\n2\n1\n0\n1275.872\n1292.567\n1275.956\n\n\n0\n1\n1\n2\n1\n1\n1170.860\n1191.729\n1170.987\n\n\n0\n1\n1\n2\n1\n2\n1172.861\n1197.903\n1173.038\n\n\n0\n2\n1\n0\n1\n0\n1566.434\n1574.777\n1566.459\n\n\n0\n2\n1\n0\n1\n1\n1287.852\n1300.367\n1287.903\n\n\n0\n2\n1\n0\n1\n2\n1289.805\n1306.492\n1289.890\n\n\n0\n2\n1\n1\n1\n0\n1442.993\n1455.508\n1443.044\n\n\n0\n2\n1\n1\n1\n1\n1289.807\n1306.494\n1289.891\n\n\n0\n2\n1\n1\n1\n2\n1290.980\n1311.838\n1291.106\n\n\n0\n2\n1\n2\n1\n0\n1381.464\n1398.151\n1381.548\n\n\n0\n2\n1\n2\n1\n1\n1291.683\n1312.542\n1291.810\n\n\n0\n2\n1\n2\n1\n2\n1293.581\n1318.611\n1293.759\n\n\n0\n3\n1\n0\n1\n0\n2031.477\n2039.817\n2031.503\n\n\n0\n3\n1\n0\n1\n1\n1765.597\n1778.106\n1765.648\n\n\n0\n3\n1\n0\n1\n2\n1767.309\n1783.987\n1767.393\n\n\n0\n3\n1\n1\n1\n0\n1919.648\n1932.157\n1919.699\n\n\n0\n3\n1\n1\n1\n1\n1767.332\n1784.011\n1767.417\n\n\n0\n3\n1\n1\n1\n2\n1768.037\n1788.885\n1768.164\n\n\n0\n3\n1\n2\n1\n0\n1844.661\n1861.339\n1844.745\n\n\n0\n3\n1\n2\n1\n1\n1768.698\n1789.546\n1768.825\n\n\n0\n1\n2\n0\n1\n0\n1456.161\n1468.682\n1456.211\n\n\n0\n1\n2\n0\n1\n1\n1153.603\n1170.298\n1153.687\n\n\n0\n1\n2\n0\n1\n2\n1154.982\n1175.851\n1155.109\n\n\n0\n1\n2\n1\n1\n0\n1320.001\n1336.696\n1320.085\n\n\n0\n1\n2\n1\n1\n1\n1155.017\n1175.886\n1155.143\n\n\n0\n1\n2\n1\n1\n2\n1157.603\n1182.645\n1157.780\n\n\n0\n1\n2\n2\n1\n0\n1256.621\n1277.490\n1256.747\n\n\n0\n1\n2\n2\n1\n1\n1156.561\n1181.604\n1156.739\n\n\n0\n1\n2\n2\n1\n2\n1157.240\n1186.457\n1157.478\n\n\n0\n2\n2\n0\n1\n0\n1480.074\n1492.589\n1480.124\n\n\n0\n2\n2\n0\n1\n1\n1186.284\n1202.970\n1186.368\n\n\n0\n2\n2\n0\n1\n2\n1188.093\n1208.952\n1188.220\n\n\n0\n2\n2\n1\n1\n0\n1341.823\n1358.510\n1341.907\n\n\n0\n2\n2\n1\n1\n1\n1188.094\n1208.953\n1188.221\n\n\n0\n2\n2\n1\n1\n2\n1190.109\n1215.139\n1190.287\n\n\n0\n2\n2\n2\n1\n0\n1291.215\n1312.074\n1291.342\n\n\n0\n2\n2\n2\n1\n1\n1190.092\n1215.122\n1190.270\n\n\n0\n3\n2\n0\n1\n0\n1574.826\n1587.335\n1574.876\n\n\n0\n3\n2\n0\n1\n1\n1294.651\n1311.330\n1294.736\n\n\n0\n3\n2\n0\n1\n2\n1296.555\n1317.404\n1296.683\n\n\n0\n3\n2\n1\n1\n0\n1452.387\n1469.066\n1452.472\n\n\n0\n3\n2\n1\n1\n1\n1296.565\n1317.413\n1296.692\n\n\n0\n3\n2\n2\n1\n0\n1391.470\n1412.318\n1391.597\n\n\n0\n1\n3\n0\n1\n0\n1420.254\n1436.949\n1420.338\n\n\n0\n1\n3\n0\n1\n1\n1127.819\n1148.688\n1127.946\n\n\n0\n1\n3\n0\n1\n2\n1129.734\n1154.777\n1129.912\n\n\n0\n1\n3\n1\n1\n0\n1293.812\n1314.681\n1293.939\n\n\n0\n1\n3\n1\n1\n1\n1129.740\n1154.783\n1129.917\n\n\n0\n1\n3\n1\n1\n2\n1130.700\n1159.916\n1130.937\n\n\n0\n1\n3\n2\n1\n0\n1232.385\n1257.428\n1232.563\n\n\n0\n1\n3\n2\n1\n1\n1131.197\n1160.413\n1131.434\n\n\n0\n2\n3\n0\n1\n0\n1470.527\n1487.214\n1470.611\n\n\n0\n2\n3\n0\n1\n1\n1174.511\n1195.369\n1174.638\n\n\n0\n2\n3\n0\n1\n2\n1176.046\n1201.077\n1176.224\n\n\n0\n2\n3\n1\n1\n0\n1335.462\n1356.321\n1335.589\n\n\n0\n2\n3\n1\n1\n1\n1176.075\n1201.105\n1176.253\n\n\n0\n2\n3\n2\n1\n0\n1273.689\n1298.720\n1273.867\n\n\n0\n3\n3\n0\n1\n0\n1493.798\n1510.477\n1493.883\n\n\n0\n3\n3\n0\n1\n1\n1199.306\n1220.154\n1199.433\n\n\n0\n3\n3\n1\n1\n0\n1356.308\n1377.156\n1356.435\n\n\n0\n1\n4\n0\n1\n0\n1413.964\n1434.833\n1414.090\n\n\n0\n1\n4\n0\n1\n1\n1122.773\n1147.816\n1122.950\n\n\n0\n1\n4\n0\n1\n2\n1124.706\n1153.923\n1124.943\n\n\n0\n1\n4\n1\n1\n0\n1287.435\n1312.478\n1287.612\n\n\n0\n1\n4\n1\n1\n1\n1124.710\n1153.927\n1124.947\n\n\n0\n1\n4\n2\n1\n0\n1227.854\n1257.071\n1228.091\n\n\n0\n2\n4\n0\n1\n0\n1440.639\n1461.497\n1440.766\n\n\n0\n2\n4\n0\n1\n1\n1152.585\n1177.615\n1152.763\n\n\n0\n2\n4\n1\n1\n0\n1315.154\n1340.184\n1315.332\n\n\n0\n3\n4\n0\n1\n0\n1485.402\n1506.250\n1485.529\n\n\n0\n1\n5\n0\n1\n0\n1414.965\n1440.007\n1415.142\n\n\n0\n1\n5\n0\n1\n1\n1121.012\n1150.228\n1121.249\n\n\n0\n1\n5\n1\n1\n0\n1284.238\n1313.454\n1284.475\n\n\n0\n2\n5\n0\n1\n0\n1433.850\n1458.881\n1434.028\n\n\n0\n1\n6\n0\n1\n0\n1414.000\n1443.217\n1414.237\n\n\n1\n1\n0\n0\n1\n0\n1494.539\n1502.887\n1494.565\n\n\n1\n1\n0\n0\n1\n1\n1198.259\n1210.780\n1198.309\n\n\n1\n1\n0\n0\n1\n2\n1200.169\n1216.864\n1200.253\n\n\n1\n1\n0\n1\n1\n0\n1357.988\n1370.509\n1358.038\n\n\n1\n1\n0\n1\n1\n1\n1200.169\n1216.864\n1200.253\n\n\n1\n1\n0\n1\n1\n2\n1202.123\n1222.992\n1202.249\n\n\n1\n1\n0\n2\n1\n0\n1309.527\n1326.222\n1309.611\n\n\n1\n1\n0\n2\n1\n1\n1202.168\n1223.037\n1202.294\n\n\n1\n1\n0\n2\n1\n2\n1204.121\n1229.164\n1204.299\n\n\n1\n2\n0\n0\n1\n0\n1802.468\n1810.812\n1802.493\n\n\n1\n2\n0\n0\n1\n1\n1513.828\n1526.343\n1513.878\n\n\n1\n2\n0\n0\n1\n2\n1515.824\n1532.511\n1515.909\n\n\n1\n2\n0\n1\n1\n0\n1674.889\n1687.404\n1674.939\n\n\n1\n2\n0\n1\n1\n1\n1515.824\n1532.511\n1515.909\n\n\n1\n2\n0\n1\n1\n2\n1517.828\n1538.686\n1517.954\n\n\n1\n2\n0\n2\n1\n0\n1624.087\n1640.774\n1624.171\n\n\n1\n2\n0\n2\n1\n1\n1517.731\n1538.589\n1517.857\n\n\n1\n2\n0\n2\n1\n2\n1519.821\n1544.851\n1519.999\n\n\n1\n3\n0\n0\n1\n0\n2224.197\n2232.536\n2224.223\n\n\n1\n3\n0\n0\n1\n1\n1942.277\n1954.786\n1942.327\n\n\n1\n3\n0\n0\n1\n2\n1944.162\n1960.841\n1944.247\n\n\n1\n3\n0\n1\n1\n0\n2102.401\n2114.910\n2102.452\n\n\n1\n3\n0\n1\n1\n1\n1944.166\n1960.845\n1944.251\n\n\n1\n3\n0\n1\n1\n2\n1946.173\n1967.021\n1946.300\n\n\n1\n3\n0\n2\n1\n0\n2045.283\n2061.961\n2045.367\n\n\n1\n3\n0\n2\n1\n1\n1945.994\n1966.842\n1946.122\n\n\n1\n1\n1\n0\n1\n0\n1414.035\n1426.557\n1414.086\n\n\n1\n1\n1\n0\n1\n1\n1124.142\n1140.837\n1124.227\n\n\n1\n1\n1\n0\n1\n2\n1126.113\n1146.982\n1126.239\n\n\n1\n1\n1\n1\n1\n0\n1287.709\n1304.404\n1287.793\n\n\n1\n1\n1\n1\n1\n1\n1126.114\n1146.983\n1126.241\n\n\n1\n1\n1\n1\n1\n2\n1126.707\n1151.750\n1126.885\n\n\n1\n1\n1\n2\n1\n0\n1224.966\n1245.835\n1225.093\n\n\n1\n1\n1\n2\n1\n1\n1127.752\n1152.795\n1127.930\n\n\n1\n1\n1\n2\n1\n2\n1129.615\n1158.832\n1129.852\n\n\n1\n2\n1\n0\n1\n0\n1506.986\n1519.501\n1507.036\n\n\n1\n2\n1\n0\n1\n1\n1216.317\n1233.004\n1216.402\n\n\n1\n2\n1\n0\n1\n2\n1218.253\n1239.111\n1218.380\n\n\n1\n2\n1\n1\n1\n0\n1371.560\n1388.247\n1371.645\n\n\n1\n2\n1\n1\n1\n1\n1218.253\n1239.111\n1218.380\n\n\n1\n2\n1\n1\n1\n2\n1220.200\n1245.231\n1220.378\n\n\n1\n2\n1\n2\n1\n0\n1323.702\n1344.561\n1323.829\n\n\n1\n2\n1\n2\n1\n1\n1220.253\n1245.283\n1220.431\n\n\n1\n3\n1\n0\n1\n0\n1804.955\n1817.464\n1805.006\n\n\n1\n3\n1\n0\n1\n1\n1521.987\n1538.665\n1522.071\n\n\n1\n3\n1\n0\n1\n2\n1523.975\n1544.823\n1524.102\n\n\n1\n3\n1\n1\n1\n0\n1678.454\n1695.132\n1678.538\n\n\n1\n3\n1\n1\n1\n1\n1523.976\n1544.824\n1524.103\n\n\n1\n3\n1\n2\n1\n0\n1628.289\n1649.137\n1628.416\n\n\n1\n1\n2\n0\n1\n0\n1410.802\n1427.497\n1410.886\n\n\n1\n1\n2\n0\n1\n1\n1118.743\n1139.612\n1118.870\n\n\n1\n1\n2\n0\n1\n2\n1120.661\n1145.703\n1120.838\n\n\n1\n1\n2\n1\n1\n0\n1280.183\n1301.052\n1280.310\n\n\n1\n1\n2\n1\n1\n1\n1120.664\n1145.706\n1120.841\n\n\n1\n1\n2\n1\n1\n2\n1122.737\n1151.953\n1122.974\n\n\n1\n1\n2\n2\n1\n0\n1222.756\n1247.799\n1222.934\n\n\n1\n1\n2\n2\n1\n1\n1122.494\n1151.710\n1122.731\n\n\n1\n2\n2\n0\n1\n0\n1434.358\n1451.045\n1434.442\n\n\n1\n2\n2\n0\n1\n1\n1147.844\n1168.702\n1147.971\n\n\n1\n2\n2\n0\n1\n2\n1150.258\n1175.288\n1150.436\n\n\n1\n2\n2\n1\n1\n0\n1309.900\n1330.758\n1310.027\n\n\n1\n2\n2\n1\n1\n1\n1150.888\n1175.918\n1151.066\n\n\n1\n2\n2\n2\n1\n0\n1249.768\n1274.798\n1249.946\n\n\n1\n3\n2\n0\n1\n0\n1516.622\n1533.301\n1516.707\n\n\n1\n3\n2\n0\n1\n1\n1225.311\n1246.159\n1225.438\n\n\n1\n3\n2\n1\n1\n0\n1382.550\n1403.398\n1382.677\n\n\n1\n1\n3\n0\n1\n0\n1411.820\n1432.689\n1411.946\n\n\n1\n1\n3\n0\n1\n1\n1119.269\n1144.311\n1119.446\n\n\n1\n1\n3\n0\n1\n2\n1121.169\n1150.386\n1121.407\n\n\n1\n1\n3\n1\n1\n0\n1281.252\n1306.295\n1281.429\n\n\n1\n1\n3\n1\n1\n1\n1121.173\n1150.390\n1121.411\n\n\n1\n1\n3\n2\n1\n0\n1224.540\n1253.757\n1224.778\n\n\n1\n2\n3\n0\n1\n0\n1467.266\n1488.124\n1467.392\n\n\n1\n2\n3\n0\n1\n1\n1143.565\n1168.596\n1143.743\n\n\n1\n2\n3\n1\n1\n0\n1337.775\n1362.805\n1337.953\n\n\n1\n3\n3\n0\n1\n0\n1501.151\n1521.999\n1501.278\n\n\n1\n1\n4\n0\n1\n0\n1404.794\n1429.837\n1404.972\n\n\n1\n1\n4\n0\n1\n1\n1121.052\n1150.269\n1121.289\n\n\n1\n1\n4\n1\n1\n0\n1283.086\n1312.302\n1283.323\n\n\n1\n2\n4\n0\n1\n0\n1437.959\n1462.990\n1438.137\n\n\n1\n1\n5\n0\n1\n0\n1415.661\n1444.877\n1415.898\n\n\n2\n1\n0\n0\n1\n0\n1483.043\n1495.564\n1483.094\n\n\n2\n1\n0\n0\n1\n1\n1186.255\n1202.950\n1186.339\n\n\n2\n1\n0\n0\n1\n2\n1188.131\n1209.000\n1188.258\n\n\n2\n1\n0\n1\n1\n0\n1344.974\n1361.669\n1345.058\n\n\n2\n1\n0\n1\n1\n1\n1188.131\n1208.999\n1188.257\n\n\n2\n1\n0\n1\n1\n2\n1190.107\n1215.149\n1190.284\n\n\n2\n1\n0\n2\n1\n0\n1294.571\n1315.440\n1294.698\n\n\n2\n1\n0\n2\n1\n1\n1190.128\n1215.170\n1190.305\n\n\n2\n1\n0\n2\n1\n2\n1192.109\n1221.325\n1192.346\n\n\n2\n2\n0\n0\n1\n0\n1711.439\n1723.954\n1711.489\n\n\n2\n2\n0\n0\n1\n1\n1420.433\n1437.119\n1420.517\n\n\n2\n2\n0\n0\n1\n2\n1422.422\n1443.281\n1422.549\n\n\n2\n2\n0\n1\n1\n0\n1579.997\n1596.684\n1580.081\n\n\n2\n2\n0\n1\n1\n1\n1422.422\n1443.281\n1422.549\n\n\n2\n2\n0\n1\n1\n2\n1424.399\n1449.430\n1424.577\n\n\n2\n2\n0\n2\n1\n0\n1532.658\n1553.517\n1532.785\n\n\n2\n2\n0\n2\n1\n1\n1424.409\n1449.439\n1424.586\n\n\n2\n3\n0\n0\n1\n0\n2042.722\n2055.230\n2042.772\n\n\n2\n3\n0\n0\n1\n1\n1764.449\n1781.127\n1764.533\n\n\n2\n3\n0\n0\n1\n2\n1766.211\n1787.059\n1766.338\n\n\n2\n3\n0\n1\n1\n0\n1925.007\n1941.686\n1925.092\n\n\n2\n3\n0\n1\n1\n1\n1766.222\n1787.070\n1766.349\n\n\n2\n3\n0\n2\n1\n0\n1874.001\n1894.849\n1874.128\n\n\n2\n1\n1\n0\n1\n0\n1410.127\n1426.822\n1410.211\n\n\n2\n1\n1\n0\n1\n1\n1117.561\n1138.430\n1117.687\n\n\n2\n1\n1\n0\n1\n2\n1119.461\n1144.504\n1119.639\n\n\n2\n1\n1\n1\n1\n0\n1279.405\n1300.274\n1279.532\n\n\n2\n1\n1\n1\n1\n1\n1119.465\n1144.508\n1119.643\n\n\n2\n1\n1\n1\n1\n2\n1120.457\n1149.673\n1120.694\n\n\n2\n1\n1\n2\n1\n0\n1222.582\n1247.625\n1222.760\n\n\n2\n1\n1\n2\n1\n1\n1121.283\n1150.500\n1121.521\n\n\n2\n2\n1\n0\n1\n0\n1495.823\n1512.510\n1495.908\n\n\n2\n2\n1\n0\n1\n1\n1204.650\n1225.509\n1204.777\n\n\n2\n2\n1\n0\n1\n2\n1206.558\n1231.588\n1206.736\n\n\n2\n2\n1\n1\n1\n0\n1358.898\n1379.757\n1359.025\n\n\n2\n2\n1\n1\n1\n1\n1206.557\n1231.587\n1206.735\n\n\n2\n2\n1\n2\n1\n0\n1309.131\n1334.162\n1309.309\n\n\n2\n3\n1\n0\n1\n0\n1714.820\n1731.499\n1714.905\n\n\n2\n3\n1\n0\n1\n1\n1429.491\n1450.339\n1429.618\n\n\n2\n3\n1\n1\n1\n0\n1584.485\n1605.334\n1584.613\n\n\n2\n1\n2\n0\n1\n0\n1412.086\n1432.955\n1412.213\n\n\n2\n1\n2\n0\n1\n1\n1119.529\n1144.571\n1119.706\n\n\n2\n1\n2\n0\n1\n2\n1121.430\n1150.647\n1121.667\n\n\n2\n1\n2\n1\n1\n0\n1281.374\n1306.416\n1281.551\n\n\n2\n1\n2\n1\n1\n1\n1121.434\n1150.651\n1121.671\n\n\n2\n1\n2\n2\n1\n0\n1227.730\n1256.946\n1227.967\n\n\n2\n2\n2\n0\n1\n0\n1430.807\n1451.666\n1430.934\n\n\n2\n2\n2\n0\n1\n1\n1141.894\n1166.924\n1142.072\n\n\n2\n2\n2\n1\n1\n0\n1365.832\n1390.863\n1366.010\n\n\n2\n3\n2\n0\n1\n0\n1506.043\n1526.891\n1506.170\n\n\n2\n1\n3\n0\n1\n0\n1401.231\n1426.274\n1401.409\n\n\n2\n1\n3\n0\n1\n1\n1120.594\n1149.810\n1120.831\n\n\n2\n1\n3\n1\n1\n0\n1283.342\n1312.558\n1283.579\n\n\n2\n2\n3\n0\n1\n0\n1433.213\n1458.244\n1433.391\n\n\n2\n1\n4\n0\n1\n0\n1400.181\n1429.397\n1400.418\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n    p d q P D Q      AIC     BIC     AICc\n214 2 1 1 0 1 1 1117.561 1138.43 1117.687\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n    p d q P D Q      AIC     BIC     AICc\n214 2 1 1 0 1 1 1117.561 1138.43 1117.687\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n    p d q P D Q      AIC     BIC     AICc\n214 2 1 1 0 1 1 1117.561 1138.43 1117.687\n\n\nThe AIC, AICc, and BIC give the same model SARIMA(2,1,1,0,1,1). SARIMA(1,1, 2,0, 1, 1) also has good performance compared to others.\n\nCross validation with 1 step ahead forecosts for SARIMA\nIn this session, a seasonal cross validation using 1 step ahead forecasts and compare the models is performed.\n\n\nCode\nk <- 200 # minimum data length for fitting a model, less than this number will lead to non-stationary fit\nn <- length(df.ts.discharge1980)\n\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- df.ts.discharge1980[1:(k-1)+i] #observations from 1 to 350\n  xtest <- df.ts.discharge1980[k+i] #351th observation as the test set\n  \n  fit <- Arima(xtrain, order=c(2,1,1), seasonal=c(0,1,1))\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,2), seasonal=c(0,1,1))\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n(MAE1=mean(err1)) # This is mean absolute error\n\n\n[1] 0.7558021\n\n\nCode\n(MAE2=mean(err2)) \n\n\n[1] 0.7619506\n\n\nCode\nRMSE1=sqrt(mean(err1^2)) #fit2,1,1,0,1,1\nRMSE2=sqrt(mean(err2^2))#fit1,1,2,0,1,1\n\nRMSE1\n\n\n[1] 0.9324466\n\n\nCode\nRMSE2\n\n\n[1] 0.9406275\n\n\nSARIMA(2,1,1,0,1,1) is the best model based on cross validation, which has lower RMSE of 0.93.\n\n\nModel diagnosis\n\n\nCode\nset.seed(123)\nmodel_output <- capture.output(sarima(df.ts.discharge1980, 2,1,1,0,1,1,12))\ncat(model_output[51:75], model_output[length(model_output)], sep = \"\\n\") \n\n\nCoefficients:\n         ar1     ar2      ma1     sma1\n      0.3751  0.1335  -1.0000  -1.0000\ns.e.  0.0452  0.0454   0.0111   0.0524\n\nsigma^2 estimated as 0.5251:  log likelihood = -553.78,  aic = 1117.56\n\n$degrees_of_freedom\n[1] 476\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.3751 0.0452   8.2902  0.0000\nar2    0.1335 0.0454   2.9416  0.0034\nma1   -1.0000 0.0111 -90.2029  0.0000\nsma1  -1.0000 0.0524 -19.0976  0.0000\n\n$AIC\n[1] 2.328251\n\n$AICc\n[1] 2.328427\n\n$BIC\n[1] 2.371728\n\n\n\n\n\nFigure 13: Final SARIMA model dignosis\n\n\n\n\nThe final model diagnosis (Figure 13) show that p-values of Ljung-box statistic have some pattern left. The ACF of residuals and QQ plot show the residuals are stationary.\n\n\nCompare with Benchmark methods\n\n\nCode\nfit <- Arima(df.ts.discharge1980, order=c(2,1,1), seasonal=c(0,1,1))\n\nautoplot(df.ts.discharge1980) +\n  autolayer(meanf(df.ts.discharge1980, h=24),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(df.ts.discharge1980, h=24),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(df.ts.discharge1980, h=24),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(df.ts.discharge1980, h=24, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,24), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nFigure 14: Benchmark models\n\n\n\n\nFrom Figure 14, seasonal naive seems have better prediction compared to others. And the fitted SARIMA model has less magnitude of the prediction compared to seasonal naive. In the following, RMSE and MAE are used to compare the performance of the two model.\nSummary of Seasonal naive model\n\n\nCode\nf2 <- snaive(df.ts.discharge1980, h=24) \n\naccuracy(f2)\n\n\n                       ME     RMSE       MAE       MPE     MAPE MASE      ACF1\nTraining set -0.006577768 1.172446 0.9249566 -1.322599 12.26289    1 0.4562968\n\n\nSummary of SARIMA(2,1,1,0,1,1).\n\n\nCode\nsummary(fit)\n\n\nSeries: df.ts.discharge1980 \nARIMA(2,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1     ar2      ma1     sma1\n      0.3751  0.1335  -1.0000  -1.0000\ns.e.  0.0452  0.0454   0.0111   0.0524\n\nsigma^2 = 0.5295:  log likelihood = -553.78\nAIC=1117.56   AICc=1117.69   BIC=1138.43\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.02658673 0.7150316 0.5639316 -0.5131699 7.377803 0.6096843\n                     ACF1\nTraining set 0.0005123345\n\n\nThe statistic shows that the final model SARIMA(2,1,1,0,1,1) has lower RMSE and MAE, which is a better model compared to seasonal naive benchmark model."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#sarimax-model",
    "href": "ARIMAX_SARIMAX_VAR.html#sarimax-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMAX model",
    "text": "SARIMAX model\n\n\n\n\nCode\n## the"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#data-preparation",
    "href": "ARIMAX_SARIMAX_VAR.html#data-preparation",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "Data preparation",
    "text": "Data preparation\nIn this session, all the data including stream discharge, turbidity, sediment, precipitation, and temperature are processed. The steps include basic data cleaning, such as filling in missing values, trimming all the data to same time frame, and converting to time series format.\nPlease see the folded code for processing details.\n\n\n\n\n\n\n\nTurbidity (2021-11-24 - 2023-01-28)\nFinal product is a time series from 2021-11-24 to 2022-12-31 with first 6 elements shown below.\n\n\n\nCode\n## read data\nturb <- read.csv(\"./data/turbidity.csv\", header = F, comment.char = \"#\")\nturb <- turb[3:nrow(turb),1:9]\ncolnames(turb) <- c(\"prefix\", \"station\",\"date\",\"max\",\"flag1\",\"min\",\"flag2\",\"mean\",\"flag3\")\nturb$date<-as.Date(turb$date,\"%m/%d/%y\")\n\n## extract daily max\nturb_max<- turb[c(\"date\",\"max\")]\nturb_max$date<-as.Date(turb_max$date,\"%m/%d/%y\")\nturb_max$max = as.numeric(turb_max$max) \n## interpolate the missing data using moving average\nlibrary(imputeTS)\nturb_max$max <- na_ma(turb_max$max, k = 4, weighting = \"exponential\")\n\n# min(turb_max$date);max(turb_max$date)\n\n## extract to 12/31/2022 as the climate max date\nturb_max = turb_max[turb_max$date <= \"2022-12-31\", ]\n# head(turb_max)\n# min(turb_max$date);max(turb_max$date)\n## sanity check\n# str(turb_max)\n# sum(is.na(turb_max))\n\nturb_df <- data.frame(turb_max$date,turb_max$max)\n\n# convert to time series\nturb_ts <- read.zoo(turb_df)\nhead(turb_ts)\n\n\n2021-11-24 2021-11-25 2021-11-26 2021-11-27 2021-11-28 2021-11-29 \n       6.4        6.3        5.9        8.8        5.6        6.0 \n\n\n\n\n\nSediment (1950-04-12 - 2003-09-29)\nFinal product is a time series from 1993-9-29 to 2003-09-29 with first 6 elements shown below.\n\n\n\nCode\n## import and processing data\nsed <- read.csv(\"./data/sediment.csv\", header = F, comment.char = \"#\")\nsed <- sed[3:nrow(sed),3:4]\ncolnames(sed) <- c(\"date\",\"sed\")\nsed$date<-as.Date(sed$date,\"%m/%d/%y\")\nsed$date <- as.Date(ifelse(sed$date > Sys.Date(), \n  format(sed$date, \"19%y-%m-%d\"), \n  format(sed$date)))\n\nsed$sed <- as.numeric(sed$sed)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\nsed$sed <- na_ma(sed$sed, k = 4, weighting = \"exponential\")\n# min(sed$date);max(sed$date)\n\n## extract from 1983-9-29 to 2003-09-29, 20-years of data\nsed = sed[sed$date >= \"1983-9-29\", ]\n\nsed_df <- data.frame(sed$date,sed$sed)\n\n# convert to time series\nsed_ts <- read.zoo(sed_df)\nhead(sed_ts)\n\n\n1983-09-29 1983-09-30 1983-10-01 1983-10-02 1983-10-03 1983-10-04 \n         9         10         18         20         24         22 \n\n\n\n\n\nStream discharge (1940-01-01 - 2023-02-14)\n\n\nFor turbidity model, 2021-11-24 to 2022-12-31, the first 6 elements are shown below.\n\n\nCode\n## import and processing data\ndischarge <- read.csv(\"./data/discharge.csv\", header = F, comment.char = \"#\")\ndischarge <- discharge[3:dim(discharge)[1],]\ncolnames(discharge) <- c(\"prefix\", \"station\",\"date\",\"discharge\",\"flag\")\ndischarge <- discharge[c(\"date\",\"discharge\")]\ndischarge$date<-as.Date(discharge$date,\"%m/%d/%y\")\ndischarge$date <- as.Date(ifelse(discharge$date > Sys.Date(), \n  format(discharge$date, \"19%y-%m-%d\"), \n  format(discharge$date)))\n\n# min(discharge$date);max(discharge$date)\n\n### discharge for turbidity model, 2021-11-24 to 2022-12-31\n\n# select data after 2021-11-24 and before 2022-12-31\ndischarge_turb <- discharge[(discharge$date >= \"2021-11-24\") & (discharge$date <= \"2022-12-31\"), ]\n\n# change to numeric\ndischarge_turb$discharge <- as.numeric(discharge_turb$discharge)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\ndischarge_turb$discharge <- na_ma(discharge_turb$discharge, k = 4, weighting = \"exponential\")\n\n## sanity check\n# str(discharge_turb)\n# sum(is.na(discharge_turb))\n\ndischarge_turb_df <- data.frame(discharge_turb$date,discharge_turb$discharge)\n\n# convert to time series\ndischarge_turb_ts <- read.zoo(discharge_turb_df)\nhead(discharge_turb_ts)\n\n\n2021-11-24 2021-11-25 2021-11-26 2021-11-27 2021-11-28 2021-11-29 \n      2470       2480       2280       2260       2510       2250 \n\n\n\n\nFor sediment model, from 1983-9-29 to 2003-09-29 , the first 6 elements are shown below.\n\n\nCode\n### discharge for sediment model, 1983-9-29 to 2003-09-29\n\n# select data after 1983-9-29  and before 2003-09-29\ndischarge_sed <- discharge[(discharge$date >= \"1983-9-29\") & (discharge$date <= \"2003-09-29\"), ]\n\n# change to numeric\ndischarge_sed$discharge <- as.numeric(discharge_sed$discharge)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\ndischarge_sed$discharge <- na_ma(discharge_sed$discharge, k = 4, weighting = \"exponential\")\n\n## sanity check\n# str(discharge_turb)\n# sum(is.na(discharge_turb))\n\ndischarge_sed_df <- data.frame(discharge_sed$date,discharge_sed$discharge)\n\n# convert to time series\ndischarge_sed_ts <- read.zoo(discharge_sed_df)\nhead(discharge_sed_ts)\n\n\n1983-09-29 1983-09-30 1983-10-01 1983-10-02 1983-10-03 1983-10-04 \n       190        220        210        210        210        205 \n\n\n\n\n\nPrecipitation and temperature (1940-01-01 - 2022-12-31)\n\n\nFor turbidity model, 2021-11-24 to 2022-12-31, the first 6 elements are shown below.\n\nPrecipitation\n\n\nCode\n## read data and aggregate\ndf <- data.frame(matrix(ncol = 6, nrow = 0))\ncolnames(df) <- c(\"station\",\"name\",\"date\",\"prcp\",\"tmax\",\"tmin\")\nfor (i in 1:3){\n  file_name <- paste0(\"climate\",i,\".csv\")\n  df_curr <- read.csv(paste0(\"./data/\",file_name), header = T)\n  colnames(df_curr) <- c(\"station\",\"name\",\"date\",\"prcp\",\"tmax\",\"tmin\")\n  df <- rbind(df_curr,df)\n}\n\n## group by date: each day, we have one average prcp, tmax, and tmin based on all stations in Toledo\n\nclimate <- df %>%\n  group_by(date) %>%\n  summarise_at(vars(prcp,tmax,tmin),  mean, na.rm = TRUE)\n\n\nclimate$date<-as.Date(climate$date,\"%Y-%m-%d\")\n## sanity check\n# any(duplicated(climate$date))\n# min(climate$date);max(climate$date)\n\n### climate for turbidity model, 2021-11-24 to 2022-12-31\n\n# select data after 2021-11-24 and before 2022-12-31\nclimate_trub <- climate[(climate$date >= \"2021-11-24\") & (climate$date <= \"2022-12-31\"), ]\n\n## interpolate the missing data using moving average\nclimate_trub$prcp <- na_ma(climate_trub$prcp, k = 4, weighting = \"exponential\")\nclimate_trub$tmax <- na_ma(climate_trub$tmax, k = 4, weighting = \"exponential\")\nclimate_trub$tmin <- na_ma(climate_trub$tmin, k = 4, weighting = \"exponential\")\n\n## sanity check\n#str(climate_trub)\n#sum(is.na(climate_trub))\n#min(climate_trub$date);max(climate_trub$date)\n\n## prcp \nprcp_trub_df <- data.frame(climate_trub$date,climate_trub$prcp)\n\n# convert to time series\nprcp_trub_ts <- read.zoo(prcp_trub_df)\nhead(prcp_trub_ts)\n\n\n 2021-11-24  2021-11-25  2021-11-26  2021-11-27  2021-11-28  2021-11-29 \n0.002142857 0.133750000 0.068666667 0.009285714 0.078666667 0.001538462 \n\n\n\n\nMaximum temperature\n\n\nCode\n## tmax\ntmax_trub_df <- data.frame(climate_trub$date,climate_trub$tmax)\n\n# convert to time series\ntmax_trub_ts <- read.zoo(tmax_trub_df)\nhead(tmax_trub_ts)\n\n\n2021-11-24 2021-11-25 2021-11-26 2021-11-27 2021-11-28 2021-11-29 \n  45.33333   50.00000   43.66667   34.00000   39.33333   41.66667 \n\n\n\n\nMinimum temperature\n\n\nCode\n## tmin\ntmin_trub_df <- data.frame(climate_trub$date,climate_trub$tmin)\n\n# convert to time series\ntmin_trub_ts <- read.zoo(tmin_trub_df)\nhead(tmin_trub_ts)\n\n\n2021-11-24 2021-11-25 2021-11-26 2021-11-27 2021-11-28 2021-11-29 \n  23.33333   33.00000   27.66667   25.00000   29.66667   23.33333 \n\n\n\n\n\nFor sediment model, from 1983-9-29 to 2003-09-29 , the first 6 elements are shown below.\n\nPrecipitation\n\n\nCode\n### climate for sediment model, 1983-9-29 to 2003-09-29\n\n# select data after 1983-9-29 and before 2003-09-29 \nclimate_sed <- climate[(climate$date >= \"1983-9-29\") & (climate$date <= \"2003-09-29\"), ]\n\n## interpolate the missing data using moving average\nclimate_sed$prcp <- na_ma(climate_sed$prcp, k = 4, weighting = \"exponential\")\nclimate_sed$tmax <- na_ma(climate_sed$tmax, k = 4, weighting = \"exponential\")\nclimate_sed$tmin <- na_ma(climate_sed$tmin, k = 4, weighting = \"exponential\")\n\n## sanity check\n#str(climate_sed)\n#sum(is.na(climate_sed))\n#min(climate_sed$date);max(climate_sed$date)\n\n## prcp \nprcp_sed_df <- data.frame(climate_sed$date,climate_sed$prcp)\n\n# convert to time series\nprcp_sed_ts <- read.zoo(prcp_sed_df)\nhead(prcp_sed_ts)\n\n\n1983-09-29 1983-09-30 1983-10-01 1983-10-02 1983-10-03 1983-10-04 \n      0.00       0.00       0.00       0.00       0.00       0.01 \n\n\n\n\nMaximum temperature\n\n\nCode\n## tmin\ntmax_sed_df <- data.frame(climate_sed$date,climate_sed$tmax)\n\n# convert to time series\ntmax_sed_ts <- read.zoo(tmax_sed_df)\nhead(tmax_sed_ts)\n\n\n1983-09-29 1983-09-30 1983-10-01 1983-10-02 1983-10-03 1983-10-04 \n      77.0       75.5       71.0       81.5       84.0       77.0 \n\n\n\n\nMinimum temperature\n\n\nCode\n## tmin\ntmin_sed_df <- data.frame(climate_sed$date,climate_sed$tmin)\n\n# convert to time series\ntmin_sed_ts <- read.zoo(tmin_sed_df)\nhead(tmin_sed_ts)\n\n\n1983-09-29 1983-09-30 1983-10-01 1983-10-02 1983-10-03 1983-10-04 \n      50.5       51.5       50.0       48.0       58.5       63.5"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#sarimax-model-of-turbidity",
    "href": "ARIMAX_SARIMAX_VAR.html#sarimax-model-of-turbidity",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "SARIMAX model of turbidity",
    "text": "SARIMAX model of turbidity\n\nPlotting the Data\n(tab-df?) shows the data sample for turbidity model. Figure 1 shows the time series plots of the variables.\n\n\nCode\nturbidity_model_df <- data.frame(turb_df,discharge_turb_df$discharge_turb.discharge,prcp_trub_df$climate_trub.prcp,tmax_trub_df$climate_trub.tmax,tmin_trub_df$climate_trub.tmin)\n\ncolnames(turbidity_model_df)<-c(\"date\",\"turbidity\",\"discharge\",\"precipitation\",\"tmax\",\"tmin\")\n\nknitr::kable(head(turbidity_model_df))\n\n\n\n\n\ndate\nturbidity\ndischarge\nprecipitation\ntmax\ntmin\n\n\n\n\n2021-11-24\n6.4\n2470\n0.0021429\n45.33333\n23.33333\n\n\n2021-11-25\n6.3\n2480\n0.1337500\n50.00000\n33.00000\n\n\n2021-11-26\n5.9\n2280\n0.0686667\n43.66667\n27.66667\n\n\n2021-11-27\n8.8\n2260\n0.0092857\n34.00000\n25.00000\n\n\n2021-11-28\n5.6\n2510\n0.0786667\n39.33333\n29.66667\n\n\n2021-11-29\n6.0\n2250\n0.0015385\n41.66667\n23.33333\n\n\n\n\n\n\n\nCode\nturbidity_model.ts<-ts(turbidity_model_df,star=decimal_date(as.Date(\"2021-11-24\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\nautoplot(turbidity_model.ts[,c(2:6)], facets=TRUE) +\n  xlab(\"Date\") + ylab(\"\") +\n  ggtitle(\"Variables influencing turbidity\")\n\n\n\n\n\nFigure 1: Time series of variables influencing sediment\n\n\n\n\nFrom SARIMA model session for stream discharge, all stream discharge need log transformation. Log transfor for precipitation would introduce NA, thus, the precipitation would stay the original form.\n\n\nCode\nlg.turbidity_model <- turbidity_model.ts[,2:6] #making a copy\nlg.turbidity_model[,1]<-log(turbidity_model_df$turbidity)\nlg.turbidity_model[,2]<-log(turbidity_model_df$discharge)\n\n\nAfter log transformation, the time series of the variables are shown in Figure 2.\n\n\nCode\nlg.turbidity_model.ts<-ts(lg.turbidity_model,star=decimal_date(as.Date(\"2021-11-24\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\nautoplot(lg.turbidity_model.ts, facets=TRUE) +\n  xlab(\"Date\") + ylab(\"\") +\n  ggtitle(\"Variables influencing turbidity\")\n\n\n\n\n\nFigure 2: Time series of variables influencing sediment after log transformation\n\n\n\n\n\n\nFitting the model using auto.arima()\nThe model using auto.arima() is shown below.\n\n\nCode\nxreg <- cbind(discharge = lg.turbidity_model.ts[, \"discharge\"],\n              precipitation = lg.turbidity_model.ts[, \"precipitation\"],\n              tmax = lg.turbidity_model.ts[, \"tmax\"],\n              tmin = lg.turbidity_model.ts[, \"tmin\"])\n\nfit <- auto.arima(lg.turbidity_model.ts[, \"turbidity\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: lg.turbidity_model.ts[, \"turbidity\"] \nRegression with ARIMA(1,0,1) errors \n\nCoefficients:\n         ar1      ma1  discharge  precipitation    tmax    tmin\n      0.8948  -0.2850     0.4246         0.0878  0.0006  0.0022\ns.e.  0.0282   0.0587     0.0233         0.0956  0.0037  0.0046\n\nsigma^2 = 0.1824:  log likelihood = -226.54\nAIC=467.08   AICc=467.36   BIC=495.07\n\nTraining set error measures:\n                     ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.01048555 0.4239335 0.269546 -1.380152 8.165318 0.1412965\n                    ACF1\nTraining set -0.01060533\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,1) errors\nQ* = 59.777, df = 79, p-value = 0.9474\n\nModel df: 2.   Total lags used: 81\n\n\n\n\n\nFigure 3: Residual diagonosis for auto.arima() model\n\n\n\n\nFigure 3 shows that the fitting model is an ARIMA(1,0,1) model. The ACF plot looks alright, all the errors are less than the threshold.\n\n\nFitting the model manually\n\nLinear regression model\n\n\n\nCode\nfit.reg <- lm(turbidity ~ precipitation+tmax+tmin+discharge, data=lg.turbidity_model.ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = turbidity ~ precipitation + tmax + tmin + discharge, \n    data = lg.turbidity_model.ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.66757 -0.46131  0.02679  0.43976  2.11160 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.2352589  0.2432210  -5.079 5.85e-07 ***\nprecipitation  0.1325368  0.1759714   0.753    0.452    \ntmax           0.0054724  0.0051649   1.060    0.290    \ntmin           0.0005487  0.0059018   0.093    0.926    \ndischarge      0.5599929  0.0252455  22.182  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6838 on 398 degrees of freedom\nMultiple R-squared:  0.5603,    Adjusted R-squared:  0.5559 \nF-statistic: 126.8 on 4 and 398 DF,  p-value: < 2.2e-16\n\n\n\nResidual fit\n\nFigure 4 shows the ACF and PACF plots for the residuals. The ACF has a fading pattern, which indicates that differencing may be needed.\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2021-11-24\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\n############## Then look at the residuals ############\nacf(res.fit)\nPacf(res.fit)\n\n\n\n\n\n\n\n\n(a) ACF plot for residuals of linear regression\n\n\n\n\n\n\n\n(b) PACF plot for residuals of linear regression\n\n\n\n\nFigure 4: ACF and PACF plots for SARIMAX model\n\n\n\n\n\nFirst differencing\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\nFigure 5: Residual diagonosis after linear regression model and differencing\n\n\n\n\nFigure 5 shows that p=1,2; q=1.\n\n\nSecond differencing with seasonality (annually, Figure 6).\n\n\nCode\nres.fit %>% diff() %>% diff(365) %>% ggtsdisplay()\n\n\n\n\n\nFigure 6: Residual diagonosis after linear regression model and second differencing with annually seasonality\n\n\n\n\n\nFinding the model parameters\n\n\n\nCode\n######################## Check for different combinations ########\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,d1,d2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(d2)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*48),nrow=48)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(d in d1:d2){\n        \n        for(P in P1:P2)\n        {\n          for(Q in Q1:Q2)\n          {\n            if(p+d+q+P+Q+D<=12)\n            {\n              skip_to_next <- FALSE\n              model<- tryCatch({\n      Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n    }, error = function(e) { skip_to_next <<- TRUE})\n  if(skip_to_next) { next }\n              ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n              i=i+1\n              #print(i)\n              }\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\nAll AIC, BIC, and AICc give the best model as ARIMA(1,2,1,0,1,0)[365] for residual.\n\n# q=0,1,; Q=1,2 and PACF plot: p=0,1,2; P=1,2, D=1 and d=0,1,2\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=2,d1=1,d2=3,P1=1,P2=3,Q1=1,Q2=3,data=res.fit)\n#output\n\nknitr::kable(output)\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n97.25568\n98.86660\n97.36997\n\n\n0\n2\n0\n0\n1\n0\n128.65527\n130.23879\n128.77292\n\n\n0\n3\n0\n0\n1\n0\n173.77761\n175.33296\n173.89883\n\n\n0\n1\n1\n0\n1\n0\n92.99954\n96.22138\n93.35248\n\n\n0\n2\n1\n0\n1\n0\n91.22853\n94.39557\n91.59217\n\n\n0\n3\n1\n0\n1\n0\n127.30644\n130.41713\n127.68144\n\n\n1\n1\n0\n0\n1\n0\n92.06895\n95.29078\n92.42189\n\n\n1\n2\n0\n0\n1\n0\n102.87068\n106.03772\n103.23432\n\n\n1\n3\n0\n0\n1\n0\n132.89719\n136.00788\n133.27219\n\n\n1\n1\n1\n0\n1\n0\n93.60573\n98.43849\n94.33301\n\n\n1\n2\n1\n0\n1\n0\n86.63715\n91.38771\n87.38715\n\n\n1\n3\n1\n0\n1\n0\n102.32171\n106.98775\n103.09590\n\n\n2\n1\n0\n0\n1\n0\n93.87314\n98.70589\n94.60041\n\n\n2\n2\n0\n0\n1\n0\n102.36060\n107.11116\n103.11060\n\n\n2\n3\n0\n0\n1\n0\n126.33729\n131.00334\n127.11149\n\n\n2\n1\n1\n0\n1\n0\n95.58437\n102.02804\n96.83437\n\n\n2\n2\n1\n0\n1\n0\n88.34723\n94.68131\n89.63755\n\n\n2\n3\n1\n0\n1\n0\n101.89712\n108.11852\n103.23046\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\noutput[which.min(output$AIC),] \n\n   p d q P D Q      AIC      BIC     AICc\n11 1 2 1 0 1 0 86.63715 91.38771 87.38715\n\noutput[which.min(output$BIC),]\n\n   p d q P D Q      AIC      BIC     AICc\n11 1 2 1 0 1 0 86.63715 91.38771 87.38715\n\noutput[which.min(output$AICc),]\n\n   p d q P D Q      AIC      BIC     AICc\n11 1 2 1 0 1 0 86.63715 91.38771 87.38715\n\n\nHowever, the turbidity data has only over 400 observations. Here. I show best model is shown below with ARIMA(1,2,1,0,1,0) .\n\n\nCode\nset.seed(1234)\n\nmodel_output12 <- capture.output(sarima(res.fit, 1,2,1, 0,1,0,0)) \n\n\n\n\n\nThe Ljun-Box statistic and Q-Q plot look ok, but ACF plot could be better. This indicate the analysis need more data to improve the model.\n\n\nCross validation\n\nn=length(res.fit)\nk=200\n \n #n-k=203\n\n\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- res.fit[1:(k-1)+i] #observations from 1 to 350\n  xtest <- res.fit[k+i] #351th observation as the test set\n  \n  fit <- Arima(xtrain, order=c(1,0,1),\n                include.drift=FALSE, method=\"ML\")\n  fcast1 <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,2,1), seasonal=c(0,1,0),\n                include.drift=FALSE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n(MAE1=mean(err1)) # This is mean absolute error\n\n[1] 0.2824119\n\n(MAE2=mean(err2)) \n\n[1] 0.304012\n\nRMSE1=sqrt(mean(err1^2)) #fit1,0,1,\nRMSE2=sqrt(mean(err2^2))#fit1,2,1,0,1,0\n\nRMSE1\n\n[1] 0.4392394\n\nRMSE2\n\n[1] 0.4838416\n\n\nBoth MAE and RMSE suggest the manually fitted model, ARIMA(1,2,1,0,1,0) is better than the auto.arima() model, ARIMA(1,0,1).\n\n\nBest model\n\nfit <- Arima(lg.turbidity_model.ts[, \"turbidity\"], order=c(1,2,1),seasonal = c(0,1,0), xreg = xreg)\nsummary(fit)\n\nSeries: lg.turbidity_model.ts[, \"turbidity\"] \nRegression with ARIMA(1,2,1)(0,1,0)[365] errors \n\nCoefficients:\n          ar1      ma1  discharge  precipitation     tmax     tmin\n      -0.4437  -0.9995     0.5288         0.4683  -0.0003  -0.0088\ns.e.   0.1576   0.0387     0.2065         0.3360   0.0124   0.0160\n\nsigma^2 = 0.6045:  log likelihood = -39.19\nAIC=92.38   AICc=96.38   BIC=103.46\n\nTraining set error measures:\n                       ME      RMSE        MAE        MPE     MAPE       MASE\nTraining set -0.001783241 0.2121265 0.04570337 -0.3000952 1.606162 0.02395778\n                    ACF1\nTraining set 0.008323962\n\n\n\\[\n(1 + 0.4437B^1 + B^2) * (1 - B^{365}) * discharge(t) = 0.5288 * discharge(t) + 0.4683 * precipitation(t) - 0.0003 * tmax(t) - 0.0088 * tmin(t) - 0.9995\\omega(t-1) * \\omega(t)\n\\]\n\n\nForcasting\n\nTurbidity\n\n\n\nCode\ndischarge_fit<-auto.arima(lg.turbidity_model[,\"discharge\"]) #fiting an ARIMA model to the Export variable\nsummary(discharge_fit) \n\n\nSeries: lg.turbidity_model[, \"discharge\"] \nARIMA(1,1,3) \n\nCoefficients:\n         ar1      ma1      ma2      ma3\n      0.7870  -0.4365  -0.3785  -0.1139\ns.e.  0.0576   0.0730   0.0527   0.0600\n\nsigma^2 = 0.115:  log likelihood = -134.08\nAIC=278.16   AICc=278.31   BIC=298.15\n\nTraining set error measures:\n                      ME      RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.01051251 0.3370327 0.237908 -0.3106121 3.198979 0.1109779\n                     ACF1\nTraining set 3.796219e-05\n\n\nCode\nfdisc<-forecast(discharge_fit,20)\n\n\n\nPrecipitation\n\n\n\nCode\nprecipitation_fit<-auto.arima(lg.turbidity_model[,\"precipitation\"]) #fiting an ARIMA model to the Export variable\nsummary(precipitation_fit) \n\n\nSeries: lg.turbidity_model[, \"precipitation\"] \nARIMA(0,0,1) with non-zero mean \n\nCoefficients:\n         ma1    mean\n      0.1524  0.0924\ns.e.  0.0510  0.0115\n\nsigma^2 = 0.04005:  log likelihood = 77.5\nAIC=-149   AICc=-148.94   BIC=-137.01\n\nTraining set error measures:\n                       ME      RMSE       MAE  MPE MAPE      MASE        ACF1\nTraining set 4.109928e-05 0.1996324 0.1246119 -Inf  Inf 0.7434769 -0.00235585\n\n\nCode\nfprcp<-forecast(precipitation_fit,20)\n\n\n\nTmax\n\n\n\nCode\ntmax_fit<-auto.arima(lg.turbidity_model[,\"tmax\"]) #fiting an ARIMA model to the Export variable\nsummary(tmax_fit) \n\n\nSeries: lg.turbidity_model[, \"tmax\"] \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1      ma2\n      0.6389  -0.2293  -0.5212  -0.2855\ns.e.  0.1075   0.0963   0.1069   0.1019\n\nsigma^2 = 37.76:  log likelihood = -1298.71\nAIC=2607.42   AICc=2607.57   BIC=2627.4\n\nTraining set error measures:\n                     ME     RMSE     MAE       MPE     MAPE      MASE\nTraining set -0.0507898 6.106834 4.42749 -3.117993 11.34176 0.3541992\n                     ACF1\nTraining set 0.0003491482\n\n\nCode\nftmax<-forecast(tmax_fit,20)\n\n\n\nTmin\n\n\n\nCode\ntmin_fit<-auto.arima(lg.turbidity_model[,\"tmin\"]) #fiting an ARIMA model to the Export variable\nsummary(tmin_fit) \n\n\nSeries: lg.turbidity_model[, \"tmin\"] \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1      ma2\n      0.5618  -0.1251  -0.5784  -0.2393\ns.e.  0.1661   0.1353   0.1648   0.1541\n\nsigma^2 = 32.33:  log likelihood = -1267.38\nAIC=2544.76   AICc=2544.91   BIC=2564.74\n\nTraining set error measures:\n                      ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.005936972 5.650462 4.257408 5.316034 27.88954 0.4737379\n                     ACF1\nTraining set -0.001209705\n\n\nCode\nftmin<-forecast(tmin_fit,20)\n\n\n4 Forecast\n\n\nCode\nfxreg <- cbind(discharge = fdisc$mean,\n              precipitation = fprcp$mean,\n              tmax = ftmax$mean,\n              tmin = ftmin$mean)\n\nfturb <- forecast(fit, xreg=fxreg) \nautoplot(fturb) + xlab(\"Year\") +\n  ylab(\"Turbidity\")\n\n\n\n\n\n\n\nConclusion\nThe manually fitted model shows that the turbidity will be decrease in the near future based on stream discharge, precipitation, tmax, tmin. This agrees with the stream discharge annual pattern: lower in fall and winter, higher in spring due to snow melting and higher precipitation. However, the annual pattern may not be captured well, because that the sample size is small. Thus, this model need to be verified with larger dataset in the future."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#var-model",
    "href": "ARIMAX_SARIMAX_VAR.html#var-model",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "VAR model",
    "text": "VAR model\nThe variables include precipitation, stream discharge, precipitation. Because I want to explore the longer lag relationship among them, I will use monthly data here.\n\n\nCode\nsed_model_df <- data.frame(sed_df,discharge_sed$discharge, prcp_sed_ts)\n\ncolnames(sed_model_df)<-c(\"date\",\"sediment\",\"discharge\",\"precipitation\")\n\n\n### can get monthly data\n# Get mean value for each month\n\n# take log of the discharge\nsed_model_df[\"discharge\"] <- log(sed_model_df[\"discharge\"])\n\nmean_data <- sed_model_df %>% \n  mutate(month = month(sed_model_df$date), year = year(sed_model_df$date)) %>% \n  group_by(year, month) %>% \n  summarise_at(vars(c(\"sediment\",\"discharge\",\"precipitation\")), list(mean = mean))\n\n\nvar_month<-ts(mean_data[3:5],star=decimal_date(as.Date(\"1983-09-29\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\n\n\n\nautoplot(var_month[,c(1:3)], facets=TRUE) +\n  xlab(\"Time\") + ylab(\"\") +\n  ggtitle(\"Monthly variables\")\n\n\n\n\n\n\n\nCode\npairs(var_month)\n\n\n\n\n\n\n\nCode\n###### select p #########        \nVARselect(var_month, lag.max=12, type=\"both\") ## largest annualy\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     8      3      1      8 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n) 0.7481091 0.7152077 0.5944947 0.5450676 0.5391289 0.4443612 0.4271302\nHQ(n)  0.8388459 0.8603866 0.7941157 0.7991307 0.8476341 0.8073084 0.8445196\nSC(n)  0.9730254 1.0750738 1.0893105 1.1748332 1.3038443 1.3440263 1.4617452\nFPE(n) 2.1130447 2.0447859 1.8125173 1.7255152 1.7159209 1.5615824 1.5359804\n               8         9       10        11        12\nAIC(n) 0.4008526 0.4433351 0.472206 0.4535587 0.4633184\nHQ(n)  0.8726840 0.9696087 1.052922 1.0887164 1.1529182\nSC(n)  1.5704173 1.7478496 1.911670 2.0279727 2.1726822\nFPE(n) 1.4975083 1.5642969 1.612407 1.5853433 1.6041771\n\n\nCode\n#summary(VAR(var_month, p=1, type='both')) # 'both' fits constant + trend\n\n\nThe results show p=1, 3 and 8 are with good SC, HQ, or AIC/FPE.\n\nVAR(1)\n\n\nCode\nsummary(vars::VAR(var_month, p=1, type='both')) # 'both' fits constant + trend\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: sediment_mean, discharge_mean, precipitation_mean \nDeterministic variables: both \nSample size: 240 \nLog Likelihood: -1105.261 \nRoots of the characteristic polynomial:\n0.642 0.2904 0.1438\nCall:\nvars::VAR(y = var_month, p = 1, type = \"both\")\n\n\nEstimation results for equation sediment_mean: \n============================================== \nsediment_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)   \nsediment_mean.l1         0.21918    0.08914   2.459   0.0147 * \ndischarge_mean.l1       15.81294    4.85883   3.254   0.0013 **\nprecipitation_mean.l1 -184.89650   85.00729  -2.175   0.0306 * \nconst                  -61.10324   35.63190  -1.715   0.0877 . \ntrend                    0.10024    0.05492   1.825   0.0692 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 57.1 on 235 degrees of freedom\nMultiple R-Squared: 0.1888, Adjusted R-squared: 0.175 \nF-statistic: 13.68 on 4 and 235 DF,  p-value: 4.852e-10 \n\n\nEstimation results for equation discharge_mean: \n=============================================== \ndischarge_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)    \nsediment_mean.l1      -0.0034489  0.0013087  -2.635  0.00896 ** \ndischarge_mean.l1      0.7729023  0.0713339  10.835  < 2e-16 ***\nprecipitation_mean.l1 -2.0939191  1.2480161  -1.678  0.09472 .  \nconst                  2.1936310  0.5231221   4.193  3.9e-05 ***\ntrend                  0.0001387  0.0008063   0.172  0.86354    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8383 on 235 degrees of freedom\nMultiple R-Squared: 0.4004, Adjusted R-squared: 0.3902 \nF-statistic: 39.24 on 4 and 235 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation precipitation_mean: \n=================================================== \nprecipitation_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)   \nsediment_mean.l1       8.486e-05  7.686e-05   1.104   0.2707   \ndischarge_mean.l1     -1.225e-03  4.190e-03  -0.292   0.7703   \nprecipitation_mean.l1  8.411e-02  7.330e-02   1.148   0.2523   \nconst                  9.280e-02  3.072e-02   3.021   0.0028 **\ntrend                 -3.091e-05  4.736e-05  -0.653   0.5145   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.04923 on 235 degrees of freedom\nMultiple R-Squared: 0.02277,    Adjusted R-squared: 0.006138 \nF-statistic: 1.369 on 4 and 235 DF,  p-value: 0.2454 \n\n\n\nCovariance matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean           3260.098       33.46433           1.344236\ndischarge_mean            33.464        0.70268           0.017749\nprecipitation_mean         1.344        0.01775           0.002424\n\nCorrelation matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean             1.0000         0.6992             0.4782\ndischarge_mean            0.6992         1.0000             0.4301\nprecipitation_mean        0.4782         0.4301             1.0000\n\n\n\n\nVAR(3)\n\n\nCode\nsummary(vars::VAR(var_month, p=3, type='both')) \n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: sediment_mean, discharge_mean, precipitation_mean \nDeterministic variables: both \nSample size: 238 \nLog Likelihood: -1058.944 \nRoots of the characteristic polynomial:\n0.7776 0.7776 0.725 0.591 0.4909 0.4909 0.4119 0.4119 0.272\nCall:\nvars::VAR(y = var_month, p = 3, type = \"both\")\n\n\nEstimation results for equation sediment_mean: \n============================================== \nsediment_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)   \nsediment_mean.l1         0.09740    0.09174   1.062  0.28952   \ndischarge_mean.l1       15.23421    5.99443   2.541  0.01171 * \nprecipitation_mean.l1 -124.09096   84.52566  -1.468  0.14347   \nsediment_mean.l2         0.16936    0.09667   1.752  0.08112 . \ndischarge_mean.l2        8.59342    7.28602   1.179  0.23946   \nprecipitation_mean.l2 -243.88524   85.65124  -2.847  0.00481 **\nsediment_mean.l3         0.21419    0.09470   2.262  0.02466 * \ndischarge_mean.l3      -12.73707    6.13465  -2.076  0.03900 * \nprecipitation_mean.l3 -246.08273   84.46078  -2.914  0.00393 **\nconst                   -0.11040   42.65437  -0.003  0.99794   \ntrend                    0.06540    0.05453   1.199  0.23160   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 54.01 on 227 degrees of freedom\nMultiple R-Squared: 0.2885, Adjusted R-squared: 0.2572 \nF-statistic: 9.206 on 10 and 227 DF,  p-value: 9.451e-13 \n\n\nEstimation results for equation discharge_mean: \n=============================================== \ndischarge_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)    \nsediment_mean.l1      -4.097e-03  1.360e-03  -3.012 0.002893 ** \ndischarge_mean.l1      7.648e-01  8.889e-02   8.604 1.28e-15 ***\nprecipitation_mean.l1 -1.069e+00  1.253e+00  -0.853 0.394548    \nsediment_mean.l2       1.430e-03  1.433e-03   0.997 0.319616    \ndischarge_mean.l2      2.069e-01  1.080e-01   1.915 0.056755 .  \nprecipitation_mean.l2 -4.415e+00  1.270e+00  -3.476 0.000609 ***\nsediment_mean.l3       7.368e-04  1.404e-03   0.525 0.600335    \ndischarge_mean.l3     -2.912e-01  9.097e-02  -3.201 0.001565 ** \nprecipitation_mean.l3 -2.050e-01  1.252e+00  -0.164 0.870161    \nconst                  3.141e+00  6.325e-01   4.965 1.35e-06 ***\ntrend                 -1.043e-05  8.086e-04  -0.013 0.989718    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8009 on 227 degrees of freedom\nMultiple R-Squared: 0.4676, Adjusted R-squared: 0.4441 \nF-statistic: 19.94 on 10 and 227 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation precipitation_mean: \n=================================================== \nprecipitation_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)\nsediment_mean.l1       8.239e-05  8.281e-05   0.995    0.321\ndischarge_mean.l1     -7.274e-03  5.411e-03  -1.344    0.180\nprecipitation_mean.l1  1.145e-01  7.630e-02   1.501    0.135\nsediment_mean.l2      -5.021e-05  8.726e-05  -0.575    0.566\ndischarge_mean.l2      8.018e-03  6.577e-03   1.219    0.224\nprecipitation_mean.l2 -2.788e-02  7.731e-02  -0.361    0.719\nsediment_mean.l3       1.935e-05  8.548e-05   0.226    0.821\ndischarge_mean.l3      5.376e-03  5.537e-03   0.971    0.333\nprecipitation_mean.l3 -9.352e-02  7.624e-02  -1.227    0.221\nconst                  4.405e-02  3.850e-02   1.144    0.254\ntrend                 -1.272e-05  4.922e-05  -0.259    0.796\n\n\nResidual standard error: 0.04875 on 227 degrees of freedom\nMultiple R-Squared: 0.05668,    Adjusted R-squared: 0.01513 \nF-statistic: 1.364 on 10 and 227 DF,  p-value: 0.1982 \n\n\n\nCovariance matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean           2916.663       30.02468           1.271526\ndischarge_mean            30.025        0.64136           0.018059\nprecipitation_mean         1.272        0.01806           0.002376\n\nCorrelation matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean             1.0000         0.6942             0.4830\ndischarge_mean            0.6942         1.0000             0.4626\nprecipitation_mean        0.4830         0.4626             1.0000\n\n\n\n\nVAR(8)\n\n\nCode\nsummary(vars::VAR(var_month, p=8, type='both')) \n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: sediment_mean, discharge_mean, precipitation_mean \nDeterministic variables: both \nSample size: 233 \nLog Likelihood: -956.712 \nRoots of the characteristic polynomial:\n0.979 0.979 0.8923 0.8331 0.8331 0.7842 0.7541 0.7541 0.7476 0.7476 0.7376 0.7376 0.7238 0.7109 0.7109 0.6854 0.6854 0.6632 0.6632 0.5744 0.5744 0.3774 0.3774 0.2044\nCall:\nvars::VAR(y = var_month, p = 8, type = \"both\")\n\n\nEstimation results for equation sediment_mean: \n============================================== \nsediment_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + sediment_mean.l4 + discharge_mean.l4 + precipitation_mean.l4 + sediment_mean.l5 + discharge_mean.l5 + precipitation_mean.l5 + sediment_mean.l6 + discharge_mean.l6 + precipitation_mean.l6 + sediment_mean.l7 + discharge_mean.l7 + precipitation_mean.l7 + sediment_mean.l8 + discharge_mean.l8 + precipitation_mean.l8 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)   \nsediment_mean.l1       4.184e-02  9.738e-02   0.430  0.66792   \ndischarge_mean.l1      1.009e+01  7.470e+00   1.350  0.17837   \nprecipitation_mean.l1 -2.040e+01  9.835e+01  -0.207  0.83592   \nsediment_mean.l2       1.477e-01  1.012e-01   1.460  0.14582   \ndischarge_mean.l2      6.393e+00  7.970e+00   0.802  0.42342   \nprecipitation_mean.l2 -1.228e+02  9.645e+01  -1.273  0.20445   \nsediment_mean.l3       1.086e-01  1.020e-01   1.065  0.28826   \ndischarge_mean.l3     -3.570e-02  7.898e+00  -0.005  0.99640   \nprecipitation_mean.l3 -2.420e+02  9.181e+01  -2.636  0.00902 **\nsediment_mean.l4       1.238e-01  1.012e-01   1.223  0.22276   \ndischarge_mean.l4     -4.606e+00  7.621e+00  -0.604  0.54621   \nprecipitation_mean.l4 -9.518e+01  9.199e+01  -1.035  0.30206   \nsediment_mean.l5       2.048e-01  1.006e-01   2.035  0.04308 * \ndischarge_mean.l5     -9.153e+00  7.511e+00  -1.219  0.22439   \nprecipitation_mean.l5 -5.019e+01  9.248e+01  -0.543  0.58792   \nsediment_mean.l6       6.143e-03  1.005e-01   0.061  0.95130   \ndischarge_mean.l6     -4.396e+00  7.712e+00  -0.570  0.56933   \nprecipitation_mean.l6  1.003e+02  9.256e+01   1.083  0.28002   \nsediment_mean.l7      -7.048e-03  9.992e-02  -0.071  0.94383   \ndischarge_mean.l7     -2.602e+00  7.752e+00  -0.336  0.73752   \nprecipitation_mean.l7  7.137e+01  1.007e+02   0.709  0.47937   \nsediment_mean.l8       2.015e-02  9.765e-02   0.206  0.83675   \ndischarge_mean.l8      2.057e+00  6.766e+00   0.304  0.76140   \nprecipitation_mean.l8  1.041e+02  9.944e+01   1.047  0.29615   \nconst                  5.646e+01  7.650e+01   0.738  0.46134   \ntrend                  7.107e-02  5.775e-02   1.231  0.21984   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 52.91 on 207 degrees of freedom\nMultiple R-Squared: 0.3283, Adjusted R-squared: 0.2471 \nF-statistic: 4.046 on 25 and 207 DF,  p-value: 9.672e-09 \n\n\nEstimation results for equation discharge_mean: \n=============================================== \ndischarge_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + sediment_mean.l4 + discharge_mean.l4 + precipitation_mean.l4 + sediment_mean.l5 + discharge_mean.l5 + precipitation_mean.l5 + sediment_mean.l6 + discharge_mean.l6 + precipitation_mean.l6 + sediment_mean.l7 + discharge_mean.l7 + precipitation_mean.l7 + sediment_mean.l8 + discharge_mean.l8 + precipitation_mean.l8 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)    \nsediment_mean.l1      -2.995e-03  1.381e-03  -2.169 0.031243 *  \ndischarge_mean.l1      4.720e-01  1.059e-01   4.456 1.37e-05 ***\nprecipitation_mean.l1  2.394e+00  1.395e+00   1.717 0.087553 .  \nsediment_mean.l2       1.122e-03  1.435e-03   0.782 0.435001    \ndischarge_mean.l2      2.122e-01  1.130e-01   1.877 0.061908 .  \nprecipitation_mean.l2 -1.670e+00  1.368e+00  -1.221 0.223352    \nsediment_mean.l3      -1.096e-05  1.446e-03  -0.008 0.993960    \ndischarge_mean.l3     -1.233e-02  1.120e-01  -0.110 0.912478    \nprecipitation_mean.l3 -1.153e+00  1.302e+00  -0.886 0.376746    \nsediment_mean.l4       5.921e-04  1.435e-03   0.413 0.680377    \ndischarge_mean.l4     -4.996e-02  1.081e-01  -0.462 0.644335    \nprecipitation_mean.l4  4.998e-01  1.305e+00   0.383 0.702030    \nsediment_mean.l5       2.281e-03  1.427e-03   1.598 0.111497    \ndischarge_mean.l5     -2.240e-01  1.065e-01  -2.103 0.036651 *  \nprecipitation_mean.l5  4.568e-01  1.311e+00   0.348 0.727974    \nsediment_mean.l6      -3.587e-04  1.425e-03  -0.252 0.801439    \ndischarge_mean.l6     -1.939e-01  1.094e-01  -1.773 0.077762 .  \nprecipitation_mean.l6  4.613e+00  1.313e+00   3.514 0.000542 ***\nsediment_mean.l7      -1.337e-04  1.417e-03  -0.094 0.924922    \ndischarge_mean.l7     -7.190e-02  1.099e-01  -0.654 0.513830    \nprecipitation_mean.l7  3.049e+00  1.428e+00   2.134 0.033992 *  \nsediment_mean.l8      -1.271e-04  1.385e-03  -0.092 0.926959    \ndischarge_mean.l8      1.260e-01  9.594e-02   1.313 0.190688    \nprecipitation_mean.l8  2.004e+00  1.410e+00   1.421 0.156716    \nconst                  4.751e+00  1.085e+00   4.380 1.89e-05 ***\ntrend                  1.367e-04  8.189e-04   0.167 0.867569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7503 on 207 degrees of freedom\nMultiple R-Squared: 0.5602, Adjusted R-squared: 0.507 \nF-statistic: 10.55 on 25 and 207 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation precipitation_mean: \n=================================================== \nprecipitation_mean = sediment_mean.l1 + discharge_mean.l1 + precipitation_mean.l1 + sediment_mean.l2 + discharge_mean.l2 + precipitation_mean.l2 + sediment_mean.l3 + discharge_mean.l3 + precipitation_mean.l3 + sediment_mean.l4 + discharge_mean.l4 + precipitation_mean.l4 + sediment_mean.l5 + discharge_mean.l5 + precipitation_mean.l5 + sediment_mean.l6 + discharge_mean.l6 + precipitation_mean.l6 + sediment_mean.l7 + discharge_mean.l7 + precipitation_mean.l7 + sediment_mean.l8 + discharge_mean.l8 + precipitation_mean.l8 + const + trend \n\n                        Estimate Std. Error t value Pr(>|t|)  \nsediment_mean.l1       3.528e-05  8.940e-05   0.395    0.694  \ndischarge_mean.l1      3.381e-03  6.858e-03   0.493    0.623  \nprecipitation_mean.l1 -1.801e-02  9.029e-02  -0.199    0.842  \nsediment_mean.l2       4.444e-08  9.287e-05   0.000    1.000  \ndischarge_mean.l2      4.697e-03  7.317e-03   0.642    0.522  \nprecipitation_mean.l2 -9.101e-02  8.855e-02  -1.028    0.305  \nsediment_mean.l3       7.905e-06  9.363e-05   0.084    0.933  \ndischarge_mean.l3     -1.600e-03  7.251e-03  -0.221    0.826  \nprecipitation_mean.l3 -3.523e-02  8.428e-02  -0.418    0.676  \nsediment_mean.l4       2.425e-05  9.292e-05   0.261    0.794  \ndischarge_mean.l4      4.559e-04  6.996e-03   0.065    0.948  \nprecipitation_mean.l4 -9.918e-02  8.446e-02  -1.174    0.242  \nsediment_mean.l5      -7.392e-06  9.238e-05  -0.080    0.936  \ndischarge_mean.l5      6.098e-03  6.896e-03   0.884    0.378  \nprecipitation_mean.l5 -6.192e-02  8.490e-02  -0.729    0.467  \nsediment_mean.l6      -8.211e-05  9.222e-05  -0.890    0.374  \ndischarge_mean.l6      9.482e-03  7.080e-03   1.339    0.182  \nprecipitation_mean.l6 -1.814e-01  8.498e-02  -2.134    0.034 *\nsediment_mean.l7      -2.095e-06  9.174e-05  -0.023    0.982  \ndischarge_mean.l7     -1.961e-03  7.117e-03  -0.276    0.783  \nprecipitation_mean.l7 -1.092e-01  9.247e-02  -1.180    0.239  \nsediment_mean.l8      -3.165e-05  8.965e-05  -0.353    0.724  \ndischarge_mean.l8      1.574e-03  6.211e-03   0.253    0.800  \nprecipitation_mean.l8 -5.069e-02  9.129e-02  -0.555    0.579  \nconst                 -1.248e-02  7.023e-02  -0.178    0.859  \ntrend                 -1.588e-05  5.301e-05  -0.299    0.765  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.04858 on 207 degrees of freedom\nMultiple R-Squared: 0.1268, Adjusted R-squared: 0.02139 \nF-statistic: 1.203 on 25 and 207 DF,  p-value: 0.2395 \n\n\n\nCovariance matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean           2799.546       28.28649            1.38484\ndischarge_mean            28.286        0.56299            0.02334\nprecipitation_mean         1.385        0.02334            0.00236\n\nCorrelation matrix of residuals:\n                   sediment_mean discharge_mean precipitation_mean\nsediment_mean             1.0000         0.7125             0.5388\ndischarge_mean            0.7125         1.0000             0.6403\nprecipitation_mean        0.5388         0.6403             1.0000\n\n\nFor VAR(1), VAR(3) and VAR(8) models, when using sediment as dependent variable, both discharge and precipitation are significant. Discharge also related to sediment adn precipitation significantly. However, precipitation is not affected by either sediment or discharge. I have also tried using 10 years data and get VAR(7) model with good AIC. On the contrary, VAR(7) model shows precipitation are correlated with lag 7 of discharge and sediment. This may imply the periodicity of water cycle. And also the length of data various may give different model.\n\n\nCross validation\n\n\nCode\nvar_month1 <- var_month[2:241,]\nn=nrow(var_month)\nk=76 #19*4\n\n#n-k=164; 164/12=13.67;\n\nrmse1 <- matrix(NA, 12*13,3)\nrmse2 <- matrix(NA, 12*13,3)\nrmse3 <- matrix(NA, 12*13,3)\n\nyear<-c()\n\n# Convert data frame to time series object\nts_obj <- ts(var_month1[, c(1:3)], star=decimal_date(as.Date(\"1983-09-29\",format = \"%Y-%m-%d\")),frequency = 12)\n\nst <- tsp(ts_obj )[1]+(k-1)/12\n\n\nfor(i in 1:13) # i is a year\n{\n  \n  xtrain <- window(ts_obj, end=st + i-1)\n  xtest <- window(ts_obj, start=st + (i-1) + 1/12, end=st + i)\n  \n  ## p=1\n  fit <- vars::VAR(ts_obj, p=1, type='both')\n  fcast <- predict(fit, n.ahead = 12)\n  \n  fsed<-fcast$fcst$sediment_mean\n  fprcp<-fcast$fcst$precipitation_mean\n  fdisc<-fcast$fcst$discharge_mean\n  ff<-data.frame(fsed[,1],fprcp[,1],fdisc[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff<-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = (i-1)*12+1\n  b= a+11\n  rmse1[c(a:b),] <- sqrt((ff-xtest)^2)\n  \n  \n  ## p=3\n  fit2 <- vars::VAR(ts_obj, p=3, type='both')\n  fcast2 <- predict(fit2, n.ahead = 12)\n  \n  fsed<-fcast2$fcst$sediment_mean\n  fprcp<-fcast2$fcst$precipitation_mean\n  fdisc<-fcast2$fcst$discharge_mean\n  ff2<-data.frame(fsed[,1],fprcp[,1],fdisc[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff2<-ts(ff2,start=c(year,1),frequency = 12)\n  \n  rmse2[c(a:b),] <-sqrt((ff2-xtest)^2)\n  \n  ## p=8\n  fit3 <- vars::VAR(ts_obj, p=8, type='both')\n  fcast3 <- predict(fit3, n.ahead = 12)\n  \n  fsed<-fcast3$fcst$sediment_mean\n  fprcp<-fcast3$fcst$precipitation_mean\n  fdisc<-fcast3$fcst$discharge_mean\n  ff3<-data.frame(fsed[,1],fprcp[,1],fdisc[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff3<-ts(ff3,start=c(year,1),frequency = 12)\n  \n  rmse3[c(a:b),] <-sqrt((ff3-xtest)^2)\n}\n \nyr = rep(c(1983:1995),each =12)\nmn = rep(c(\"September\", \"October\", \"November\", \"December\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\", \"August\"),13)\n\nrmse1 = data.frame(yr,mn,rmse1)\nnames(rmse1) =c(\"Year\", \"Month\",\"Sediment\",\"Discharge\",\"Precipitation\")\nrmse2 = data.frame(yr,mn,rmse2)\nnames(rmse2) =c(\"Year\", \"Month\",\"Sediment\",\"Discharge\",\"Precipitation\")\nrmse3 = data.frame(yr,mn,rmse3)\nnames(rmse3) =c(\"Year\", \"Month\",\"Sediment\",\"Discharge\",\"Precipitation\")\n\n### sediment\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Sediment,colour=\"VAR(1)\")) +\n  geom_line(data = rmse2, aes(x = Year, y = Sediment,colour=\"VAR(3)\")) +\n  geom_line(data = rmse3, aes(x = Year, y = Sediment,colour=\"VAR(8)\")) +\n  labs(\n    title = \"CV RMSE for Sediment\",\n    x = \"Date\",\n    y = \"RMSE\") +\n  scale_color_manual(name = \"VAR Models\", values = c(\"VAR(1)\" = \"blue\", \"VAR(3)\" = \"red\",\"VAR(8)\" = \"black\"))\n\n\n\n\n\nFigure 7: Cross validation RMSE for sediment\n\n\n\n\n\n\nCode\n## discharge\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Discharge,colour=\"VAR(1)\")) +\n  geom_line(data = rmse2, aes(x = Year, y = Discharge,colour=\"VAR(3)\")) +\n  geom_line(data = rmse3, aes(x = Year, y = Discharge,colour=\"VAR(8)\")) +\n  labs(\n    title = \"CV RMSE for Discharge\",\n    x = \"Date\",\n    y = \"RMSE\") +\n  scale_color_manual(name = \"VAR Models\", values = c(\"VAR(1)\" = \"blue\", \"VAR(3)\" = \"red\",\"VAR(8)\" = \"black\"))\n\n\n\n\n\nFigure 8: Cross validation RMSE for stream discharge\n\n\n\n\n\n\nCode\n## discharge\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Precipitation,colour=\"VAR(1)\")) +\n  geom_line(data = rmse2, aes(x = Year, y = Precipitation,colour=\"VAR(3)\")) +\n  geom_line(data = rmse3, aes(x = Year, y = Precipitation,colour=\"VAR(8)\")) +\n  labs(\n    title = \"CV RMSE for Precipitation\",\n    x = \"Date\",\n    y = \"RMSE\") +\n  scale_color_manual(name = \"VAR Models\", values = c(\"VAR(1)\" = \"blue\", \"VAR(3)\" = \"red\",\"VAR(8)\" = \"black\"))\n\n\n\n\n\nFigure 9: Cross validation RMSE for precipitation\n\n\n\n\n\n\nCode\nvar3 <- vars::VAR(ts_obj, p=3, type='both')\nacf(residuals(var3))\nserial.test(var3, lags.pt=10, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var3\nChi-squared = 76.221, df = 63, p-value = 0.1225\n\n\n\n\n\nFigure 10: VAR(3) ACF\n\n\n\n\nFrom Figure 7 ,Figure 8, and Figure 9, we can see the value of p has little impact on stream discharge. For precipitation, VAR(8) has lower RMSE compared to VAR(1) and VAR(3). For sediment, the results varied by time. VAR(1) perform better with lower RMSE around 1982-1985m 1990, 1992, 1995, while VAR(1) and VAR(8) both have lower RMSE in some years. The residuals for VAR(3) model pass the test for serial correlation, while others failed. ACF plots for Figure 10 looks ok. Thus, I choose VAR(3) for the overall model.\n\n\nForecast\n\n\nCode\nforecast(var3) %>%\n  autoplot() + xlab(\"Year\")\n\n\n\n\n\nFigure 11: VAR(3) forecast\n\n\n\n\n\n\nConclusion\nThe VAR models show that the long term correlation caused by water cycle is not significant or cannot be detected. The precipitation is not affected by discharge or sediment. The discharge is not affected by sediment. The strong relationships are still one way. Thus, SARIMAX model may be more suitble for these variables with sediment as dependent variables, and precipitation and discharge plus temperature as the independent variables."
  },
  {
    "objectID": "financial.html#water-treatment-provider",
    "href": "financial.html#water-treatment-provider",
    "title": "Financial Time Series Models",
    "section": "Water treatment provider",
    "text": "Water treatment provider\nDue to the climate change and human activities, extreme climate and environment events, such as the water crisis caused by eutrophication in City of Toledo, OH, occur more frequently (Climate change indicators), and may be regularly. This phenomenon brings opportunities for environmental companies such as water treatment firm. Xylem Inc. is a large American water technology provider, in public utility, residential, commercial, agricultural and industrial settings. The company was founded in 2011 and does business in more than 150 countries (Wiki). In this report, the stock of Xylem was modeled using financial time series models, such as ARMA+GARCH to forecast the volatility of future returns.\n\nXylem stock prices\nFigure 1 shows the plot of Xylem stock prices from 2011 to 2023. We can see the price has increased more than triple since 2014. Figure 2 shows the return plot, where we can see time-varying volatility in the data series.\n\n\nCode\n## read data since 2011\nxyl <- getSymbols(\"XYL\",auto.assign = FALSE, from = \"2011-10-14\",src=\"yahoo\")\nxyl$date<-as.Date(xyl$date,\"%Y-%m-%d\")\n\nchartSeries(xyl, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color)\n\n\nxyl.close<- Ad(xyl)\n\n\n\n\n\nFigure 1: Xylem Inc. stock price 2011-2023\n\n\n\n\n\n\nCode\nreturns = diff(log(xyl.close))\n\nchartSeries(returns, theme=\"white\")\n\n\n\n\n\nFigure 2: Returns 2011-2023\n\n\n\n\n\n\nACF and PACF of returns\nFigure 3 and Figure 4 show the ACF and PACF plots for returns and square of returns, respectively. We can see the returns are not stationary and the error is fading. Thus, before fitting any advanced model, differencing is needed. The first differencing is performed in Figure 5. p can be in range 0-7, while q can be in range 0-7.\n\n\nCode\nacf(returns, na.action = na.pass)\npacf(returns, na.action = na.pass)\n\n\n\n\n\n\n\n\n(a) ACF plot for returns\n\n\n\n\n\n\n\n(b) PACF plot for returns\n\n\n\n\nFigure 3: ACF and PACF plots returns\n\n\n\n\n\nCode\nacf(returns^2, na.action = na.pass)\npacf(returns^2, na.action = na.pass)\n\n\n\n\n\n\n\n\n(a) ACF plot for square of returns\n\n\n\n\n\n\n\n(b) PACF plot for square of returns\n\n\n\n\nFigure 4: ACF and PACF plots square of returns\n\n\n\n\n\nCode\n######### First Differencing#########\nreturns %>% diff() %>% ggtsdisplay() \n\n\n####### Second order Differencing ########\n# returns %>% diff() %>% diff() %>% ggtsdisplay()\n\n## not making much difference\n\n\n\n\n\nFigure 5: Differencing return ACF and PACF plots\n\n\n\n\n\n\nChoose ARIMA model\n\nFitting ARIMA model\n\nManualy fit ARIMA model.\n\n\nARIMA <- list() ## set counter\ncc <- 1\n\ntemp= data.frame()\nls=matrix(rep(NA,6*192),nrow=192)\n\nfor (p in 0:7) {\n  for (q in 0:7)\n  {\n    for (d in 0:2){\n      if(p+d+q<=12)\n            {\n              skip_to_next <- FALSE\n              model<- tryCatch({\n      Arima(returns, order=c(p,d,q))\n    }, error = function(e) { skip_to_next <<- TRUE})\n  if(skip_to_next) { next }\n              ls[cc,]= c(p,d,q,model$aic,model$bic,model$aicc)\n              cc <- cc + 1\n              #print(i)\n              }\n    }\n  }\n} \ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n\ntemp[which.min(temp$AIC),]#2,0,6\n\n   p d q       AIC      BIC      AICc\n67 2 0 6 -15475.53 -15415.8 -15475.45\n\ntemp[which.min(temp$BIC),]#2,0,2\n\n   p d q       AIC       BIC      AICc\n55 2 0 2 -15472.86 -15437.02 -15472.83\n\ntemp[which.min(temp$AICc),]#2,0,6\n\n   p d q       AIC      BIC      AICc\n67 2 0 6 -15475.53 -15415.8 -15475.45\n\n\n\nauto.arima() model.\n\n\n\nCode\nauto.arima(returns)\n\n\nSeries: returns \nARIMA(2,0,2) with non-zero mean \n\nCoefficients:\n          ar1      ar2     ma1     ma2   mean\n      -1.7049  -0.9074  1.6407  0.8499  6e-04\ns.e.   0.0253   0.0327  0.0328  0.0449  3e-04\n\nsigma^2 = 0.0002824:  log likelihood = 7742.43\nAIC=-15472.86   AICc=-15472.83   BIC=-15437.02\n\n\nBoth AIC and AICc give ARIMA(2,0,6) model. BIC give ARIMA(2,0,2) model. And auto.arima() give ARIMA(3,0, 3) model.\n\n\nChoosing ARIMA model\n\nARIMA(2,0,6)\n\n\n\nCode\n######### compare 3 models\nreturns <- na_ma(returns, k = 4, weighting = \"exponential\")\nmodel_output <- capture.output(sarima(returns, 2,0,6))\n\n\n\n\n\nCode\ncat(model_output[105:138], model_output[length(model_output)], sep = \"\\n\")\n\n\n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ma1     ma2     ma3      ma4      ma5      ma6\n      -1.6598  -0.8580  1.6216  0.8440  0.0041  -0.0698  -0.0819  -0.0464\ns.e.   0.0440   0.0498  0.0478  0.0599  0.0385   0.0385   0.0366   0.0208\n      xmean\n      6e-04\ns.e.  3e-04\n\nsigma^2 estimated as 0.0002808:  log likelihood = 7750.71,  aic = -15481.42\n\n$degrees_of_freedom\n[1] 2894\n\n$ttable\n      Estimate     SE  t.value p.value\nar1    -1.6598 0.0440 -37.6864  0.0000\nar2    -0.8580 0.0498 -17.2239  0.0000\nma1     1.6216 0.0478  33.9508  0.0000\nma2     0.8440 0.0599  14.0906  0.0000\nma3     0.0041 0.0385   0.1071  0.9147\nma4    -0.0698 0.0385  -1.8129  0.0700\nma5    -0.0819 0.0366  -2.2401  0.0252\nma6    -0.0464 0.0208  -2.2293  0.0259\nxmean   0.0006 0.0003   1.9513  0.0511\n\n$AIC\n[1] -5.332903\n\n$AICc\n[1] -5.332881\n\n$BIC\n\n\n\nARIMA(2,0,2)\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(returns, 2,0,2))\n\n\n\n\n\nCode\ncat(model_output2[197:222], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$degrees_of_freedom\n[1] 2898\n\n$ttable\n      Estimate     SE  t.value p.value\nar1    -1.7051 0.0252 -67.7727  0.0000\nar2    -0.9081 0.0323 -28.1292  0.0000\nma1     1.6414 0.0326  50.3692  0.0000\nma2     0.8512 0.0443  19.1980  0.0000\nxmean   0.0005 0.0003   1.8041  0.0713\n\n$AIC\n[1] -5.332003\n\n$AICc\n[1] -5.331996\n\n$BIC\n[1] -5.319657\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nARIMA(3,0,3)\n\n\n\nCode\nmodel_output3 <- capture.output(sarima(returns, 3,0,3))\n\n\n\n\n\nCode\ncat(model_output3[121:148], model_output3[length(model_output3)], sep = \"\\n\")\n\n\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3  xmean\n      -0.7711  0.6857  0.8547  0.7028  -0.6937  -0.8099  5e-04\ns.e.   0.0800  0.1302  0.0721  0.0804   0.1218   0.0707  3e-04\n\nsigma^2 estimated as 0.0002818:  log likelihood = 7745.93,  aic = -15475.86\n\n$degrees_of_freedom\n[1] 2896\n\n$ttable\n      Estimate     SE  t.value p.value\nar1    -0.7711 0.0800  -9.6431  0.0000\nar2     0.6857 0.1302   5.2643  0.0000\nar3     0.8547 0.0721  11.8540  0.0000\nma1     0.7028 0.0804   8.7416  0.0000\nma2    -0.6937 0.1218  -5.6941  0.0000\nma3    -0.8099 0.0707 -11.4556  0.0000\nxmean   0.0005 0.0003   1.9828  0.0475\n\n$AIC\n[1] -5.330989\n\n$AICc\n\n\nBoth ARIMA(2,0,2) and ARIMA(3,0,3) look ok. Based on the law of parsimony, ARIMA(2,0,2) is chosen. The residuals show around 500 and 2100, 2600, there are volatility, so further model is needed.\n\n\n\nGARCH(p,q)\nFigure 6 shows the residual ACF and PACF plots after fitting ARIMA(2,0,2) model. The p ranges from 1 to 6, while q ranges from 1 to 8.\n\n\nCode\narma = Arima(returns, order=c(2,0,2)) \narma.re= arma$residuals\n\n\nacf(arma.re^2) #q=1, 8\npacf(arma.re^2) #p=1, 6\n\n\n\n\n\n\n\n\n(a) ACF plot\n\n\n\n\n\n\n\n(b) PACF plot\n\n\n\n\nFigure 6: ACF and PACF plots of the residuals after ARIMA(2,0,2)\n\n\n\n\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:6) {\n  for (q in 1:8) {\n  \nmodel[[cc]] <- garch(arma.re,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n[1] 1\n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\nCall:\ngarch(x = arma.re, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n1.705e-05  8.628e-02  8.525e-01  \n\n\nBest GARCH model is GARCH(1,1).\nThus, our final model is ARMA(2,2)+GARCH(1,1) as the summary shown below.\n\n\nCode\nsummary(fit <- garchFit(~arma(2,2)+garch(1,1),returns,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(2, 2) + garch(1, 1), data = returns, \n    trace = F) \n\nMean and Variance Equation:\n data ~ arma(2, 2) + garch(1, 1)\n<environment: 0x7fb695c69030>\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ar2          ma1          ma2        omega  \n 1.9173e-03  -1.0000e+00  -5.5752e-01   9.7676e-01   5.6441e-01   1.7845e-05  \n     alpha1        beta1  \n 8.8821e-02   8.4696e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu      1.917e-03   9.248e-04    2.073   0.0382 *  \nar1    -1.000e+00   5.941e-01   -1.683   0.0923 .  \nar2    -5.575e-01   2.659e-01   -2.096   0.0360 *  \nma1     9.768e-01   5.776e-01    1.691   0.0908 .  \nma2     5.644e-01   2.483e-01    2.273   0.0230 *  \nomega   1.784e-05   3.891e-06    4.586 4.52e-06 ***\nalpha1  8.882e-02   1.473e-02    6.029 1.65e-09 ***\nbeta1   8.470e-01   2.535e-02   33.417  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 7945.788    normalized:  2.737095 \n\nDescription:\n Mon May  1 21:10:14 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  6307.283  0        \n Shapiro-Wilk Test  R    W      0.9486205 0        \n Ljung-Box Test     R    Q(10)  5.783548  0.8331089\n Ljung-Box Test     R    Q(15)  8.667962  0.894229 \n Ljung-Box Test     R    Q(20)  13.98472  0.8312699\n Ljung-Box Test     R^2  Q(10)  3.163005  0.9773234\n Ljung-Box Test     R^2  Q(15)  4.694232  0.9944139\n Ljung-Box Test     R^2  Q(20)  7.302087  0.9955702\n LM Arch Test       R    TR^2   4.620648  0.9694658\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.468679 -5.452218 -5.468694 -5.462748 \n\n\n\n\nModel equation\n\\(\\phi(B) x_t = \\delta + \\theta(B) y_t\\),\nwhere \\(\\phi(B)=(1.922e{-03})-B-0.5567*B^2\\); and \\(\\theta(B)=(1.922e{-03})+0.9767* B+0.5637*B^2\\)\n\\(y_t=\\sigma_t \\epsilon_t\\)\n\\(var(y_t|y_{t-1})=\\sigma ^2= 1.757e{-05}+ 0.08776*(y_{t-1})^2 + 0.8491 \\sigma_{t-1}^2\\)"
  },
  {
    "objectID": "dl_discharge.html",
    "href": "dl_discharge.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.layers import Dense, SimpleRNN,LSTM,GRU\n\n\n2023-05-01 21:11:00.472594: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "dl_discharge.html#data-processing",
    "href": "dl_discharge.html#data-processing",
    "title": "Deep Learning for TS",
    "section": "Data processing",
    "text": "Data processing\n\nMonthly discharge\n\n\nCode\n## read data\ndaily_discharge = pd.read_csv(\"./data/clean_discharge.csv\")\ndaily_discharge = daily_discharge.rename(columns={\"discharge.date\": \"date\", \"discharge.discharge\": \"discharge\"})\ndaily_discharge['date'] = pd.to_datetime(daily_discharge['date'])\n\n\n\n\nCode\ndaily_discharge\n\n\n\n\n\n\n  \n    \n      \n      date\n      discharge\n    \n  \n  \n    \n      0\n      1970-01-01\n      850.0\n    \n    \n      1\n      1970-01-02\n      850.0\n    \n    \n      2\n      1970-01-03\n      800.0\n    \n    \n      3\n      1970-01-04\n      750.0\n    \n    \n      4\n      1970-01-05\n      750.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      19398\n      2023-02-10\n      28800.0\n    \n    \n      19399\n      2023-02-11\n      26100.0\n    \n    \n      19400\n      2023-02-12\n      17800.0\n    \n    \n      19401\n      2023-02-13\n      11000.0\n    \n    \n      19402\n      2023-02-14\n      8850.0\n    \n  \n\n19403 rows × 2 columns\n\n\n\n\n\nCode\ndaily_discharge.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19403 entries, 0 to 19402\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       19403 non-null  datetime64[ns]\n 1   discharge  19394 non-null  float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 303.3 KB\n\n\n\n\nCode\n## log transformation\ndaily_discharge['log_discharge'] = np.log(daily_discharge['discharge'])\n\n## filter 1980-2022\nmask = (daily_discharge['date'] > '1980-01-01') & (daily_discharge['date'] <= '2022-12-31')\n\ndischarge1980 = daily_discharge.loc[mask]\n\n# ## convert to monthly\n# monthly_logmean = discharge1980.groupby(pd.PeriodIndex(discharge1980['date'], freq=\"M\"))['log_discharge'].mean()\n\n# monthly_logmean.index = monthly_logmean.index.strftime('%Y-%m')\n# monthly_logmean_df = monthly_logmean.to_frame().reset_index()\n# print(monthly_logmean_df.shape)\n\n\n\n\nCode\ndischarge1980.info()\ndischarge1980.head()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 15705 entries, 3653 to 19357\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   date           15705 non-null  datetime64[ns]\n 1   discharge      15698 non-null  float64       \n 2   log_discharge  15698 non-null  float64       \ndtypes: datetime64[ns](1), float64(2)\nmemory usage: 490.8 KB\n\n\n\n\n\n\n  \n    \n      \n      date\n      discharge\n      log_discharge\n    \n  \n  \n    \n      3653\n      1980-01-02\n      7420.0\n      8.911934\n    \n    \n      3654\n      1980-01-03\n      5770.0\n      8.660427\n    \n    \n      3655\n      1980-01-04\n      4590.0\n      8.431635\n    \n    \n      3656\n      1980-01-05\n      3690.0\n      8.213382\n    \n    \n      3657\n      1980-01-06\n      3330.0\n      8.110728\n    \n  \n\n\n\n\n\n\nCode\nX = np.array(discharge1980[\"log_discharge\"].values.astype('float32')).reshape(discharge1980.shape[0],1)\nprint(X.shape)\n\n\n(15705, 1)"
  },
  {
    "objectID": "dl_discharge.html#visualization-plotting-function",
    "href": "dl_discharge.html#visualization-plotting-function",
    "title": "Deep Learning for TS",
    "section": "Visualization plotting function",
    "text": "Visualization plotting function\n\n\nCode\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\n\n# UTILITY\ndef plotly_line_plot(t,y,title=\"Plot\",x_label=\"t: time (months)\",y_label=\"y(t): Response variable\"):\n\n    # GENERATE PLOTLY FIGURE\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n    # IMPORTANT: SVG FIXES RENDER ISSUES WITH SAD FACE \n    # https://community.plotly.com/t/plotly-express-line-charts-are-not-shown/39715\n    \n    # ADD MORE\n    for i in range(1,len(y)):\n        if len(t[i])==1:\n            #print(t[i],y[i])\n            fig.add_scatter(x=t[i],y=y[i])\n        else:\n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n\n    fig.update_layout(\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"plotly_white\",\n        showlegend=False\n    )\n    fig.show()"
  },
  {
    "objectID": "dl_discharge.html#visualize",
    "href": "dl_discharge.html#visualize",
    "title": "Deep Learning for TS",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\n\n# UTILITY\ndef plotly_line_plot(t,y,title=\"Plot\",x_label=\"t: time (months)\",y_label=\"y(t): Response variable\"):\n\n    # GENERATE PLOTLY FIGURE\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n    # IMPORTANT: SVG FIXES RENDER ISSUES WITH SAD FACE \n    # https://community.plotly.com/t/plotly-express-line-charts-are-not-shown/39715\n    \n    # ADD MORE\n    for i in range(1,len(y)):\n        if len(t[i])==1:\n            #print(t[i],y[i])\n            fig.add_scatter(x=t[i],y=y[i])\n        else:\n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n\n    fig.update_layout(\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"plotly_white\",\n        showlegend=False\n    )\n    fig.show()\n\n\n\n\nCode\n# SINGLE SERIES \nt=[*range(0,len(X))]\nplotly_line_plot([t],[X[:,0]],title=\"Daily log stream discharge since 1980-01\")"
  },
  {
    "objectID": "dl_discharge.html#test-train-split-data",
    "href": "dl_discharge.html#test-train-split-data",
    "title": "Deep Learning for TS",
    "section": "Test-train split data",
    "text": "Test-train split data\nfirst 80% of the data is training, laster 20% of the data is testing\n\n\nCode\n### Utility function: \n# Parameter split_percent defines the ratio of training examples\ndef get_train_test(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\ntrain_data, test_data, data = get_train_test(X)\n\nprint(train_data.shape)\nprint(test_data.shape)\n\n\n(12564,)\n(3141,)\n\n\n\n\nCode\nt1=[*range(0,len(train_data))]\nt2=len(train_data)+np.array([*range(0,len(test_data))])\nplotly_line_plot([t1,t2],[train_data,test_data],title=\"Daily log stream discharge since 1980-01\")"
  },
  {
    "objectID": "dl_discharge.html#re-format-data-into-required-shape",
    "href": "dl_discharge.html#re-format-data-into-required-shape",
    "title": "Deep Learning for TS",
    "section": "Re-format data into required shape",
    "text": "Re-format data into required shape\n\n\nCode\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] #if time_steps=10 remove every 10th entry\n    X = dat[X_ind]; \n\n    #PLOT\n    if(plot_data_partition):\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    #RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n#PARTITION DATA, to monthly\np=31 # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\n\n\n\nCode\nprint(testX.shape,testY.shape)\nprint(trainX.shape,trainY.shape)\nprint(type(trainX))\n\n\n(101, 30, 1) (101,)\n(405, 30, 1) (405,)\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "dl_discharge.html#simple-rnn",
    "href": "dl_discharge.html#simple-rnn",
    "title": "Deep Learning for TS",
    "section": "Simple RNN",
    "text": "Simple RNN\n\nModel and training parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=400\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\n# max_features = 10000    #DEFINES SIZE OF VOCBULARY TO USE\n# maxlen       = 250      #CUTOFF REVIEWS maxlen 20 WORDS)\nbatch_size   = 1000\nverbose      = 0\nembed_dim    = 8        #DIMENSION OF EMBEDING SPACE (SIZE OF VECTOR FOR EACH WORD)\nlr           = 0.001    #LEARNING RATE\nvalidation_split=0.2\n\n\n\n\nCreate model with l2 regulation\n\n\nCode\nfrom tensorflow.keras import regularizers\n\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\nmodel.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1), ##L2 regulation\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n\n\n                                                                 \n\n\n dense (Dense)               (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19\n\n\nTrainable params: 19\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\nTrain model\n\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=0)\n\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Training epoch vs errors\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/13 [=>............................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 1ms/step\n\n\n1/4 [======>.......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n(405, 30, 1) (405,) (405,) (101, 30, 1) (101,) (101,)\n(405,) (405,)\n0.0017565492\n0.001448677\nTrain MSE = 0.00176 RMSE = 0.04191\nTest MSE = 0.00145 RMSE = 0.03806\n\n\n\n                                                \n\n\n[0.3108874559402466, 0.3002217710018158, 0.29341238737106323, 0.2879091799259186, 0.2830987572669983, 0.2787294387817383, 0.2746223509311676, 0.27066898345947266, 0.26683858036994934, 0.26306378841400146, 0.2593434751033783, 0.25567761063575745, 0.2520497739315033, 0.24843847751617432, 0.24487212300300598, 0.24134834110736847, 0.23784564435482025, 0.2344035804271698, 0.23096559941768646, 0.22758549451828003, 0.22423666715621948, 0.22094479203224182, 0.2177116721868515, 0.2144675999879837, 0.21130380034446716, 0.20815880596637726, 0.2051047533750534, 0.2020280808210373, 0.1990342140197754, 0.19605594873428345, 0.19314661622047424, 0.19023971259593964, 0.18738973140716553, 0.1845601350069046, 0.1817697286605835, 0.17902310192584991, 0.17629359662532806, 0.17360234260559082, 0.1709124743938446, 0.1682795286178589, 0.16565634310245514, 0.16310067474842072, 0.16051968932151794, 0.15800635516643524, 0.15551301836967468, 0.1530269980430603, 0.15058569610118866, 0.14818330109119415, 0.14578482508659363, 0.1434529572725296, 0.14110635221004486, 0.13880696892738342, 0.1365364044904709, 0.1342657059431076, 0.1320527046918869, 0.12986314296722412, 0.12769165635108948, 0.1255718469619751, 0.1234433576464653, 0.12134256213903427, 0.1192866787314415, 0.11726214736700058, 0.11524344980716705, 0.11327764391899109, 0.11130592226982117, 0.1093662902712822, 0.10745450109243393, 0.10560530424118042, 0.1037127897143364, 0.10187818109989166, 0.10006969422101974, 0.09828022122383118, 0.09651742875576019, 0.09478812664747238, 0.0930565819144249, 0.09136470407247543, 0.08970402181148529, 0.08806207031011581, 0.0864313542842865, 0.084837406873703, 0.0832684189081192, 0.08171334862709045, 0.08018843829631805, 0.07869245857000351, 0.07719044387340546, 0.07572417706251144, 0.0742911845445633, 0.07286542654037476, 0.07146595418453217, 0.07010039687156677, 0.06872881203889847, 0.06739397346973419, 0.06609196960926056, 0.06477686017751694, 0.06352757662534714, 0.06223146244883537, 0.060990504920482635, 0.059772104024887085, 0.058565665036439896, 0.057385269552469254, 0.05620497837662697, 0.05507645010948181, 0.053922805935144424, 0.0528072789311409, 0.051717594265937805, 0.05065013840794563, 0.04956983029842377, 0.048517122864723206, 0.04749080911278725, 0.04649000242352486, 0.04550648853182793, 0.04450184106826782, 0.04353545978665352, 0.042607735842466354, 0.041689809411764145, 0.04075021669268608, 0.03986654058098793, 0.038982175290584564, 0.038102660328149796, 0.03725041449069977, 0.03641289100050926, 0.035577744245529175, 0.034758102148771286, 0.033966630697250366, 0.03316803276538849, 0.03240276873111725, 0.03164050728082657, 0.03090744838118553, 0.030162066221237183, 0.029430806636810303, 0.02873983234167099, 0.028017668053507805, 0.02733144722878933, 0.026650656014680862, 0.025983547791838646, 0.025329209864139557, 0.02467360533773899, 0.02402767725288868, 0.023433811962604523, 0.02277667075395584, 0.022168274968862534, 0.0215758066624403, 0.020985452458262444, 0.020407672971487045, 0.019841602072119713, 0.019254939630627632, 0.018693849444389343, 0.01816033199429512, 0.017655955627560616, 0.017089413478970528, 0.016582297161221504, 0.016077985987067223, 0.015563241206109524, 0.015114822424948215, 0.014596800319850445, 0.014118356630206108, 0.013673419132828712, 0.013203499838709831, 0.012760370969772339, 0.012325075455009937, 0.011892430484294891, 0.011484779417514801, 0.011091209016740322, 0.010684026405215263, 0.010318579152226448, 0.009914617985486984, 0.009561680257320404, 0.009176211431622505, 0.008846615441143513, 0.00851520337164402, 0.008155605755746365, 0.007820555940270424, 0.007513440679758787, 0.007199685554951429, 0.006902997847646475, 0.006608111783862114, 0.0063477614894509315, 0.006075848359614611, 0.005797544028609991, 0.005543072242289782, 0.005363908130675554, 0.00507157389074564, 0.004856729414314032, 0.004623419605195522, 0.004419589880853891, 0.004240622278302908, 0.004028300754725933, 0.003827929962426424, 0.003705485025420785, 0.003523463848978281, 0.003359363880008459, 0.003186840331181884, 0.0030680084601044655, 0.0029167686589062214, 0.0027999153826385736, 0.002681467914953828, 0.002576600294560194, 0.002474091248586774, 0.0023786176461726427, 0.0022884332574903965, 0.00220343261025846, 0.002120204968377948, 0.0021204459480941296, 0.002005714224651456, 0.0019407669315114617, 0.0018925239564850926, 0.0018540947930887341, 0.0018290443113073707, 0.0017838024068623781, 0.0017663839971646667, 0.0017417536582797766, 0.0017711326945573092, 0.00170981977134943, 0.0016832747496664524, 0.0017001150408759713, 0.0016579336952418089, 0.0016627070726826787, 0.0016468095127493143, 0.0016890092520043254, 0.0016625035787001252, 0.0016558965435251594, 0.0016414475394412875, 0.0016773197567090392, 0.0016528323758393526, 0.001659937435761094, 0.001697191852144897, 0.001658569904975593, 0.0016417148290202022, 0.0016875729197636247, 0.0016449176473543048, 0.0016445742221549153, 0.0016319196438416839, 0.001683987444266677, 0.0016456709709018469, 0.001686456031166017, 0.0016349944053217769, 0.0016796526033431292, 0.0016351897502318025, 0.0016432531410828233, 0.0016615907661616802, 0.0016409717500209808, 0.0016709122573956847, 0.0016550408909097314, 0.001665656454861164, 0.001652262988500297, 0.0016531517030671239, 0.001676955376751721, 0.0016751107759773731, 0.0016398034058511257, 0.001643301104195416, 0.0016396474093198776, 0.0016350271180272102, 0.0016415690770372748, 0.0016499205958098173, 0.0016476131277158856, 0.0017006995622068644, 0.0016508708940818906, 0.0016341347945854068, 0.0016480338526889682, 0.0016465912340208888, 0.0016423226334154606, 0.0016662358539178967, 0.0016724058659747243, 0.0016621130052953959, 0.0016352083766832948, 0.001646813820116222, 0.0016336061526089907, 0.0016417417209595442, 0.0016377130523324013, 0.0016498841578140855, 0.0016399112064391375, 0.0016631014877930284, 0.0016414260026067495, 0.0016451205592602491, 0.0016404526541009545, 0.0016757852863520384, 0.0016490864800289273, 0.001652371371164918, 0.0016575445188209414, 0.001637123292312026, 0.001641914015635848, 0.0016348310746252537, 0.0016645336290821433, 0.0016450824914500117, 0.0016274790978059173, 0.001646086573600769, 0.001665248884819448, 0.0016374988481402397, 0.0016353527316823602, 0.0016604415141046047, 0.001633237348869443, 0.0016397909494116902, 0.0016440346371382475, 0.0016506194369867444, 0.001636544126085937, 0.001663472969084978, 0.0016205005813390017, 0.00163619767408818, 0.0016328758792951703, 0.001646209042519331, 0.0016341268783435225, 0.0016191101167351007, 0.0016744815511628985, 0.0016255077207461, 0.0016226897714659572, 0.0016396496212109923, 0.0016382241155952215, 0.0016389364609494805, 0.001677077729254961, 0.0016395436832681298, 0.0016204466810449958, 0.0016375802224501967, 0.0016358671709895134, 0.0016425400972366333, 0.0016306322067975998, 0.0016578489448875189, 0.0016185777494683862, 0.0016573844477534294, 0.0016848831437528133, 0.0016222138656303287, 0.001621727948077023, 0.0016350664664059877, 0.001671533565968275, 0.0016361677553504705, 0.0016284191515296698, 0.0016238391399383545, 0.001646394724957645, 0.0016785067273303866, 0.0016243065474554896, 0.0016257170354947448, 0.0016344134928658605, 0.0016391593962907791, 0.00163178239017725, 0.0016266582533717155, 0.0016470750560984015, 0.0016706578899174929, 0.001631065271794796, 0.0016258800169453025, 0.0016361434245482087, 0.0016745810862630606, 0.0016341726295650005, 0.0016230697510764003, 0.0016261960845440626, 0.0016294200904667377, 0.0016375231789425015, 0.0016561797820031643, 0.0016509474953636527, 0.0016426618676632643, 0.0016297653783112764, 0.001623251591809094, 0.0016529312124475837, 0.0016449373215436935, 0.0016329633072018623, 0.0016247101593762636, 0.001639148686081171, 0.0016417242586612701, 0.0016259046969935298, 0.0016322566661983728, 0.0016647200100123882, 0.0016187661094591022, 0.001623236108571291, 0.0016304469900205731, 0.0016279341652989388, 0.0016253682551905513, 0.001645428012125194, 0.0016227226005867124, 0.001628930913284421, 0.0016285051824524999, 0.0016413613921031356, 0.001634867861866951, 0.0016286795726045966, 0.001630656304769218, 0.0016500180354341865, 0.00162640237249434, 0.0016261078417301178, 0.0016374981496483088, 0.0016449119430035353, 0.0016189611051231623, 0.0016265478916466236, 0.001632209401577711, 0.0016273913206532598, 0.0016246887389570475, 0.0016771301161497831, 0.00161454186309129, 0.0016325765755027533, 0.0016331998631358147, 0.0016148980939760804, 0.0016213845228776336, 0.0016666391165927052, 0.0016283392906188965, 0.0016138991340994835, 0.001638756482861936, 0.0016233565984293818, 0.0016395364655181766, 0.0016218567034229636, 0.0016495217569172382, 0.0016466252272948623, 0.0016419795574620366, 0.0016299488488584757, 0.0016154582845047116]\n\n\n<matplotlib.legend.Legend at 0x1a7986b30>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nVisualize predictions-2\n\n\nCode\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=80)\n    #ORIGINAL DATA\n    print(X.shape,Y.shape)\n    plt.plot(Y_ind, Y,'o', label='target')\n    plt.plot(X_ind, X,'.', label='training points');     \n    plt.plot(Y_ind, train_predict,'r.', label='prediction');    \n    plt.plot(Y_ind, train_predict,'-');    \n    plt.legend()\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Log daily discharge')\n    #plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n    plt.show()\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(12150,) (405,)\n\n\n\n\n\n\n\nModel with no regulation\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\nmodel.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\n#recurrent_regularizer=regularizers.L2(1e-1), ##L2 regulation\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19\n\n\nTrainable params: 19\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\nTrain model\n\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=0)\n\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Training epoch vs errors\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/13 [=>............................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 1ms/step\n\n\n1/4 [======>.......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n(405, 30, 1) (405,) (405,) (101, 30, 1) (101,) (101,)\n(405,) (405,)\n0.0027458135\n0.002553787\nTrain MSE = 0.00275 RMSE = 0.05240\nTest MSE = 0.00255 RMSE = 0.05054\n\n\n\n                                                \n\n\n[0.17553949356079102, 0.12068244814872742, 0.08950356394052505, 0.06777141988277435, 0.05180659145116806, 0.0397934764623642, 0.03076493926346302, 0.024105669930577278, 0.019302118569612503, 0.015961138531565666, 0.01369176059961319, 0.012254060246050358, 0.011384543962776661, 0.010869958437979221, 0.010518142953515053, 0.010232736356556416, 0.00995564740151167, 0.009635726921260357, 0.009312110021710396, 0.00896198395639658, 0.008623262867331505, 0.008279991336166859, 0.007993420585989952, 0.007614126894623041, 0.007417799439281225, 0.007116354536265135, 0.007014830131083727, 0.006799818016588688, 0.006605956237763166, 0.006456151604652405, 0.006364735774695873, 0.006265074480324984, 0.006163946818560362, 0.006143555045127869, 0.006087973713874817, 0.005950120277702808, 0.005897593218833208, 0.005798038095235825, 0.005694576073437929, 0.005627084989100695, 0.0056846183724701405, 0.005624100100249052, 0.005616683978587389, 0.005476015619933605, 0.005425041541457176, 0.005492782220244408, 0.005380299873650074, 0.005307394079864025, 0.005237346049398184, 0.005177626386284828, 0.005221046973019838, 0.0051308488473296165, 0.0050027999095618725, 0.0050979917868971825, 0.005081131588667631, 0.005000936333090067, 0.004900648724287748, 0.004884544759988785, 0.004929750692099333, 0.00501401349902153, 0.0048269350081682205, 0.00484246201813221, 0.00473794573917985, 0.004741076845675707, 0.004683703184127808, 0.004746627062559128, 0.004668680019676685, 0.004616029094904661, 0.00464197201654315, 0.004611275624483824, 0.004550042562186718, 0.00462440075352788, 0.0045507014729082584, 0.0045784469693899155, 0.00449120020493865, 0.004482499789446592, 0.00450771814212203, 0.004429935477674007, 0.0044355024583637714, 0.004499053582549095, 0.004386972635984421, 0.004396696574985981, 0.0045066168531775475, 0.004306909628212452, 0.004388179164379835, 0.004298947285860777, 0.004426384344696999, 0.004258922766894102, 0.004512183368206024, 0.004307942930608988, 0.00433084461838007, 0.004206458572298288, 0.004231011029332876, 0.004193468019366264, 0.004190606530755758, 0.004291216377168894, 0.004227930214256048, 0.004166227765381336, 0.004214003216475248, 0.004140913952142, 0.004136207513511181, 0.00417302455753088, 0.004140851553529501, 0.004083710256963968, 0.004071402363479137, 0.004158083349466324, 0.004066001623868942, 0.004121261648833752, 0.004058812744915485, 0.004022428300231695, 0.003984862472862005, 0.0039937617257237434, 0.004166712518781424, 0.004114989656955004, 0.003995445091277361, 0.004145863000303507, 0.003944194409996271, 0.003920343704521656, 0.0039709764532744884, 0.003952853847295046, 0.003917883150279522, 0.004068244248628616, 0.003953602630645037, 0.003887831699103117, 0.0039874291978776455, 0.003893587039783597, 0.003963975235819817, 0.0038317290600389242, 0.003914784640073776, 0.0038873425219208, 0.003966192249208689, 0.0038273469544947147, 0.0038505836855620146, 0.0038852805737406015, 0.003941224422305822, 0.0037892721593379974, 0.003758979495614767, 0.003881596028804779, 0.0037893950939178467, 0.0037858306895941496, 0.0037519067991524935, 0.0037654053885489702, 0.003753477707505226, 0.0036974651739001274, 0.0037469472736120224, 0.003762972541153431, 0.0036566180642694235, 0.0037657280918210745, 0.0037015092093497515, 0.0036959967110306025, 0.0037373355589807034, 0.003740261774510145, 0.0037669450975954533, 0.0036526028998196125, 0.0036391718313097954, 0.0036233828868716955, 0.0037001946475356817, 0.0035793110728263855, 0.0035990660544484854, 0.0036076137330383062, 0.0036162748001515865, 0.003639609320089221, 0.0037183610256761312, 0.0035410162527114153, 0.003527236171066761, 0.0035699422005563974, 0.0035761226899921894, 0.0036076009273529053, 0.0035301130264997482, 0.0035050050355494022, 0.0035147331655025482, 0.0036081508733332157, 0.0035279670264571905, 0.003460733685642481, 0.003569528926163912, 0.0035568755120038986, 0.00347566744312644, 0.0034793049562722445, 0.0034871255047619343, 0.0034508437383919954, 0.0034269175957888365, 0.003470513503998518, 0.0033962721936404705, 0.003410259960219264, 0.0034763868898153305, 0.0034916650038212538, 0.0033964260946959257, 0.003413277678191662, 0.003546642605215311, 0.003396973479539156, 0.0033654968719929457, 0.003407300217077136, 0.0033676440361887217, 0.0034191717859357595, 0.0033488189801573753, 0.0033306004479527473, 0.003442916553467512, 0.003323407843708992, 0.0033991883974522352, 0.0033164885826408863, 0.003336629830300808, 0.0033200376201421022, 0.0033700759522616863, 0.003328401828184724, 0.003309599356725812, 0.0033800266683101654, 0.003293861635029316, 0.0032640688586980104, 0.00328762736171484, 0.003262812038883567, 0.0033049052581191063, 0.0032000537030398846, 0.0032866988331079483, 0.0032661184668540955, 0.0032069352455437183, 0.00323704001493752, 0.0033436904195696115, 0.0032243779860436916, 0.0031794786918908358, 0.0032896925695240498, 0.0031749531626701355, 0.0031715715304017067, 0.003219352103769779, 0.003225706284865737, 0.0031860084272921085, 0.0032688924111425877, 0.003146496368572116, 0.0032662420999258757, 0.003183226566761732, 0.003136623417958617, 0.003158558625727892, 0.0031191427260637283, 0.0031967214308679104, 0.0031283730641007423, 0.0031293965876102448, 0.0030663395300507545, 0.003134111175313592, 0.0031036899890750647, 0.003071162151172757, 0.003076742636039853, 0.0030866898596286774, 0.003199914703145623, 0.00307306251488626, 0.003073278348892927, 0.00311904214322567, 0.003064614487811923, 0.0030907506588846445, 0.003041153075173497, 0.003072127467021346, 0.0030420448165386915, 0.0030382750555872917, 0.0030450071208178997, 0.003096160013228655, 0.0030788930598646402, 0.003040068782866001, 0.0030084168538451195, 0.0030453011859208345, 0.0030227573588490486, 0.0030795636121183634, 0.002988225780427456, 0.0029892344027757645, 0.0030202835332602262, 0.002966913627460599, 0.003009933279827237, 0.0029619173146784306, 0.002985304920002818, 0.003015725174918771, 0.002942917635664344, 0.0029881782829761505, 0.003008410334587097, 0.002948872046545148, 0.0029336772859096527, 0.0029065196868032217, 0.0029745493084192276, 0.002964355517178774, 0.0029481658712029457, 0.0029511526226997375, 0.0029022013768553734, 0.0029033508617430925, 0.002930557355284691, 0.0028946632519364357, 0.002963949227705598, 0.0029105825815349817, 0.0029113166965544224, 0.002921349834650755, 0.00286635709926486, 0.0028526228852570057, 0.003007040824741125, 0.002866384806111455, 0.002951426198706031, 0.0028487190138548613, 0.002835446037352085, 0.002867699135094881, 0.0028415003325790167, 0.0029906509444117546, 0.0028232214972376823, 0.0028331740759313107, 0.0029371101409196854, 0.002849818440154195, 0.002920216880738735, 0.0028278236277401447, 0.0028724621515721083, 0.0028184966649860144, 0.002828397788107395, 0.0028008101508021355, 0.0028770440258085728, 0.0028312206268310547, 0.0029679930303245783, 0.0028620557859539986, 0.0028523276560008526, 0.002784410025924444, 0.0027596557047218084, 0.0027826197911053896, 0.0028055349830538034, 0.0027789510786533356, 0.0028463632334023714, 0.0027689223643392324, 0.0028152160812169313, 0.0028258347883820534, 0.0027519010473042727, 0.002742550102993846, 0.0028404274489730597, 0.00276729604229331, 0.0028002425096929073, 0.002728391671553254, 0.002774313557893038, 0.002743650460615754, 0.002724936231970787, 0.0027687239926308393, 0.0027143550105392933, 0.0027028450276702642, 0.002779311267659068, 0.0027049854397773743, 0.002715200651437044, 0.002705087186768651, 0.0027168532833456993, 0.002691148314625025, 0.0027612592093646526, 0.0026768012903630733, 0.0026811007410287857, 0.0027315840125083923, 0.0027275800239294767, 0.002647737041115761, 0.0027127056382596493, 0.0026661651208996773, 0.0026923527475446463, 0.002647331915795803, 0.00267124455422163, 0.0026676144916564226, 0.0027568552177399397, 0.0026418534107506275, 0.0026381979696452618, 0.0026818853802978992, 0.0026822376530617476, 0.00262140529230237, 0.0026194527745246887, 0.0027552847750484943, 0.0026063525583595037, 0.0026470301672816277, 0.0026099802926182747, 0.0027315933257341385, 0.0026436038315296173, 0.002674320712685585, 0.0026612868532538414, 0.0026165402960032225, 0.002664056606590748, 0.002595866098999977, 0.002656121738255024, 0.002606835914775729, 0.002643544226884842, 0.002587841125205159, 0.0026252339594066143, 0.002600385108962655, 0.0026085374411195517, 0.002621924038976431, 0.002586494432762265, 0.00256694620475173, 0.0026775768492370844, 0.0025811486411839724, 0.002563599031418562, 0.0025636840146034956, 0.002591121708974242, 0.002607807982712984, 0.002644626423716545, 0.0026203945744782686, 0.0025571754667907953, 0.0025512315332889557, 0.002582620596513152, 0.002554487669840455, 0.002609263639897108, 0.0025346591137349606, 0.0025266429875046015, 0.002619985956698656, 0.0026219403371214867, 0.0025224541313946247, 0.002602993045002222, 0.0026333327405154705, 0.0025428254157304764, 0.002527728211134672, 0.002490841783583164]\n\n\n<matplotlib.legend.Legend at 0x1a81c0ac0>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nVisualize predictions-2\n\n\nCode\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=80)\n    #ORIGINAL DATA\n    print(X.shape,Y.shape)\n    plt.plot(Y_ind, Y,'o', label='target')\n    plt.plot(X_ind, X,'.', label='training points');     \n    plt.plot(Y_ind, train_predict,'r.', label='prediction');    \n    plt.plot(Y_ind, train_predict,'-');    \n    plt.legend()\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Log daily discharge')\n    #plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n    plt.show()\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(12150,) (405,)"
  },
  {
    "objectID": "dl_discharge.html#gru",
    "href": "dl_discharge.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=200\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\n# max_features = 10000    #DEFINES SIZE OF VOCBULARY TO USE\n# maxlen       = 250      #CUTOFF REVIEWS maxlen 20 WORDS)\nbatch_size   = 1000\nverbose      = 0\nembed_dim    = 8        #DIMENSION OF EMBEDING SPACE (SIZE OF VECTOR FOR EACH WORD)\nlr           = 0.001    #LEARNING RATE\nvalidation_split=0.2\n\n\n\nModel with L2 regulation\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\n#model.add(SimpleRNN(\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_2\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru (GRU)                   (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58\n\n\nTrainable params: 58\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n2023-05-01 21:11:36.350830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:36.353073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:36.354800: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=0)\n\n\n2023-05-01 21:11:36.831823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:36.834231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:36.835960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:37.333161: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:37.335568: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:37.336886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:38.297690: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:38.300004: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:38.301289: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/13 [=>............................] - ETA: 3s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 2ms/step\n\n\n1/4 [======>.......................] - ETA: 0s\n\n\n2023-05-01 21:11:45.684327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:45.687454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:45.688498: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n(405, 30, 1) (405,) (405,) (101, 30, 1) (101,) (101,)\n(405,) (405,)\n0.0021219314\n0.0020427871\nTrain MSE = 0.00212 RMSE = 0.04606\nTest MSE = 0.00204 RMSE = 0.04520\n\n\n\n                                                \n\n\n[0.5157573819160461, 0.4692356586456299, 0.43849626183509827, 0.4133245646953583, 0.3912471830844879, 0.3712652623653412, 0.35296231508255005, 0.3360000252723694, 0.32024553418159485, 0.3055782914161682, 0.2919202148914337, 0.2792167067527771, 0.26735761761665344, 0.25629153847694397, 0.24594680964946747, 0.2363181710243225, 0.2274027019739151, 0.21917039155960083, 0.21156582236289978, 0.2045873999595642, 0.19823305308818817, 0.19240276515483856, 0.18707218766212463, 0.18219903111457825, 0.17762117087841034, 0.17324493825435638, 0.16904284060001373, 0.16492822766304016, 0.16085515916347504, 0.15687763690948486, 0.15293990075588226, 0.1490546017885208, 0.14522235095500946, 0.14148899912834167, 0.1377636343240738, 0.1341337263584137, 0.13056828081607819, 0.1270795613527298, 0.12362979352474213, 0.12031405419111252, 0.11701170355081558, 0.11377989500761032, 0.11066509783267975, 0.10756171494722366, 0.10453341901302338, 0.10170283168554306, 0.0988038033246994, 0.09603468328714371, 0.09326037764549255, 0.09058220684528351, 0.0879785344004631, 0.08541229367256165, 0.08289710432291031, 0.08053935319185257, 0.07809469103813171, 0.07581153512001038, 0.07355624437332153, 0.07133492082357407, 0.06923746317625046, 0.06714025884866714, 0.06511256098747253, 0.06314316391944885, 0.06123119965195656, 0.05937529355287552, 0.057557083666324615, 0.05583435297012329, 0.05416397377848625, 0.0524776354432106, 0.05087820813059807, 0.04935876280069351, 0.047838035970926285, 0.04637293890118599, 0.044959768652915955, 0.043568793684244156, 0.04225369542837143, 0.04095357656478882, 0.03969404473900795, 0.03847236931324005, 0.037312574684619904, 0.036189768463373184, 0.035056885331869125, 0.033993467688560486, 0.03293876722455025, 0.03192973509430885, 0.0309449452906847, 0.030001437291502953, 0.029126375913619995, 0.028227249160408974, 0.027337316423654556, 0.026535993441939354, 0.02576516568660736, 0.02492292784154415, 0.02417547069489956, 0.02346384897828102, 0.022759107872843742, 0.022108662873506546, 0.021409178152680397, 0.020710710436105728, 0.02010739967226982, 0.01947556436061859, 0.018918948248028755, 0.018300319090485573, 0.01773008704185486, 0.017209181562066078, 0.016638796776533127, 0.016140973195433617, 0.015633756294846535, 0.015134720131754875, 0.014639586210250854, 0.014177517034113407, 0.01370970718562603, 0.013317693956196308, 0.012882930226624012, 0.012450090609490871, 0.012087957002222538, 0.011702761985361576, 0.011315356008708477, 0.010955810546875, 0.0106152119114995, 0.010278912261128426, 0.010000352747738361, 0.009623810648918152, 0.0093850614503026, 0.00915272906422615, 0.008799373172223568, 0.008508676663041115, 0.008261500857770443, 0.007991588674485683, 0.007751255761831999, 0.007582066114991903, 0.007328204344958067, 0.007088568527251482, 0.006920965388417244, 0.006700804922729731, 0.006486766040325165, 0.006317368242889643, 0.006136758718639612, 0.005971359554678202, 0.005778525490313768, 0.005612426903098822, 0.005453775636851788, 0.0053823161870241165, 0.005156146828085184, 0.005056710448116064, 0.004941604565829039, 0.004750540480017662, 0.004656645469367504, 0.004497708287090063, 0.004355887416750193, 0.004264088347554207, 0.004151045344769955, 0.00404286477714777, 0.003911311738193035, 0.0038128134328871965, 0.003783582942560315, 0.0036464028526097536, 0.003520734142512083, 0.0034363633021712303, 0.003375046420842409, 0.003255825722590089, 0.00317319855093956, 0.003108660690486431, 0.0030079446732997894, 0.002948505338281393, 0.0028478463646024466, 0.002804376184940338, 0.0027196898590773344, 0.0026690703816711903, 0.0025840532034635544, 0.002541040303185582, 0.0024844370782375336, 0.0024247884284704924, 0.002374680247157812, 0.002354185562580824, 0.0022845296189188957, 0.0022832867689430714, 0.0022087732795625925, 0.002181288320571184, 0.0021797632798552513, 0.0021202319767326117, 0.002080219332128763, 0.0020736022852361202, 0.0020657030399888754, 0.002025269903242588, 0.001982390182092786, 0.0019909546244889498, 0.001967706484720111, 0.0019613325130194426, 0.0019379204604774714, 0.0019338526763021946, 0.0019242969574406743, 0.0019373215036466718, 0.0019691900815814734, 0.0019021877087652683, 0.0019493205472826958, 0.0019696317613124847, 0.0019410249078646302, 0.001894633169285953, 0.0018988895462825894, 0.0018935938132926822]\n\n\n<matplotlib.legend.Legend at 0x1a8731d20>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nModel with no regulation\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\n#model.add(SimpleRNN(\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\n#recurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_3\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_1 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58\n\n\nTrainable params: 58\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n2023-05-01 21:11:46.395105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:46.396819: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:46.398125: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=0)\n\n\n2023-05-01 21:11:46.686521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:46.688955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:46.690539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:47.170871: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:47.173688: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:47.174793: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:48.247903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:48.249946: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:48.251127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/13 [=>............................] - ETA: 3s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 2ms/step\n\n\n2023-05-01 21:11:55.793195: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:55.796589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:55.797711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n1/4 [======>.......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n(405, 30, 1) (405,) (405,) (101, 30, 1) (101,) (101,)\n(405,) (405,)\n0.001985531\n0.0017872186\nTrain MSE = 0.00199 RMSE = 0.04456\nTest MSE = 0.00179 RMSE = 0.04228\n\n\n\n                                                \n\n\n[0.23154202103614807, 0.19227877259254456, 0.1664840281009674, 0.14549386501312256, 0.1273602843284607, 0.1113758459687233, 0.0970463827252388, 0.08406805247068405, 0.07237926870584488, 0.06198374554514885, 0.05270953103899956, 0.0445738285779953, 0.03753545880317688, 0.031545184552669525, 0.026547735556960106, 0.022448353469371796, 0.01925448514521122, 0.01684710942208767, 0.015096431598067284, 0.01390346884727478, 0.013143083080649376, 0.012687419541180134, 0.012388340197503567, 0.012128680013120174, 0.011892511509358883, 0.01160454098135233, 0.011448721401393414, 0.011019553989171982, 0.010729259811341763, 0.010369636118412018, 0.010011440142989159, 0.009682334959506989, 0.009277276694774628, 0.008958418853580952, 0.008547543548047543, 0.008273988962173462, 0.007884925231337547, 0.007636081427335739, 0.0072836014442145824, 0.006993264891207218, 0.006682075094431639, 0.006383806001394987, 0.006090670358389616, 0.005856506060808897, 0.005554206669330597, 0.005363412667065859, 0.005096404813230038, 0.004854407627135515, 0.004630482289940119, 0.004425233695656061, 0.004259099252521992, 0.00403919443488121, 0.003921535797417164, 0.003786623477935791, 0.003621908603236079, 0.0035134698264300823, 0.0034200409427285194, 0.0033463190775364637, 0.0032691822852939367, 0.0032455401960760355, 0.0031191559974104166, 0.003075205720961094, 0.003037015674635768, 0.002986555453389883, 0.003040911862626672, 0.002916044555604458, 0.002882962580770254, 0.002883400535210967, 0.002868739189580083, 0.002816662658005953, 0.002828773809596896, 0.0027802239637821913, 0.002885267371311784, 0.0027700175996869802, 0.002722994890064001, 0.0027167361695319414, 0.002706776838749647, 0.002783282194286585, 0.0026958638336509466, 0.002673647366464138, 0.002637824509292841, 0.0026716813445091248, 0.002605729503557086, 0.002649247646331787, 0.0025853493716567755, 0.0025538853369653225, 0.0026047280989587307, 0.002561460481956601, 0.002527136355638504, 0.0025122067891061306, 0.0025846646167337894, 0.0025132710579782724, 0.0024810200557112694, 0.0024776719510555267, 0.0024763511028140783, 0.0024508496280759573, 0.0024458386469632387, 0.002408430213108659, 0.002416240982711315, 0.0024632387794554234, 0.002449202351272106, 0.002430651104077697, 0.0023833236191421747, 0.002362483413890004, 0.0023703069891780615, 0.002382331294938922, 0.0023365262895822525, 0.002322381129488349, 0.0023213978856801987, 0.0023370853159576654, 0.002330938121303916, 0.0022823945619165897, 0.0022626493591815233, 0.0022802201565355062, 0.002251681638881564, 0.00224294513463974, 0.002241385169327259, 0.0022820851299911737, 0.0022211798932403326, 0.002226771553978324, 0.0022011692635715008, 0.0022150659933686256, 0.002197299152612686, 0.0022007827647030354, 0.0022343462333083153, 0.0022043862845748663, 0.002160316798835993, 0.0022067672107368708, 0.0021476009860634804, 0.002166672144085169, 0.002191637409850955, 0.0021336600184440613, 0.002143392339348793, 0.002117827534675598, 0.0021310129668563604, 0.0020962688140571117, 0.0021090551745146513, 0.0021098207216709852, 0.002109446795657277, 0.002085671527311206, 0.0020897516515105963, 0.002061601262539625, 0.002090279245749116, 0.0020844852551817894, 0.0020512163173407316, 0.0020769911352545023, 0.0020605900790542364, 0.0020526587031781673, 0.002069084672257304, 0.0020255115814507008, 0.0020282554905861616, 0.002013089368119836, 0.002024822635576129, 0.002024324843659997, 0.0020027451682835817, 0.0020325041841715574, 0.001980741508305073, 0.002009419957175851, 0.001986963674426079, 0.0019937765318900347, 0.0019668687600642443, 0.0019668187014758587, 0.002001954009756446, 0.0019734641537070274, 0.001981533132493496, 0.001974727027118206, 0.0019417090807110071, 0.0019760190043598413, 0.00195549544878304, 0.001924636890180409, 0.0019217479275539517, 0.0019470477709546685, 0.0019219968235120177, 0.0019402519101276994, 0.0019520140485838056, 0.0019278335385024548, 0.001911359024234116, 0.0019019325263798237, 0.001938705099746585, 0.001918058842420578, 0.0019064443185925484, 0.001896390225738287, 0.0018932921811938286, 0.0019109994173049927, 0.0018873735098168254, 0.0018822052516043186, 0.0018960678717121482, 0.001862287288531661, 0.0018909001955762506, 0.0019246387528255582, 0.0018782311817631125, 0.0018567803781479597, 0.0019277587998658419, 0.0018551895627751946, 0.0018555395072326064, 0.001848737709224224, 0.001847939332947135, 0.0018857261165976524, 0.0018436183454468846, 0.0018663452938199043]\n\n\n<matplotlib.legend.Legend at 0x1a7d41ff0>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "dl_discharge.html#lstm",
    "href": "dl_discharge.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nBuild model\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_4\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm (LSTM)                 (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_4 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64\n\n\nTrainable params: 64\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n2023-05-01 21:11:56.539409: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:56.541127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:56.542826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nModel and training parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=200\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCreate model\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_5\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_1 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_5 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64\n\n\nTrainable params: 64\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n2023-05-01 21:11:56.822527: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:56.824597: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:56.826064: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=0)\n\n\n2023-05-01 21:11:57.130169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:57.132628: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:57.134422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:57.645825: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:57.648274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:57.650191: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n2023-05-01 21:11:58.554794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:11:58.556967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:11:58.558752: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/13 [=>............................] - ETA: 4s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 2ms/step\n\n\n2023-05-01 21:12:06.211437: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-05-01 21:12:06.213336: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-05-01 21:12:06.214873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n1/4 [======>.......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n(405, 30, 1) (405,) (405,) (101, 30, 1) (101,) (101,)\n(405,) (405,)\n0.004491217\n0.0047633937\nTrain MSE = 0.00449 RMSE = 0.06702\nTest MSE = 0.00476 RMSE = 0.06902\n\n\n\n                                                \n\n\n[1.0176533460617065, 0.9497477412223816, 0.902538537979126, 0.8621962070465088, 0.8256446123123169, 0.7915557026863098, 0.7593333125114441, 0.7286203503608704, 0.6990993022918701, 0.670648455619812, 0.6431789398193359, 0.6166859269142151, 0.5910941958427429, 0.5663237571716309, 0.5423434376716614, 0.5191223621368408, 0.49664556980133057, 0.4749299883842468, 0.4540040194988251, 0.4337618350982666, 0.41423243284225464, 0.3954367935657501, 0.37730053067207336, 0.35980451107025146, 0.34303340315818787, 0.32695573568344116, 0.3115937411785126, 0.29687023162841797, 0.2828998863697052, 0.2696416974067688, 0.2571463882923126, 0.24537882208824158, 0.23431119322776794, 0.22395169734954834, 0.21432015299797058, 0.20543788373470306, 0.1972178816795349, 0.18965891003608704, 0.18276000022888184, 0.1764661818742752, 0.17070730030536652, 0.16542011499404907, 0.1605381965637207, 0.1559676080942154, 0.15174075961112976, 0.14775758981704712, 0.1439659744501114, 0.14034812152385712, 0.13685518503189087, 0.1334870606660843, 0.13015897572040558, 0.12689340114593506, 0.1237279623746872, 0.12061209976673126, 0.11757007986307144, 0.11456337571144104, 0.111615389585495, 0.10889080166816711, 0.1059795394539833, 0.10329976677894592, 0.10064822435379028, 0.09797465801239014, 0.09540580958127975, 0.0928443893790245, 0.0904008224606514, 0.0879736840724945, 0.08559713512659073, 0.08330561220645905, 0.08099265396595001, 0.0788484588265419, 0.07660622894763947, 0.07448678463697433, 0.0724174752831459, 0.07034537196159363, 0.06842953711748123, 0.06646622717380524, 0.06467675417661667, 0.06270912289619446, 0.06100109592080116, 0.05919802561402321, 0.05751366168260574, 0.05579923093318939, 0.0541980043053627, 0.05265374109148979, 0.051059506833553314, 0.049542296677827835, 0.0480433814227581, 0.04660363122820854, 0.04520805552601814, 0.04378642141819, 0.04246830940246582, 0.04117802157998085, 0.039933670312166214, 0.038650304079055786, 0.03743775561451912, 0.03626672923564911, 0.03513926640152931, 0.0341075137257576, 0.03299102187156677, 0.03195952624082565, 0.030931884422898293, 0.02995382994413376, 0.02897142991423607, 0.028054354712367058, 0.02713714726269245, 0.026333773508667946, 0.025499572977423668, 0.02466735802590847, 0.023934554308652878, 0.023122912272810936, 0.022416627034544945, 0.021694624796509743, 0.021192600950598717, 0.020339788869023323, 0.01970394141972065, 0.019184410572052002, 0.01858561858534813, 0.017930762842297554, 0.017364898696541786, 0.016878103837370872, 0.016323348507285118, 0.01585661992430687, 0.015335078351199627, 0.014924979768693447, 0.014385294169187546, 0.014002617448568344, 0.01352753210812807, 0.013236653991043568, 0.012821847572922707, 0.012352966703474522, 0.011983904987573624, 0.01170961931347847, 0.011279378086328506, 0.010988453403115273, 0.010747198946774006, 0.010411174036562443, 0.01013487484306097, 0.009830459021031857, 0.009511404670774937, 0.009377012960612774, 0.009073462337255478, 0.008781752549111843, 0.008664448745548725, 0.008362673223018646, 0.00813625194132328, 0.008020441047847271, 0.007809289265424013, 0.007579836528748274, 0.007433220744132996, 0.007337604649364948, 0.007177890278398991, 0.00696278503164649, 0.006871972698718309, 0.006891277618706226, 0.006598668172955513, 0.006468593142926693, 0.006439580582082272, 0.006226511672139168, 0.006264014169573784, 0.0060500893741846085, 0.005946026183664799, 0.005900935735553503, 0.0058409227058291435, 0.005716957617551088, 0.005641493014991283, 0.005568088032305241, 0.005427916068583727, 0.005452467128634453, 0.005309168715029955, 0.005283710081130266, 0.005148322321474552, 0.005465130787342787, 0.005038856528699398, 0.005038457922637463, 0.004932562820613384, 0.004976186435669661, 0.004852009937167168, 0.004813759587705135, 0.0047822631895542145, 0.004802268464118242, 0.0046907649375498295, 0.004672663751989603, 0.004642048384994268, 0.004578446503728628, 0.004597311373800039, 0.004618233069777489, 0.004552128259092569, 0.004455289803445339, 0.0044319964945316315, 0.004485290963202715, 0.004400874953716993, 0.004428036976605654, 0.004354421515017748, 0.004451636224985123, 0.004330550320446491, 0.004320953972637653, 0.004285729024559259, 0.004312477540224791, 0.004292778670787811, 0.004270952194929123]\n\n\n<matplotlib.legend.Legend at 0x1a9a85660>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "dl_discharge_copy.html",
    "href": "dl_discharge_copy.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.layers import Dense, SimpleRNN,LSTM,GRU\n\n\n\n2023-04-28 22:06:04.759305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "dl_discharge_copy.html#data-processing",
    "href": "dl_discharge_copy.html#data-processing",
    "title": "Deep Learning for TS",
    "section": "Data processing",
    "text": "Data processing\n\nMonthly discharge\n\n\nCode\n## read data\ndaily_discharge = pd.read_csv(\"./data/clean_discharge.csv\")\ndaily_discharge = daily_discharge.rename(columns={\"discharge.date\": \"date\", \"discharge.discharge\": \"discharge\"})\ndaily_discharge['date'] = pd.to_datetime(daily_discharge['date'])\n\n\n\n\nCode\ndaily_discharge\n\n\n\n\n\n\n  \n    \n      \n      date\n      discharge\n    \n  \n  \n    \n      0\n      1970-01-01\n      850.0\n    \n    \n      1\n      1970-01-02\n      850.0\n    \n    \n      2\n      1970-01-03\n      800.0\n    \n    \n      3\n      1970-01-04\n      750.0\n    \n    \n      4\n      1970-01-05\n      750.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      19398\n      2023-02-10\n      28800.0\n    \n    \n      19399\n      2023-02-11\n      26100.0\n    \n    \n      19400\n      2023-02-12\n      17800.0\n    \n    \n      19401\n      2023-02-13\n      11000.0\n    \n    \n      19402\n      2023-02-14\n      8850.0\n    \n  \n\n19403 rows × 2 columns\n\n\n\n\n\nCode\ndaily_discharge.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19403 entries, 0 to 19402\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       19403 non-null  datetime64[ns]\n 1   discharge  19394 non-null  float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 303.3 KB\n\n\n\n\nCode\n\n## log transformation\ndaily_discharge['log_discharge'] = np.log(daily_discharge['discharge'])\n\n## filter 1980-2022\nmask = (daily_discharge['date'] > '1980-01-01') & (daily_discharge['date'] <= '2022-12-31')\n\ndischarge1980 = daily_discharge.loc[mask]\n\n# ## convert to monthly\n# monthly_logmean = discharge1980.groupby(pd.PeriodIndex(discharge1980['date'], freq=\"M\"))['log_discharge'].mean()\n\n# monthly_logmean.index = monthly_logmean.index.strftime('%Y-%m')\n# monthly_logmean_df = monthly_logmean.to_frame().reset_index()\n# print(monthly_logmean_df.shape)\n\n\n\n\n\nCode\ndischarge1980.info()\ndischarge1980.head()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 15705 entries, 3653 to 19357\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   date           15705 non-null  datetime64[ns]\n 1   discharge      15698 non-null  float64       \n 2   log_discharge  15698 non-null  float64       \ndtypes: datetime64[ns](1), float64(2)\nmemory usage: 490.8 KB\n\n\n\n\n\n\n  \n    \n      \n      date\n      discharge\n      log_discharge\n    \n  \n  \n    \n      3653\n      1980-01-02\n      7420.0\n      8.911934\n    \n    \n      3654\n      1980-01-03\n      5770.0\n      8.660427\n    \n    \n      3655\n      1980-01-04\n      4590.0\n      8.431635\n    \n    \n      3656\n      1980-01-05\n      3690.0\n      8.213382\n    \n    \n      3657\n      1980-01-06\n      3330.0\n      8.110728\n    \n  \n\n\n\n\n\n\nCode\nX = np.array(discharge1980[\"log_discharge\"].values.astype('float32')).reshape(discharge1980.shape[0],1)\nprint(X.shape)\n\n\n(15705, 1)"
  },
  {
    "objectID": "dl_discharge_copy.html#visualization-plotting-function",
    "href": "dl_discharge_copy.html#visualization-plotting-function",
    "title": "Deep Learning for TS",
    "section": "Visualization plotting function",
    "text": "Visualization plotting function\n\n\nCode\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\n\n# UTILITY\ndef plotly_line_plot(t,y,title=\"Plot\",x_label=\"t: time (days)\",y_label=\"y(t): log daily discharge\"):\n\n    # GENERATE PLOTLY FIGURE\n    fig = px.line(x=t[0],y=y[0], title=title, render_mode='SVG')  \n    # IMPORTANT: SVG FIXES RENDER ISSUES WITH SAD FACE \n    # https://community.plotly.com/t/plotly-express-line-charts-are-not-shown/39715\n    \n    # ADD MORE\n    for i in range(1,len(y)):\n        if len(t[i])==1:\n            #print(t[i],y[i])\n            fig.add_scatter(x=t[i],y=y[i])\n        else:\n            fig.add_scatter(x=t[i],y=y[i], mode='lines')\n\n    fig.update_layout(\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"plotly_white\",\n        showlegend=False\n    )\n    fig.show()"
  },
  {
    "objectID": "dl_discharge_copy.html#visualize",
    "href": "dl_discharge_copy.html#visualize",
    "title": "Deep Learning for TS",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\n# SINGLE SERIES \nt=[*range(0,len(X))]\nplotly_line_plot([t],[X[:,0]],title=\"Daily log stream discharge since 1980-01\")"
  },
  {
    "objectID": "dl_discharge_copy.html#test-train-split-data",
    "href": "dl_discharge_copy.html#test-train-split-data",
    "title": "Deep Learning for TS",
    "section": "Test-train split data",
    "text": "Test-train split data\nfirst 80% of the data is training, laster 20% of the data is testing\n\n\nCode\n### Utility function: \n# Parameter split_percent defines the ratio of training examples\ndef get_train_test(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\ntrain_data, test_data, data = get_train_test(X)\n\nprint(train_data.shape)\nprint(test_data.shape)\n\n\n(12564,)\n(3141,)\n\n\n\n\nCode\nt1=[*range(0,len(train_data))]\nt2=len(train_data)+np.array([*range(0,len(test_data))])\nplotly_line_plot([t1,t2],[train_data,test_data],title=\"Daily log stream discharge since 1980-01\")"
  },
  {
    "objectID": "dl_discharge_copy.html#re-format-data-into-required-shape",
    "href": "dl_discharge_copy.html#re-format-data-into-required-shape",
    "title": "Deep Learning for TS",
    "section": "Re-format data into required shape",
    "text": "Re-format data into required shape\n\n\nCode\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] #if time_steps=10 remove every 10th entry\n    X = dat[X_ind]; \n\n    #PLOT\n    if(plot_data_partition):\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    #RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n#PARTITION DATA, to monthly\np=30 # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\n\n\n\nCode\nprint(testX.shape,testY.shape)\nprint(trainX.shape,trainY.shape)\nprint(type(trainX))\n\n\n(104, 29, 1) (104,)\n(418, 29, 1) (418,)\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "dl_discharge_copy.html#simple-rnn",
    "href": "dl_discharge_copy.html#simple-rnn",
    "title": "Deep Learning for TS",
    "section": "Simple RNN",
    "text": "Simple RNN\n\nModel and training parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=400\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\n# max_features = 10000    #DEFINES SIZE OF VOCBULARY TO USE\n# maxlen       = 250      #CUTOFF REVIEWS maxlen 20 WORDS)\nbatch_size   = 1000\nverbose      = 1\nembed_dim    = 8        #DIMENSION OF EMBEDING SPACE (SIZE OF VECTOR FOR EACH WORD)\nlr           = 0.001    #LEARNING RATE\nvalidation_split=0.2\n\nprint(trainX.shape,p,trainY.shape)\n\n# trainY=trainY.reshape(trainY.shape[0],1)\n# testY=testY.reshape(testY.shape[0],1)\nprint(p,trainX.shape,testX.shape,trainY.shape,testY.shape)\n\n\n(418, 29, 1) 30 (418,)\n30 (418, 29, 1) (104, 29, 1) (418,) (104,)\n\n\n\n\nCreate model with l2 regulation\n\n\nCode\nfrom tensorflow.keras import regularizers\n\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\nmodel.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1), ##L2 regulation\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nTrain model\n\n\nCode\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\n\nEpoch 1/400\n5/5 - 2s - loss: 2.4285 - val_loss: 2.2127 - 2s/epoch - 304ms/step\nEpoch 2/400\n5/5 - 0s - loss: 2.2319 - val_loss: 2.0643 - 47ms/epoch - 9ms/step\nEpoch 3/400\n5/5 - 0s - loss: 2.0903 - val_loss: 1.9336 - 51ms/epoch - 10ms/step\nEpoch 4/400\n5/5 - 0s - loss: 1.9621 - val_loss: 1.8077 - 72ms/epoch - 14ms/step\nEpoch 5/400\n5/5 - 0s - loss: 1.8376 - val_loss: 1.6859 - 56ms/epoch - 11ms/step\nEpoch 6/400\n5/5 - 0s - loss: 1.7167 - val_loss: 1.5681 - 54ms/epoch - 11ms/step\nEpoch 7/400\n5/5 - 0s - loss: 1.5992 - val_loss: 1.4504 - 49ms/epoch - 10ms/step\nEpoch 8/400\n5/5 - 0s - loss: 1.4816 - val_loss: 1.3310 - 47ms/epoch - 9ms/step\nEpoch 9/400\n5/5 - 0s - loss: 1.3627 - val_loss: 1.2139 - 46ms/epoch - 9ms/step\nEpoch 10/400\n5/5 - 0s - loss: 1.2461 - val_loss: 1.1012 - 48ms/epoch - 10ms/step\nEpoch 11/400\n5/5 - 0s - loss: 1.1335 - val_loss: 0.9962 - 47ms/epoch - 9ms/step\nEpoch 12/400\n5/5 - 0s - loss: 1.0280 - val_loss: 0.8969 - 48ms/epoch - 10ms/step\nEpoch 13/400\n5/5 - 0s - loss: 0.9277 - val_loss: 0.8085 - 44ms/epoch - 9ms/step\nEpoch 14/400\n5/5 - 0s - loss: 0.8375 - val_loss: 0.7280 - 47ms/epoch - 9ms/step\nEpoch 15/400\n5/5 - 0s - loss: 0.7549 - val_loss: 0.6539 - 45ms/epoch - 9ms/step\nEpoch 16/400\n5/5 - 0s - loss: 0.6785 - val_loss: 0.5938 - 55ms/epoch - 11ms/step\nEpoch 17/400\n5/5 - 0s - loss: 0.6151 - val_loss: 0.5344 - 69ms/epoch - 14ms/step\nEpoch 18/400\n5/5 - 0s - loss: 0.5531 - val_loss: 0.4886 - 53ms/epoch - 11ms/step\nEpoch 19/400\n5/5 - 0s - loss: 0.5042 - val_loss: 0.4508 - 67ms/epoch - 13ms/step\nEpoch 20/400\n5/5 - 0s - loss: 0.4634 - val_loss: 0.4145 - 56ms/epoch - 11ms/step\nEpoch 21/400\n5/5 - 0s - loss: 0.4248 - val_loss: 0.3864 - 58ms/epoch - 12ms/step\nEpoch 22/400\n5/5 - 0s - loss: 0.3947 - val_loss: 0.3619 - 50ms/epoch - 10ms/step\nEpoch 23/400\n5/5 - 0s - loss: 0.3686 - val_loss: 0.3410 - 47ms/epoch - 9ms/step\nEpoch 24/400\n5/5 - 0s - loss: 0.3464 - val_loss: 0.3244 - 46ms/epoch - 9ms/step\nEpoch 25/400\n5/5 - 0s - loss: 0.3289 - val_loss: 0.3114 - 47ms/epoch - 9ms/step\nEpoch 26/400\n5/5 - 0s - loss: 0.3154 - val_loss: 0.3032 - 46ms/epoch - 9ms/step\nEpoch 27/400\n5/5 - 0s - loss: 0.3068 - val_loss: 0.2938 - 47ms/epoch - 9ms/step\nEpoch 28/400\n5/5 - 0s - loss: 0.2970 - val_loss: 0.2872 - 44ms/epoch - 9ms/step\nEpoch 29/400\n5/5 - 0s - loss: 0.2902 - val_loss: 0.2794 - 42ms/epoch - 8ms/step\nEpoch 30/400\n5/5 - 0s - loss: 0.2819 - val_loss: 0.2736 - 44ms/epoch - 9ms/step\nEpoch 31/400\n5/5 - 0s - loss: 0.2757 - val_loss: 0.2682 - 44ms/epoch - 9ms/step\nEpoch 32/400\n5/5 - 0s - loss: 0.2701 - val_loss: 0.2621 - 44ms/epoch - 9ms/step\nEpoch 33/400\n5/5 - 0s - loss: 0.2633 - val_loss: 0.2571 - 42ms/epoch - 8ms/step\nEpoch 34/400\n5/5 - 0s - loss: 0.2580 - val_loss: 0.2518 - 41ms/epoch - 8ms/step\nEpoch 35/400\n5/5 - 0s - loss: 0.2522 - val_loss: 0.2472 - 44ms/epoch - 9ms/step\nEpoch 36/400\n5/5 - 0s - loss: 0.2468 - val_loss: 0.2429 - 42ms/epoch - 8ms/step\nEpoch 37/400\n5/5 - 0s - loss: 0.2420 - val_loss: 0.2380 - 45ms/epoch - 9ms/step\nEpoch 38/400\n5/5 - 0s - loss: 0.2375 - val_loss: 0.2339 - 42ms/epoch - 8ms/step\nEpoch 39/400\n5/5 - 0s - loss: 0.2331 - val_loss: 0.2297 - 46ms/epoch - 9ms/step\nEpoch 40/400\n5/5 - 0s - loss: 0.2291 - val_loss: 0.2260 - 46ms/epoch - 9ms/step\nEpoch 41/400\n5/5 - 0s - loss: 0.2246 - val_loss: 0.2223 - 45ms/epoch - 9ms/step\nEpoch 42/400\n5/5 - 0s - loss: 0.2206 - val_loss: 0.2178 - 45ms/epoch - 9ms/step\nEpoch 43/400\n5/5 - 0s - loss: 0.2166 - val_loss: 0.2139 - 46ms/epoch - 9ms/step\nEpoch 44/400\n5/5 - 0s - loss: 0.2130 - val_loss: 0.2103 - 44ms/epoch - 9ms/step\nEpoch 45/400\n5/5 - 0s - loss: 0.2092 - val_loss: 0.2065 - 44ms/epoch - 9ms/step\nEpoch 46/400\n5/5 - 0s - loss: 0.2055 - val_loss: 0.2028 - 43ms/epoch - 9ms/step\nEpoch 47/400\n5/5 - 0s - loss: 0.2019 - val_loss: 0.1994 - 45ms/epoch - 9ms/step\nEpoch 48/400\n5/5 - 0s - loss: 0.1979 - val_loss: 0.1961 - 43ms/epoch - 9ms/step\nEpoch 49/400\n5/5 - 0s - loss: 0.1945 - val_loss: 0.1933 - 42ms/epoch - 8ms/step\nEpoch 50/400\n5/5 - 0s - loss: 0.1912 - val_loss: 0.1888 - 127ms/epoch - 25ms/step\nEpoch 51/400\n5/5 - 0s - loss: 0.1874 - val_loss: 0.1859 - 46ms/epoch - 9ms/step\nEpoch 52/400\n5/5 - 0s - loss: 0.1842 - val_loss: 0.1823 - 44ms/epoch - 9ms/step\nEpoch 53/400\n5/5 - 0s - loss: 0.1809 - val_loss: 0.1788 - 43ms/epoch - 9ms/step\nEpoch 54/400\n5/5 - 0s - loss: 0.1777 - val_loss: 0.1755 - 42ms/epoch - 8ms/step\nEpoch 55/400\n5/5 - 0s - loss: 0.1746 - val_loss: 0.1724 - 42ms/epoch - 8ms/step\nEpoch 56/400\n5/5 - 0s - loss: 0.1712 - val_loss: 0.1692 - 42ms/epoch - 8ms/step\nEpoch 57/400\n5/5 - 0s - loss: 0.1681 - val_loss: 0.1661 - 43ms/epoch - 9ms/step\nEpoch 58/400\n5/5 - 0s - loss: 0.1649 - val_loss: 0.1628 - 40ms/epoch - 8ms/step\nEpoch 59/400\n5/5 - 0s - loss: 0.1618 - val_loss: 0.1599 - 43ms/epoch - 9ms/step\nEpoch 60/400\n5/5 - 0s - loss: 0.1586 - val_loss: 0.1567 - 43ms/epoch - 9ms/step\nEpoch 61/400\n5/5 - 0s - loss: 0.1555 - val_loss: 0.1536 - 42ms/epoch - 8ms/step\nEpoch 62/400\n5/5 - 0s - loss: 0.1525 - val_loss: 0.1506 - 43ms/epoch - 9ms/step\nEpoch 63/400\n5/5 - 0s - loss: 0.1496 - val_loss: 0.1476 - 42ms/epoch - 8ms/step\nEpoch 64/400\n5/5 - 0s - loss: 0.1465 - val_loss: 0.1448 - 41ms/epoch - 8ms/step\nEpoch 65/400\n5/5 - 0s - loss: 0.1434 - val_loss: 0.1416 - 42ms/epoch - 8ms/step\nEpoch 66/400\n5/5 - 0s - loss: 0.1405 - val_loss: 0.1387 - 42ms/epoch - 8ms/step\nEpoch 67/400\n5/5 - 0s - loss: 0.1376 - val_loss: 0.1359 - 42ms/epoch - 8ms/step\nEpoch 68/400\n5/5 - 0s - loss: 0.1350 - val_loss: 0.1331 - 41ms/epoch - 8ms/step\nEpoch 69/400\n5/5 - 0s - loss: 0.1320 - val_loss: 0.1303 - 43ms/epoch - 9ms/step\nEpoch 70/400\n5/5 - 0s - loss: 0.1293 - val_loss: 0.1276 - 42ms/epoch - 8ms/step\nEpoch 71/400\n5/5 - 0s - loss: 0.1266 - val_loss: 0.1251 - 43ms/epoch - 9ms/step\nEpoch 72/400\n5/5 - 0s - loss: 0.1238 - val_loss: 0.1224 - 44ms/epoch - 9ms/step\nEpoch 73/400\n5/5 - 0s - loss: 0.1213 - val_loss: 0.1201 - 42ms/epoch - 8ms/step\nEpoch 74/400\n5/5 - 0s - loss: 0.1187 - val_loss: 0.1172 - 44ms/epoch - 9ms/step\nEpoch 75/400\n5/5 - 0s - loss: 0.1161 - val_loss: 0.1151 - 43ms/epoch - 9ms/step\nEpoch 76/400\n5/5 - 0s - loss: 0.1137 - val_loss: 0.1123 - 41ms/epoch - 8ms/step\nEpoch 77/400\n5/5 - 0s - loss: 0.1111 - val_loss: 0.1098 - 42ms/epoch - 8ms/step\nEpoch 78/400\n5/5 - 0s - loss: 0.1087 - val_loss: 0.1072 - 41ms/epoch - 8ms/step\nEpoch 79/400\n5/5 - 0s - loss: 0.1062 - val_loss: 0.1050 - 42ms/epoch - 8ms/step\nEpoch 80/400\n5/5 - 0s - loss: 0.1039 - val_loss: 0.1025 - 43ms/epoch - 9ms/step\nEpoch 81/400\n5/5 - 0s - loss: 0.1015 - val_loss: 0.1002 - 43ms/epoch - 9ms/step\nEpoch 82/400\n5/5 - 0s - loss: 0.0991 - val_loss: 0.0979 - 42ms/epoch - 8ms/step\nEpoch 83/400\n5/5 - 0s - loss: 0.0968 - val_loss: 0.0956 - 44ms/epoch - 9ms/step\nEpoch 84/400\n5/5 - 0s - loss: 0.0946 - val_loss: 0.0935 - 43ms/epoch - 9ms/step\nEpoch 85/400\n5/5 - 0s - loss: 0.0923 - val_loss: 0.0917 - 44ms/epoch - 9ms/step\nEpoch 86/400\n5/5 - 0s - loss: 0.0904 - val_loss: 0.0891 - 42ms/epoch - 8ms/step\nEpoch 87/400\n5/5 - 0s - loss: 0.0880 - val_loss: 0.0870 - 46ms/epoch - 9ms/step\nEpoch 88/400\n5/5 - 0s - loss: 0.0859 - val_loss: 0.0849 - 44ms/epoch - 9ms/step\nEpoch 89/400\n5/5 - 0s - loss: 0.0839 - val_loss: 0.0829 - 44ms/epoch - 9ms/step\nEpoch 90/400\n5/5 - 0s - loss: 0.0818 - val_loss: 0.0807 - 49ms/epoch - 10ms/step\nEpoch 91/400\n5/5 - 0s - loss: 0.0798 - val_loss: 0.0787 - 52ms/epoch - 10ms/step\nEpoch 92/400\n5/5 - 0s - loss: 0.0778 - val_loss: 0.0768 - 45ms/epoch - 9ms/step\nEpoch 93/400\n5/5 - 0s - loss: 0.0758 - val_loss: 0.0748 - 44ms/epoch - 9ms/step\nEpoch 94/400\n5/5 - 0s - loss: 0.0738 - val_loss: 0.0728 - 42ms/epoch - 8ms/step\nEpoch 95/400\n5/5 - 0s - loss: 0.0719 - val_loss: 0.0709 - 42ms/epoch - 8ms/step\nEpoch 96/400\n5/5 - 0s - loss: 0.0700 - val_loss: 0.0692 - 44ms/epoch - 9ms/step\nEpoch 97/400\n5/5 - 0s - loss: 0.0682 - val_loss: 0.0673 - 42ms/epoch - 8ms/step\nEpoch 98/400\n5/5 - 0s - loss: 0.0663 - val_loss: 0.0655 - 52ms/epoch - 10ms/step\nEpoch 99/400\n5/5 - 0s - loss: 0.0645 - val_loss: 0.0639 - 47ms/epoch - 9ms/step\nEpoch 100/400\n5/5 - 0s - loss: 0.0629 - val_loss: 0.0621 - 41ms/epoch - 8ms/step\nEpoch 101/400\n5/5 - 0s - loss: 0.0611 - val_loss: 0.0604 - 44ms/epoch - 9ms/step\nEpoch 102/400\n5/5 - 0s - loss: 0.0595 - val_loss: 0.0589 - 49ms/epoch - 10ms/step\nEpoch 103/400\n5/5 - 0s - loss: 0.0579 - val_loss: 0.0572 - 50ms/epoch - 10ms/step\nEpoch 104/400\n5/5 - 0s - loss: 0.0562 - val_loss: 0.0557 - 68ms/epoch - 14ms/step\nEpoch 105/400\n5/5 - 0s - loss: 0.0547 - val_loss: 0.0540 - 69ms/epoch - 14ms/step\nEpoch 106/400\n5/5 - 0s - loss: 0.0531 - val_loss: 0.0525 - 47ms/epoch - 9ms/step\nEpoch 107/400\n5/5 - 0s - loss: 0.0516 - val_loss: 0.0510 - 123ms/epoch - 25ms/step\nEpoch 108/400\n5/5 - 0s - loss: 0.0501 - val_loss: 0.0495 - 66ms/epoch - 13ms/step\nEpoch 109/400\n5/5 - 0s - loss: 0.0486 - val_loss: 0.0483 - 48ms/epoch - 10ms/step\nEpoch 110/400\n5/5 - 0s - loss: 0.0474 - val_loss: 0.0470 - 45ms/epoch - 9ms/step\nEpoch 111/400\n5/5 - 0s - loss: 0.0460 - val_loss: 0.0454 - 42ms/epoch - 8ms/step\nEpoch 112/400\n5/5 - 0s - loss: 0.0445 - val_loss: 0.0440 - 42ms/epoch - 8ms/step\nEpoch 113/400\n5/5 - 0s - loss: 0.0432 - val_loss: 0.0428 - 42ms/epoch - 8ms/step\nEpoch 114/400\n5/5 - 0s - loss: 0.0419 - val_loss: 0.0414 - 43ms/epoch - 9ms/step\nEpoch 115/400\n5/5 - 0s - loss: 0.0406 - val_loss: 0.0402 - 42ms/epoch - 8ms/step\nEpoch 116/400\n5/5 - 0s - loss: 0.0393 - val_loss: 0.0390 - 62ms/epoch - 12ms/step\nEpoch 117/400\n5/5 - 0s - loss: 0.0381 - val_loss: 0.0379 - 50ms/epoch - 10ms/step\nEpoch 118/400\n5/5 - 0s - loss: 0.0369 - val_loss: 0.0368 - 43ms/epoch - 9ms/step\nEpoch 119/400\n5/5 - 0s - loss: 0.0358 - val_loss: 0.0355 - 42ms/epoch - 8ms/step\nEpoch 120/400\n5/5 - 0s - loss: 0.0345 - val_loss: 0.0344 - 41ms/epoch - 8ms/step\nEpoch 121/400\n5/5 - 0s - loss: 0.0334 - val_loss: 0.0332 - 40ms/epoch - 8ms/step\nEpoch 122/400\n5/5 - 0s - loss: 0.0323 - val_loss: 0.0321 - 43ms/epoch - 9ms/step\nEpoch 123/400\n5/5 - 0s - loss: 0.0312 - val_loss: 0.0311 - 42ms/epoch - 8ms/step\nEpoch 124/400\n5/5 - 0s - loss: 0.0302 - val_loss: 0.0301 - 45ms/epoch - 9ms/step\nEpoch 125/400\n5/5 - 0s - loss: 0.0292 - val_loss: 0.0295 - 49ms/epoch - 10ms/step\nEpoch 126/400\n5/5 - 0s - loss: 0.0283 - val_loss: 0.0282 - 48ms/epoch - 10ms/step\nEpoch 127/400\n5/5 - 0s - loss: 0.0272 - val_loss: 0.0274 - 57ms/epoch - 11ms/step\nEpoch 128/400\n5/5 - 0s - loss: 0.0263 - val_loss: 0.0263 - 51ms/epoch - 10ms/step\nEpoch 129/400\n5/5 - 0s - loss: 0.0254 - val_loss: 0.0255 - 46ms/epoch - 9ms/step\nEpoch 130/400\n5/5 - 0s - loss: 0.0245 - val_loss: 0.0249 - 41ms/epoch - 8ms/step\nEpoch 131/400\n5/5 - 0s - loss: 0.0237 - val_loss: 0.0238 - 39ms/epoch - 8ms/step\nEpoch 132/400\n5/5 - 0s - loss: 0.0228 - val_loss: 0.0230 - 44ms/epoch - 9ms/step\nEpoch 133/400\n5/5 - 0s - loss: 0.0220 - val_loss: 0.0222 - 43ms/epoch - 9ms/step\nEpoch 134/400\n5/5 - 0s - loss: 0.0213 - val_loss: 0.0215 - 44ms/epoch - 9ms/step\nEpoch 135/400\n5/5 - 0s - loss: 0.0205 - val_loss: 0.0208 - 42ms/epoch - 8ms/step\nEpoch 136/400\n5/5 - 0s - loss: 0.0199 - val_loss: 0.0200 - 42ms/epoch - 8ms/step\nEpoch 137/400\n5/5 - 0s - loss: 0.0191 - val_loss: 0.0193 - 43ms/epoch - 9ms/step\nEpoch 138/400\n5/5 - 0s - loss: 0.0184 - val_loss: 0.0187 - 42ms/epoch - 8ms/step\nEpoch 139/400\n5/5 - 0s - loss: 0.0177 - val_loss: 0.0182 - 43ms/epoch - 9ms/step\nEpoch 140/400\n5/5 - 0s - loss: 0.0171 - val_loss: 0.0174 - 42ms/epoch - 8ms/step\nEpoch 141/400\n5/5 - 0s - loss: 0.0164 - val_loss: 0.0168 - 42ms/epoch - 8ms/step\nEpoch 142/400\n5/5 - 0s - loss: 0.0159 - val_loss: 0.0162 - 42ms/epoch - 8ms/step\nEpoch 143/400\n5/5 - 0s - loss: 0.0154 - val_loss: 0.0157 - 43ms/epoch - 9ms/step\nEpoch 144/400\n5/5 - 0s - loss: 0.0148 - val_loss: 0.0151 - 43ms/epoch - 9ms/step\nEpoch 145/400\n5/5 - 0s - loss: 0.0142 - val_loss: 0.0146 - 44ms/epoch - 9ms/step\nEpoch 146/400\n5/5 - 0s - loss: 0.0137 - val_loss: 0.0141 - 45ms/epoch - 9ms/step\nEpoch 147/400\n5/5 - 0s - loss: 0.0132 - val_loss: 0.0136 - 42ms/epoch - 8ms/step\nEpoch 148/400\n5/5 - 0s - loss: 0.0128 - val_loss: 0.0132 - 43ms/epoch - 9ms/step\nEpoch 149/400\n5/5 - 0s - loss: 0.0122 - val_loss: 0.0127 - 44ms/epoch - 9ms/step\nEpoch 150/400\n5/5 - 0s - loss: 0.0118 - val_loss: 0.0125 - 43ms/epoch - 9ms/step\nEpoch 151/400\n5/5 - 0s - loss: 0.0114 - val_loss: 0.0120 - 43ms/epoch - 9ms/step\nEpoch 152/400\n5/5 - 0s - loss: 0.0110 - val_loss: 0.0115 - 43ms/epoch - 9ms/step\nEpoch 153/400\n5/5 - 0s - loss: 0.0106 - val_loss: 0.0112 - 41ms/epoch - 8ms/step\nEpoch 154/400\n5/5 - 0s - loss: 0.0102 - val_loss: 0.0107 - 43ms/epoch - 9ms/step\nEpoch 155/400\n5/5 - 0s - loss: 0.0098 - val_loss: 0.0103 - 41ms/epoch - 8ms/step\nEpoch 156/400\n5/5 - 0s - loss: 0.0094 - val_loss: 0.0099 - 42ms/epoch - 8ms/step\nEpoch 157/400\n5/5 - 0s - loss: 0.0091 - val_loss: 0.0097 - 42ms/epoch - 8ms/step\nEpoch 158/400\n5/5 - 0s - loss: 0.0088 - val_loss: 0.0094 - 42ms/epoch - 8ms/step\nEpoch 159/400\n5/5 - 0s - loss: 0.0084 - val_loss: 0.0090 - 42ms/epoch - 8ms/step\nEpoch 160/400\n5/5 - 0s - loss: 0.0081 - val_loss: 0.0087 - 42ms/epoch - 8ms/step\nEpoch 161/400\n5/5 - 0s - loss: 0.0079 - val_loss: 0.0083 - 44ms/epoch - 9ms/step\nEpoch 162/400\n5/5 - 0s - loss: 0.0075 - val_loss: 0.0081 - 43ms/epoch - 9ms/step\nEpoch 163/400\n5/5 - 0s - loss: 0.0072 - val_loss: 0.0077 - 46ms/epoch - 9ms/step\nEpoch 164/400\n5/5 - 0s - loss: 0.0069 - val_loss: 0.0076 - 43ms/epoch - 9ms/step\nEpoch 165/400\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0072 - 42ms/epoch - 8ms/step\nEpoch 166/400\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0069 - 43ms/epoch - 9ms/step\nEpoch 167/400\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0067 - 42ms/epoch - 8ms/step\nEpoch 168/400\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0064 - 54ms/epoch - 11ms/step\nEpoch 169/400\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0063 - 85ms/epoch - 17ms/step\nEpoch 170/400\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0063 - 41ms/epoch - 8ms/step\nEpoch 171/400\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0058 - 43ms/epoch - 9ms/step\nEpoch 172/400\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0059 - 42ms/epoch - 8ms/step\nEpoch 173/400\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0054 - 41ms/epoch - 8ms/step\nEpoch 174/400\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0052 - 43ms/epoch - 9ms/step\nEpoch 175/400\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0054 - 42ms/epoch - 8ms/step\nEpoch 176/400\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0049 - 42ms/epoch - 8ms/step\nEpoch 177/400\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0051 - 42ms/epoch - 8ms/step\nEpoch 178/400\n5/5 - 0s - loss: 0.0039 - val_loss: 0.0048 - 42ms/epoch - 8ms/step\nEpoch 179/400\n5/5 - 0s - loss: 0.0037 - val_loss: 0.0044 - 42ms/epoch - 8ms/step\nEpoch 180/400\n5/5 - 0s - loss: 0.0036 - val_loss: 0.0044 - 43ms/epoch - 9ms/step\nEpoch 181/400\n5/5 - 0s - loss: 0.0034 - val_loss: 0.0041 - 41ms/epoch - 8ms/step\nEpoch 182/400\n5/5 - 0s - loss: 0.0033 - val_loss: 0.0042 - 44ms/epoch - 9ms/step\nEpoch 183/400\n5/5 - 0s - loss: 0.0032 - val_loss: 0.0039 - 41ms/epoch - 8ms/step\nEpoch 184/400\n5/5 - 0s - loss: 0.0031 - val_loss: 0.0037 - 41ms/epoch - 8ms/step\nEpoch 185/400\n5/5 - 0s - loss: 0.0029 - val_loss: 0.0037 - 42ms/epoch - 8ms/step\nEpoch 186/400\n5/5 - 0s - loss: 0.0028 - val_loss: 0.0036 - 40ms/epoch - 8ms/step\nEpoch 187/400\n5/5 - 0s - loss: 0.0028 - val_loss: 0.0034 - 43ms/epoch - 9ms/step\nEpoch 188/400\n5/5 - 0s - loss: 0.0026 - val_loss: 0.0034 - 64ms/epoch - 13ms/step\nEpoch 189/400\n5/5 - 0s - loss: 0.0026 - val_loss: 0.0033 - 41ms/epoch - 8ms/step\nEpoch 190/400\n5/5 - 0s - loss: 0.0024 - val_loss: 0.0031 - 41ms/epoch - 8ms/step\nEpoch 191/400\n5/5 - 0s - loss: 0.0024 - val_loss: 0.0032 - 44ms/epoch - 9ms/step\nEpoch 192/400\n5/5 - 0s - loss: 0.0023 - val_loss: 0.0032 - 43ms/epoch - 9ms/step\nEpoch 193/400\n5/5 - 0s - loss: 0.0022 - val_loss: 0.0029 - 42ms/epoch - 8ms/step\nEpoch 194/400\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0029 - 42ms/epoch - 8ms/step\nEpoch 195/400\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0032 - 45ms/epoch - 9ms/step\nEpoch 196/400\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0028 - 44ms/epoch - 9ms/step\nEpoch 197/400\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0030 - 43ms/epoch - 9ms/step\nEpoch 198/400\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 44ms/epoch - 9ms/step\nEpoch 199/400\n5/5 - 0s - loss: 0.0019 - val_loss: 0.0028 - 41ms/epoch - 8ms/step\nEpoch 200/400\n5/5 - 0s - loss: 0.0019 - val_loss: 0.0028 - 42ms/epoch - 8ms/step\nEpoch 201/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 202/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0025 - 40ms/epoch - 8ms/step\nEpoch 203/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0027 - 42ms/epoch - 8ms/step\nEpoch 204/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0028 - 42ms/epoch - 8ms/step\nEpoch 205/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0026 - 41ms/epoch - 8ms/step\nEpoch 206/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 207/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 43ms/epoch - 9ms/step\nEpoch 208/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 209/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 210/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 211/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 212/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0028 - 41ms/epoch - 8ms/step\nEpoch 213/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0027 - 43ms/epoch - 9ms/step\nEpoch 214/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0026 - 42ms/epoch - 8ms/step\nEpoch 215/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 216/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 217/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 218/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 219/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 220/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 221/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 42ms/epoch - 8ms/step\nEpoch 222/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0026 - 42ms/epoch - 8ms/step\nEpoch 223/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 224/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 225/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 226/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 227/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 228/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 131ms/epoch - 26ms/step\nEpoch 229/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 45ms/epoch - 9ms/step\nEpoch 230/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 231/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 232/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 233/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 234/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 235/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 236/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 237/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 238/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 239/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 240/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 241/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 242/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 243/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 42ms/epoch - 8ms/step\nEpoch 244/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 245/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 43ms/epoch - 9ms/step\nEpoch 246/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 37ms/epoch - 7ms/step\nEpoch 247/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 38ms/epoch - 8ms/step\nEpoch 248/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 37ms/epoch - 7ms/step\nEpoch 249/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 37ms/epoch - 7ms/step\nEpoch 250/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 39ms/epoch - 8ms/step\nEpoch 251/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 38ms/epoch - 8ms/step\nEpoch 252/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0028 - 44ms/epoch - 9ms/step\nEpoch 253/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 45ms/epoch - 9ms/step\nEpoch 254/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 43ms/epoch - 9ms/step\nEpoch 255/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 39ms/epoch - 8ms/step\nEpoch 256/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 49ms/epoch - 10ms/step\nEpoch 257/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 51ms/epoch - 10ms/step\nEpoch 258/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 53ms/epoch - 11ms/step\nEpoch 259/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 49ms/epoch - 10ms/step\nEpoch 260/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 261/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 262/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 263/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 264/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 40ms/epoch - 8ms/step\nEpoch 265/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 266/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 267/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 268/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 44ms/epoch - 9ms/step\nEpoch 269/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 270/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 271/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 272/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 273/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 274/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 275/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 276/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 277/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 133ms/epoch - 27ms/step\nEpoch 278/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 44ms/epoch - 9ms/step\nEpoch 279/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 280/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 281/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 282/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0028 - 42ms/epoch - 8ms/step\nEpoch 283/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 284/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 42ms/epoch - 8ms/step\nEpoch 285/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 286/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 287/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 40ms/epoch - 8ms/step\nEpoch 288/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 289/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 45ms/epoch - 9ms/step\nEpoch 290/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 291/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 292/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 293/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 294/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 295/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 296/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 297/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 298/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 299/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 300/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 301/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 302/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 303/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 304/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 305/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 306/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 307/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 44ms/epoch - 9ms/step\nEpoch 308/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 309/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 310/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 41ms/epoch - 8ms/step\nEpoch 311/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 312/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 43ms/epoch - 9ms/step\nEpoch 313/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 314/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 315/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 316/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 41ms/epoch - 8ms/step\nEpoch 317/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 318/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 319/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 40ms/epoch - 8ms/step\nEpoch 320/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 40ms/epoch - 8ms/step\nEpoch 321/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 322/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 323/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 324/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 40ms/epoch - 8ms/step\nEpoch 325/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 326/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 105ms/epoch - 21ms/step\nEpoch 327/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 44ms/epoch - 9ms/step\nEpoch 328/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 329/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 330/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 40ms/epoch - 8ms/step\nEpoch 331/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 42ms/epoch - 8ms/step\nEpoch 332/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 333/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 41ms/epoch - 8ms/step\nEpoch 334/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 335/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 336/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 337/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 38ms/epoch - 8ms/step\nEpoch 338/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 339/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 340/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 42ms/epoch - 8ms/step\nEpoch 341/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 41ms/epoch - 8ms/step\nEpoch 342/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 40ms/epoch - 8ms/step\nEpoch 343/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 44ms/epoch - 9ms/step\nEpoch 344/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0026 - 42ms/epoch - 8ms/step\nEpoch 345/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 41ms/epoch - 8ms/step\nEpoch 346/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 45ms/epoch - 9ms/step\nEpoch 347/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 43ms/epoch - 9ms/step\nEpoch 348/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 349/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 44ms/epoch - 9ms/step\nEpoch 350/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 92ms/epoch - 18ms/step\nEpoch 351/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 202ms/epoch - 40ms/step\nEpoch 352/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 112ms/epoch - 22ms/step\nEpoch 353/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 56ms/epoch - 11ms/step\nEpoch 354/400\n5/5 - 0s - loss: 0.0018 - val_loss: 0.0024 - 74ms/epoch - 15ms/step\nEpoch 355/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 133ms/epoch - 27ms/step\nEpoch 356/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 74ms/epoch - 15ms/step\nEpoch 357/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 101ms/epoch - 20ms/step\nEpoch 358/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 94ms/epoch - 19ms/step\nEpoch 359/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 138ms/epoch - 28ms/step\nEpoch 360/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 127ms/epoch - 25ms/step\nEpoch 361/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0027 - 114ms/epoch - 23ms/step\nEpoch 362/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 74ms/epoch - 15ms/step\nEpoch 363/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 129ms/epoch - 26ms/step\nEpoch 364/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 125ms/epoch - 25ms/step\nEpoch 365/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 85ms/epoch - 17ms/step\nEpoch 366/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 109ms/epoch - 22ms/step\nEpoch 367/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 133ms/epoch - 27ms/step\nEpoch 368/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 111ms/epoch - 22ms/step\nEpoch 369/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 128ms/epoch - 26ms/step\nEpoch 370/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 134ms/epoch - 27ms/step\nEpoch 371/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 79ms/epoch - 16ms/step\nEpoch 372/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 86ms/epoch - 17ms/step\nEpoch 373/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 157ms/epoch - 31ms/step\nEpoch 374/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 91ms/epoch - 18ms/step\nEpoch 375/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 255ms/epoch - 51ms/step\nEpoch 376/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0025 - 166ms/epoch - 33ms/step\nEpoch 377/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 87ms/epoch - 17ms/step\nEpoch 378/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 178ms/epoch - 36ms/step\nEpoch 379/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 185ms/epoch - 37ms/step\nEpoch 380/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 76ms/epoch - 15ms/step\nEpoch 381/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 73ms/epoch - 15ms/step\nEpoch 382/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 60ms/epoch - 12ms/step\nEpoch 383/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0027 - 81ms/epoch - 16ms/step\nEpoch 384/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 103ms/epoch - 21ms/step\nEpoch 385/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 58ms/epoch - 12ms/step\nEpoch 386/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 108ms/epoch - 22ms/step\nEpoch 387/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 106ms/epoch - 21ms/step\nEpoch 388/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 81ms/epoch - 16ms/step\nEpoch 389/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0025 - 68ms/epoch - 14ms/step\nEpoch 390/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 89ms/epoch - 18ms/step\nEpoch 391/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 51ms/epoch - 10ms/step\nEpoch 392/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 42ms/epoch - 8ms/step\nEpoch 393/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0024 - 43ms/epoch - 9ms/step\nEpoch 394/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 44ms/epoch - 9ms/step\nEpoch 395/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0026 - 44ms/epoch - 9ms/step\nEpoch 396/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 397/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0023 - 43ms/epoch - 9ms/step\nEpoch 398/400\n5/5 - 0s - loss: 0.0016 - val_loss: 0.0026 - 43ms/epoch - 9ms/step\nEpoch 399/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0024 - 44ms/epoch - 9ms/step\nEpoch 400/400\n5/5 - 0s - loss: 0.0017 - val_loss: 0.0023 - 47ms/epoch - 9ms/step\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Training epoch vs errors\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n14/14 [==============================] - 0s 2ms/step\n4/4 [==============================] - 0s 3ms/step\n(418, 29, 1) (418,) (418,) (104, 29, 1) (104,) (104,)\n(418,) (418,)\n0.0017489118\n0.0015717533\nTrain MSE = 0.00175 RMSE = 0.04182\nTest MSE = 0.00157 RMSE = 0.03965\n\n\n\n                                                \n\n\n[2.4284820556640625, 2.2318687438964844, 2.0902767181396484, 1.962050437927246, 1.8376444578170776, 1.716717004776001, 1.5991896390914917, 1.4816266298294067, 1.3626973628997803, 1.2461438179016113, 1.133485198020935, 1.0280439853668213, 0.927708625793457, 0.8375345468521118, 0.7548702359199524, 0.6784657835960388, 0.6151179075241089, 0.5531068444252014, 0.5041781067848206, 0.4633999764919281, 0.42478489875793457, 0.39466190338134766, 0.3685621917247772, 0.3464075028896332, 0.32892173528671265, 0.31537333130836487, 0.3067549765110016, 0.29699426889419556, 0.2902131974697113, 0.2819463610649109, 0.275716096162796, 0.2701210081577301, 0.26332762837409973, 0.25804397463798523, 0.25218451023101807, 0.24684147536754608, 0.24200670421123505, 0.2374621480703354, 0.23313823342323303, 0.22909538447856903, 0.2246454656124115, 0.22056467831134796, 0.21664024889469147, 0.21296000480651855, 0.2091941088438034, 0.20548821985721588, 0.2019054889678955, 0.19794413447380066, 0.1945054978132248, 0.19120199978351593, 0.187399759888649, 0.1841520071029663, 0.18085047602653503, 0.17770151793956757, 0.17456942796707153, 0.1711607277393341, 0.16806460916996002, 0.164877787232399, 0.1617864966392517, 0.15856915712356567, 0.15554778277873993, 0.15247811377048492, 0.14958545565605164, 0.14654554426670074, 0.14343149960041046, 0.1404823213815689, 0.1376086324453354, 0.13496270775794983, 0.1319829225540161, 0.12929633259773254, 0.1266232281923294, 0.1238144189119339, 0.12126720696687698, 0.11873719096183777, 0.11605911701917648, 0.11369097232818604, 0.1111263781785965, 0.10870964080095291, 0.10623852908611298, 0.10385789722204208, 0.10151436179876328, 0.09912492334842682, 0.0968451276421547, 0.09459076076745987, 0.09233015030622482, 0.0903557762503624, 0.08801816403865814, 0.08592895418405533, 0.0838581845164299, 0.08176887035369873, 0.07976227253675461, 0.07775640487670898, 0.07575391978025436, 0.0737847164273262, 0.07188794016838074, 0.06998216360807419, 0.06821250915527344, 0.0663081556558609, 0.06452625244855881, 0.06286758929491043, 0.0611296109855175, 0.059530194848775864, 0.05794072151184082, 0.05624007433652878, 0.054727718234062195, 0.05308583378791809, 0.05158230662345886, 0.05006635561585426, 0.048611074686050415, 0.04735930263996124, 0.046040523797273636, 0.044484786689281464, 0.04316096380352974, 0.04189546778798103, 0.04056303948163986, 0.0392908938229084, 0.03806459531188011, 0.03692756965756416, 0.03575044125318527, 0.034524742513895035, 0.03344516083598137, 0.032281745225191116, 0.031231485307216644, 0.03019242361187935, 0.029204942286014557, 0.02831779047846794, 0.027242178097367287, 0.02634492516517639, 0.025399498641490936, 0.024542493745684624, 0.02373851276934147, 0.02284029684960842, 0.022022932767868042, 0.021272601559758186, 0.020494874566793442, 0.019917171448469162, 0.019053520634770393, 0.018364278599619865, 0.01771293208003044, 0.01709473691880703, 0.016422089189291, 0.0159078948199749, 0.01537048164755106, 0.014849617145955563, 0.014186440967023373, 0.013664331287145615, 0.013164696283638477, 0.012757834047079086, 0.012231235392391682, 0.011783046647906303, 0.011393445543944836, 0.010980356484651566, 0.010554983280599117, 0.010183809325098991, 0.009792853146791458, 0.009416202083230019, 0.009070201776921749, 0.008820326998829842, 0.008398713544011116, 0.008064134046435356, 0.007851649075746536, 0.007461511995643377, 0.0071501717902719975, 0.006874388549476862, 0.006717680487781763, 0.006331215146929026, 0.0061090984381735325, 0.00584033876657486, 0.005603510420769453, 0.005376383196562529, 0.005181518383324146, 0.004960233345627785, 0.004754663910716772, 0.004584088921546936, 0.004331874195486307, 0.004271596670150757, 0.003999588545411825, 0.003928836435079575, 0.0037327155005186796, 0.003551794681698084, 0.0034263976849615574, 0.0032934846822172403, 0.003162977984175086, 0.0030995726119726896, 0.00291839730925858, 0.00279157399199903, 0.0027735999319702387, 0.0026156664825975895, 0.002612548414617777, 0.0024258214980363846, 0.0023617795668542385, 0.002287394367158413, 0.002232489176094532, 0.0021393708884716034, 0.0020837076008319855, 0.0020816035103052855, 0.0019596789497882128, 0.001955389743670821, 0.0018884098390117288, 0.0018613061401993036, 0.0018393544014543295, 0.00175622315146029, 0.0018084784969687462, 0.0017402955563738942, 0.001761551946401596, 0.0017002917593345046, 0.0016474418807774782, 0.0016665176954120398, 0.0016533802263438702, 0.0016227749874815345, 0.001629541744478047, 0.0016131025040522218, 0.001684392336755991, 0.0016818862641230226, 0.0016653684433549643, 0.0016007517697289586, 0.0017088719177991152, 0.001606921898201108, 0.0016510343411937356, 0.0016123270615935326, 0.0016056392341852188, 0.00165063445456326, 0.0016392635880038142, 0.0016249401960521936, 0.0016119523206725717, 0.0016527008265256882, 0.001648745615966618, 0.0016601759707555175, 0.0015994224231690168, 0.0016809223452582955, 0.001663501956500113, 0.0016299823764711618, 0.0016040389891713858, 0.0017408871790394187, 0.001604305231012404, 0.001599488197825849, 0.001607580459676683, 0.001687221578322351, 0.0016257455572485924, 0.0016036245506256819, 0.0016062702052295208, 0.0015990904066711664, 0.0016430576797574759, 0.0016399153973907232, 0.00160034722648561, 0.0016445278888568282, 0.001609930070117116, 0.0016002168413251638, 0.001667918753810227, 0.0016041453927755356, 0.0016144159017130733, 0.0015946185449138284, 0.0017046596622094512, 0.0016261124983429909, 0.0016359867295250297, 0.0016228677704930305, 0.001708601019345224, 0.0017362029757350683, 0.0016324148746207356, 0.0016111601144075394, 0.0016200467944145203, 0.0016159110236912966, 0.0016170474700629711, 0.0016174031188711524, 0.0016236669616773725, 0.0016529308632016182, 0.0016186940483748913, 0.0016079675406217575, 0.0017020625527948141, 0.0016257703537121415, 0.001607741927728057, 0.001600008923560381, 0.001601934083737433, 0.0016260059783235192, 0.0016348854405805469, 0.001695021754130721, 0.0016122550005093217, 0.0017218011198565364, 0.0016094110906124115, 0.0016265336889773607, 0.0016005808720365167, 0.0016004678327590227, 0.0017514409264549613, 0.0016089996788650751, 0.001717509818263352, 0.0016096759354695678, 0.0016081065405160189, 0.0016444312641397119, 0.0017228586366400123, 0.0018187068635597825, 0.0016096439212560654, 0.0016231791814789176, 0.0016016260487958789, 0.001602714997716248, 0.0016029420075938106, 0.001629421953111887, 0.0016036214074119925, 0.0016117346240207553, 0.001633085310459137, 0.0016966412076726556, 0.0018094482365995646, 0.0016103580128401518, 0.0016015409491956234, 0.0017335546435788274, 0.0016013029962778091, 0.0016442517517134547, 0.0016476375749334693, 0.001608805381692946, 0.001618087524548173, 0.0016002628253772855, 0.001656882232055068, 0.0016186385182663798, 0.0016899543115869164, 0.0016166894929483533, 0.001664277515374124, 0.0016375307459384203, 0.0016496579628437757, 0.0016114118043333292, 0.0016243222635239363, 0.0016024088254198432, 0.0016725632594898343, 0.001646813121624291, 0.0017052958719432354, 0.0016026590019464493, 0.0016535426257178187, 0.0016492505092173815, 0.0016958963824436069, 0.0015994722489267588, 0.0016431087860837579, 0.001627679681405425, 0.001705825561657548, 0.0016103893285617232, 0.001658791908994317, 0.0016473656287416816, 0.0016197090735659003, 0.0016323134768754244, 0.0016025998629629612, 0.001617522444576025, 0.001628052326850593, 0.0016201154794543982, 0.0016171379247680306, 0.0016015155706554651, 0.0016323605086654425, 0.001662551425397396, 0.001628768164664507, 0.0016035096487030387, 0.0016556181944906712, 0.0018009821651503444, 0.0016337820561602712, 0.0016137890052050352, 0.001603717915713787, 0.0016027495730668306, 0.0016208862653002143, 0.0017505341675132513, 0.0016191283939406276, 0.0016271404456347227, 0.0016169125447049737, 0.001623237505555153, 0.0016211742768064141, 0.0016065732343122363, 0.0016655785730108619, 0.0016761451261118054, 0.0016228107269853354, 0.0016222399426624179, 0.0016873457934707403, 0.0016216773074120283, 0.0016242603305727243, 0.0016286249738186598, 0.001625050324946642, 0.0016604774864390492, 0.001598617760464549, 0.0016540160868316889, 0.0016403865301981568, 0.0016123016830533743, 0.0016215074574574828, 0.0016867882804945111, 0.0016295408131554723, 0.0016037968453019857, 0.0016304138116538525, 0.001650093705393374, 0.0016086286632344127, 0.0016559101641178131, 0.0016100510256364942, 0.0016951952129602432, 0.0017143493751063943, 0.0016685199225321412, 0.001612261519767344, 0.0016053743893280625, 0.0016047857934609056, 0.0016436788719147444, 0.0016292380169034004, 0.0016400921158492565, 0.0016064902301877737, 0.0016047797398641706, 0.001662814524024725, 0.0016284326557070017, 0.001638496294617653, 0.00160015223082155, 0.0016603332478553057, 0.001657361164689064]\n\n\n<matplotlib.legend.Legend at 0x19f3cb0a0>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nVisualize predictions-2\n\n\nCode\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=80)\n    #ORIGINAL DATA\n    print(X.shape,Y.shape)\n    plt.plot(Y_ind, Y,'o', label='target')\n    plt.plot(X_ind, X,'.', label='training points');     \n    plt.plot(Y_ind, train_predict,'r.', label='prediction');    \n    plt.plot(Y_ind, train_predict,'-');    \n    plt.legend()\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Log daily discharge')\n    #plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n    plt.show()\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(12122,) (418,)\n\n\n\n\n\n\n\nModel with no regulation\n\n\nCode\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\nmodel.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\n#recurrent_regularizer=regularizers.L2(1e-1), ##L2 regulation\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_4 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_8 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nTrain model\n\n\nCode\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\n\nEpoch 1/200\n5/5 - 2s - loss: 0.5478 - val_loss: 0.2160 - 2s/epoch - 320ms/step\nEpoch 2/200\n5/5 - 0s - loss: 0.2000 - val_loss: 0.1322 - 53ms/epoch - 11ms/step\nEpoch 3/200\n5/5 - 0s - loss: 0.1315 - val_loss: 0.0957 - 47ms/epoch - 9ms/step\nEpoch 4/200\n5/5 - 0s - loss: 0.0981 - val_loss: 0.0690 - 61ms/epoch - 12ms/step\nEpoch 5/200\n5/5 - 0s - loss: 0.0727 - val_loss: 0.0565 - 51ms/epoch - 10ms/step\nEpoch 6/200\n5/5 - 0s - loss: 0.0598 - val_loss: 0.0455 - 54ms/epoch - 11ms/step\nEpoch 7/200\n5/5 - 0s - loss: 0.0481 - val_loss: 0.0430 - 56ms/epoch - 11ms/step\nEpoch 8/200\n5/5 - 0s - loss: 0.0449 - val_loss: 0.0397 - 49ms/epoch - 10ms/step\nEpoch 9/200\n5/5 - 0s - loss: 0.0406 - val_loss: 0.0384 - 47ms/epoch - 9ms/step\nEpoch 10/200\n5/5 - 0s - loss: 0.0383 - val_loss: 0.0386 - 51ms/epoch - 10ms/step\nEpoch 11/200\n5/5 - 0s - loss: 0.0372 - val_loss: 0.0392 - 46ms/epoch - 9ms/step\nEpoch 12/200\n5/5 - 0s - loss: 0.0371 - val_loss: 0.0379 - 47ms/epoch - 9ms/step\nEpoch 13/200\n5/5 - 0s - loss: 0.0365 - val_loss: 0.0376 - 48ms/epoch - 10ms/step\nEpoch 14/200\n5/5 - 0s - loss: 0.0360 - val_loss: 0.0375 - 46ms/epoch - 9ms/step\nEpoch 15/200\n5/5 - 0s - loss: 0.0355 - val_loss: 0.0362 - 49ms/epoch - 10ms/step\nEpoch 16/200\n5/5 - 0s - loss: 0.0348 - val_loss: 0.0352 - 75ms/epoch - 15ms/step\nEpoch 17/200\n5/5 - 0s - loss: 0.0350 - val_loss: 0.0347 - 68ms/epoch - 14ms/step\nEpoch 18/200\n5/5 - 0s - loss: 0.0347 - val_loss: 0.0346 - 57ms/epoch - 11ms/step\nEpoch 19/200\n5/5 - 0s - loss: 0.0345 - val_loss: 0.0333 - 60ms/epoch - 12ms/step\nEpoch 20/200\n5/5 - 0s - loss: 0.0327 - val_loss: 0.0330 - 60ms/epoch - 12ms/step\nEpoch 21/200\n5/5 - 0s - loss: 0.0321 - val_loss: 0.0336 - 44ms/epoch - 9ms/step\nEpoch 22/200\n5/5 - 0s - loss: 0.0317 - val_loss: 0.0329 - 44ms/epoch - 9ms/step\nEpoch 23/200\n5/5 - 0s - loss: 0.0310 - val_loss: 0.0312 - 41ms/epoch - 8ms/step\nEpoch 24/200\n5/5 - 0s - loss: 0.0309 - val_loss: 0.0314 - 42ms/epoch - 8ms/step\nEpoch 25/200\n5/5 - 0s - loss: 0.0298 - val_loss: 0.0315 - 43ms/epoch - 9ms/step\nEpoch 26/200\n5/5 - 0s - loss: 0.0296 - val_loss: 0.0308 - 43ms/epoch - 9ms/step\nEpoch 27/200\n5/5 - 0s - loss: 0.0292 - val_loss: 0.0302 - 43ms/epoch - 9ms/step\nEpoch 28/200\n5/5 - 0s - loss: 0.0288 - val_loss: 0.0322 - 41ms/epoch - 8ms/step\nEpoch 29/200\n5/5 - 0s - loss: 0.0289 - val_loss: 0.0291 - 43ms/epoch - 9ms/step\nEpoch 30/200\n5/5 - 0s - loss: 0.0276 - val_loss: 0.0278 - 42ms/epoch - 8ms/step\nEpoch 31/200\n5/5 - 0s - loss: 0.0271 - val_loss: 0.0293 - 43ms/epoch - 9ms/step\nEpoch 32/200\n5/5 - 0s - loss: 0.0271 - val_loss: 0.0270 - 56ms/epoch - 11ms/step\nEpoch 33/200\n5/5 - 0s - loss: 0.0262 - val_loss: 0.0266 - 49ms/epoch - 10ms/step\nEpoch 34/200\n5/5 - 0s - loss: 0.0258 - val_loss: 0.0274 - 51ms/epoch - 10ms/step\nEpoch 35/200\n5/5 - 0s - loss: 0.0255 - val_loss: 0.0282 - 51ms/epoch - 10ms/step\nEpoch 36/200\n5/5 - 0s - loss: 0.0257 - val_loss: 0.0250 - 56ms/epoch - 11ms/step\nEpoch 37/200\n5/5 - 0s - loss: 0.0246 - val_loss: 0.0249 - 50ms/epoch - 10ms/step\nEpoch 38/200\n5/5 - 0s - loss: 0.0243 - val_loss: 0.0245 - 49ms/epoch - 10ms/step\nEpoch 39/200\n5/5 - 0s - loss: 0.0241 - val_loss: 0.0253 - 47ms/epoch - 9ms/step\nEpoch 40/200\n5/5 - 0s - loss: 0.0238 - val_loss: 0.0248 - 45ms/epoch - 9ms/step\nEpoch 41/200\n5/5 - 0s - loss: 0.0234 - val_loss: 0.0233 - 43ms/epoch - 9ms/step\nEpoch 42/200\n5/5 - 0s - loss: 0.0229 - val_loss: 0.0236 - 43ms/epoch - 9ms/step\nEpoch 43/200\n5/5 - 0s - loss: 0.0224 - val_loss: 0.0235 - 41ms/epoch - 8ms/step\nEpoch 44/200\n5/5 - 0s - loss: 0.0223 - val_loss: 0.0249 - 42ms/epoch - 8ms/step\nEpoch 45/200\n5/5 - 0s - loss: 0.0224 - val_loss: 0.0231 - 44ms/epoch - 9ms/step\nEpoch 46/200\n5/5 - 0s - loss: 0.0215 - val_loss: 0.0213 - 43ms/epoch - 9ms/step\nEpoch 47/200\n5/5 - 0s - loss: 0.0211 - val_loss: 0.0210 - 41ms/epoch - 8ms/step\nEpoch 48/200\n5/5 - 0s - loss: 0.0207 - val_loss: 0.0227 - 41ms/epoch - 8ms/step\nEpoch 49/200\n5/5 - 0s - loss: 0.0209 - val_loss: 0.0226 - 42ms/epoch - 8ms/step\nEpoch 50/200\n5/5 - 0s - loss: 0.0205 - val_loss: 0.0200 - 42ms/epoch - 8ms/step\nEpoch 51/200\n5/5 - 0s - loss: 0.0195 - val_loss: 0.0194 - 42ms/epoch - 8ms/step\nEpoch 52/200\n5/5 - 0s - loss: 0.0191 - val_loss: 0.0195 - 43ms/epoch - 9ms/step\nEpoch 53/200\n5/5 - 0s - loss: 0.0188 - val_loss: 0.0196 - 124ms/epoch - 25ms/step\nEpoch 54/200\n5/5 - 0s - loss: 0.0183 - val_loss: 0.0187 - 63ms/epoch - 13ms/step\nEpoch 55/200\n5/5 - 0s - loss: 0.0178 - val_loss: 0.0176 - 44ms/epoch - 9ms/step\nEpoch 56/200\n5/5 - 0s - loss: 0.0174 - val_loss: 0.0173 - 43ms/epoch - 9ms/step\nEpoch 57/200\n5/5 - 0s - loss: 0.0173 - val_loss: 0.0170 - 42ms/epoch - 8ms/step\nEpoch 58/200\n5/5 - 0s - loss: 0.0171 - val_loss: 0.0166 - 43ms/epoch - 9ms/step\nEpoch 59/200\n5/5 - 0s - loss: 0.0167 - val_loss: 0.0163 - 42ms/epoch - 8ms/step\nEpoch 60/200\n5/5 - 0s - loss: 0.0162 - val_loss: 0.0157 - 41ms/epoch - 8ms/step\nEpoch 61/200\n5/5 - 0s - loss: 0.0157 - val_loss: 0.0154 - 43ms/epoch - 9ms/step\nEpoch 62/200\n5/5 - 0s - loss: 0.0152 - val_loss: 0.0160 - 40ms/epoch - 8ms/step\nEpoch 63/200\n5/5 - 0s - loss: 0.0150 - val_loss: 0.0149 - 43ms/epoch - 9ms/step\nEpoch 64/200\n5/5 - 0s - loss: 0.0148 - val_loss: 0.0141 - 42ms/epoch - 8ms/step\nEpoch 65/200\n5/5 - 0s - loss: 0.0141 - val_loss: 0.0155 - 42ms/epoch - 8ms/step\nEpoch 66/200\n5/5 - 0s - loss: 0.0143 - val_loss: 0.0143 - 42ms/epoch - 8ms/step\nEpoch 67/200\n5/5 - 0s - loss: 0.0136 - val_loss: 0.0133 - 41ms/epoch - 8ms/step\nEpoch 68/200\n5/5 - 0s - loss: 0.0133 - val_loss: 0.0131 - 43ms/epoch - 9ms/step\nEpoch 69/200\n5/5 - 0s - loss: 0.0130 - val_loss: 0.0131 - 42ms/epoch - 8ms/step\nEpoch 70/200\n5/5 - 0s - loss: 0.0131 - val_loss: 0.0133 - 42ms/epoch - 8ms/step\nEpoch 71/200\n5/5 - 0s - loss: 0.0126 - val_loss: 0.0124 - 43ms/epoch - 9ms/step\nEpoch 72/200\n5/5 - 0s - loss: 0.0121 - val_loss: 0.0124 - 42ms/epoch - 8ms/step\nEpoch 73/200\n5/5 - 0s - loss: 0.0122 - val_loss: 0.0121 - 42ms/epoch - 8ms/step\nEpoch 74/200\n5/5 - 0s - loss: 0.0117 - val_loss: 0.0117 - 42ms/epoch - 8ms/step\nEpoch 75/200\n5/5 - 0s - loss: 0.0114 - val_loss: 0.0119 - 42ms/epoch - 8ms/step\nEpoch 76/200\n5/5 - 0s - loss: 0.0112 - val_loss: 0.0105 - 43ms/epoch - 9ms/step\nEpoch 77/200\n5/5 - 0s - loss: 0.0105 - val_loss: 0.0105 - 41ms/epoch - 8ms/step\nEpoch 78/200\n5/5 - 0s - loss: 0.0102 - val_loss: 0.0109 - 42ms/epoch - 8ms/step\nEpoch 79/200\n5/5 - 0s - loss: 0.0103 - val_loss: 0.0111 - 42ms/epoch - 8ms/step\nEpoch 80/200\n5/5 - 0s - loss: 0.0101 - val_loss: 0.0107 - 43ms/epoch - 9ms/step\nEpoch 81/200\n5/5 - 0s - loss: 0.0096 - val_loss: 0.0089 - 42ms/epoch - 8ms/step\nEpoch 82/200\n5/5 - 0s - loss: 0.0090 - val_loss: 0.0094 - 42ms/epoch - 8ms/step\nEpoch 83/200\n5/5 - 0s - loss: 0.0089 - val_loss: 0.0089 - 43ms/epoch - 9ms/step\nEpoch 84/200\n5/5 - 0s - loss: 0.0087 - val_loss: 0.0101 - 42ms/epoch - 8ms/step\nEpoch 85/200\n5/5 - 0s - loss: 0.0090 - val_loss: 0.0082 - 41ms/epoch - 8ms/step\nEpoch 86/200\n5/5 - 0s - loss: 0.0081 - val_loss: 0.0082 - 44ms/epoch - 9ms/step\nEpoch 87/200\n5/5 - 0s - loss: 0.0080 - val_loss: 0.0087 - 44ms/epoch - 9ms/step\nEpoch 88/200\n5/5 - 0s - loss: 0.0083 - val_loss: 0.0085 - 41ms/epoch - 8ms/step\nEpoch 89/200\n5/5 - 0s - loss: 0.0079 - val_loss: 0.0076 - 42ms/epoch - 8ms/step\nEpoch 90/200\n5/5 - 0s - loss: 0.0076 - val_loss: 0.0095 - 43ms/epoch - 9ms/step\nEpoch 91/200\n5/5 - 0s - loss: 0.0082 - val_loss: 0.0072 - 40ms/epoch - 8ms/step\nEpoch 92/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0073 - 43ms/epoch - 9ms/step\nEpoch 93/200\n5/5 - 0s - loss: 0.0073 - val_loss: 0.0076 - 42ms/epoch - 8ms/step\nEpoch 94/200\n5/5 - 0s - loss: 0.0076 - val_loss: 0.0097 - 44ms/epoch - 9ms/step\nEpoch 95/200\n5/5 - 0s - loss: 0.0083 - val_loss: 0.0073 - 44ms/epoch - 9ms/step\nEpoch 96/200\n5/5 - 0s - loss: 0.0072 - val_loss: 0.0069 - 41ms/epoch - 8ms/step\nEpoch 97/200\n5/5 - 0s - loss: 0.0070 - val_loss: 0.0091 - 43ms/epoch - 9ms/step\nEpoch 98/200\n5/5 - 0s - loss: 0.0076 - val_loss: 0.0086 - 42ms/epoch - 8ms/step\nEpoch 99/200\n5/5 - 0s - loss: 0.0075 - val_loss: 0.0071 - 43ms/epoch - 9ms/step\nEpoch 100/200\n5/5 - 0s - loss: 0.0070 - val_loss: 0.0086 - 43ms/epoch - 9ms/step\nEpoch 101/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0080 - 41ms/epoch - 8ms/step\nEpoch 102/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0083 - 43ms/epoch - 9ms/step\nEpoch 103/200\n5/5 - 0s - loss: 0.0072 - val_loss: 0.0085 - 43ms/epoch - 9ms/step\nEpoch 104/200\n5/5 - 0s - loss: 0.0071 - val_loss: 0.0067 - 41ms/epoch - 8ms/step\nEpoch 105/200\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0083 - 43ms/epoch - 9ms/step\nEpoch 106/200\n5/5 - 0s - loss: 0.0071 - val_loss: 0.0065 - 43ms/epoch - 9ms/step\nEpoch 107/200\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0078 - 42ms/epoch - 8ms/step\nEpoch 108/200\n5/5 - 0s - loss: 0.0069 - val_loss: 0.0066 - 106ms/epoch - 21ms/step\nEpoch 109/200\n5/5 - 0s - loss: 0.0066 - val_loss: 0.0065 - 59ms/epoch - 12ms/step\nEpoch 110/200\n5/5 - 0s - loss: 0.0065 - val_loss: 0.0074 - 43ms/epoch - 9ms/step\nEpoch 111/200\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0086 - 40ms/epoch - 8ms/step\nEpoch 112/200\n5/5 - 0s - loss: 0.0070 - val_loss: 0.0063 - 41ms/epoch - 8ms/step\nEpoch 113/200\n5/5 - 0s - loss: 0.0064 - val_loss: 0.0085 - 43ms/epoch - 9ms/step\nEpoch 114/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0067 - 42ms/epoch - 8ms/step\nEpoch 115/200\n5/5 - 0s - loss: 0.0065 - val_loss: 0.0062 - 43ms/epoch - 9ms/step\nEpoch 116/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0062 - 43ms/epoch - 9ms/step\nEpoch 117/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0064 - 42ms/epoch - 8ms/step\nEpoch 118/200\n5/5 - 0s - loss: 0.0062 - val_loss: 0.0102 - 42ms/epoch - 8ms/step\nEpoch 119/200\n5/5 - 0s - loss: 0.0071 - val_loss: 0.0063 - 44ms/epoch - 9ms/step\nEpoch 120/200\n5/5 - 0s - loss: 0.0062 - val_loss: 0.0061 - 41ms/epoch - 8ms/step\nEpoch 121/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0061 - 44ms/epoch - 9ms/step\nEpoch 122/200\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0074 - 41ms/epoch - 8ms/step\nEpoch 123/200\n5/5 - 0s - loss: 0.0065 - val_loss: 0.0078 - 41ms/epoch - 8ms/step\nEpoch 124/200\n5/5 - 0s - loss: 0.0064 - val_loss: 0.0074 - 44ms/epoch - 9ms/step\nEpoch 125/200\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0073 - 43ms/epoch - 9ms/step\nEpoch 126/200\n5/5 - 0s - loss: 0.0066 - val_loss: 0.0069 - 42ms/epoch - 8ms/step\nEpoch 127/200\n5/5 - 0s - loss: 0.0062 - val_loss: 0.0061 - 44ms/epoch - 9ms/step\nEpoch 128/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0061 - 41ms/epoch - 8ms/step\nEpoch 129/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0076 - 42ms/epoch - 8ms/step\nEpoch 130/200\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0063 - 43ms/epoch - 9ms/step\nEpoch 131/200\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0059 - 42ms/epoch - 8ms/step\nEpoch 132/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0071 - 43ms/epoch - 9ms/step\nEpoch 133/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0074 - 40ms/epoch - 8ms/step\nEpoch 134/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0072 - 43ms/epoch - 9ms/step\nEpoch 135/200\n5/5 - 0s - loss: 0.0064 - val_loss: 0.0059 - 42ms/epoch - 8ms/step\nEpoch 136/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0058 - 42ms/epoch - 8ms/step\nEpoch 137/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0061 - 44ms/epoch - 9ms/step\nEpoch 138/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0063 - 41ms/epoch - 8ms/step\nEpoch 139/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0069 - 42ms/epoch - 8ms/step\nEpoch 140/200\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0058 - 42ms/epoch - 8ms/step\nEpoch 141/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0057 - 43ms/epoch - 9ms/step\nEpoch 142/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0072 - 45ms/epoch - 9ms/step\nEpoch 143/200\n5/5 - 0s - loss: 0.0059 - val_loss: 0.0061 - 43ms/epoch - 9ms/step\nEpoch 144/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0057 - 42ms/epoch - 8ms/step\nEpoch 145/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0063 - 42ms/epoch - 8ms/step\nEpoch 146/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0056 - 43ms/epoch - 9ms/step\nEpoch 147/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0086 - 41ms/epoch - 8ms/step\nEpoch 148/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0075 - 43ms/epoch - 9ms/step\nEpoch 149/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0055 - 42ms/epoch - 8ms/step\nEpoch 150/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0074 - 42ms/epoch - 8ms/step\nEpoch 151/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0057 - 44ms/epoch - 9ms/step\nEpoch 152/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0067 - 41ms/epoch - 8ms/step\nEpoch 153/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0055 - 42ms/epoch - 8ms/step\nEpoch 154/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0059 - 42ms/epoch - 8ms/step\nEpoch 155/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0060 - 42ms/epoch - 8ms/step\nEpoch 156/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0064 - 42ms/epoch - 8ms/step\nEpoch 157/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0056 - 41ms/epoch - 8ms/step\nEpoch 158/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0071 - 43ms/epoch - 9ms/step\nEpoch 159/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0059 - 42ms/epoch - 8ms/step\nEpoch 160/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0059 - 43ms/epoch - 9ms/step\nEpoch 161/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0055 - 42ms/epoch - 8ms/step\nEpoch 162/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0062 - 83ms/epoch - 17ms/step\nEpoch 163/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0056 - 44ms/epoch - 9ms/step\nEpoch 164/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0053 - 72ms/epoch - 14ms/step\nEpoch 165/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0075 - 59ms/epoch - 12ms/step\nEpoch 166/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0055 - 43ms/epoch - 9ms/step\nEpoch 167/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0058 - 43ms/epoch - 9ms/step\nEpoch 168/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0055 - 47ms/epoch - 9ms/step\nEpoch 169/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0065 - 41ms/epoch - 8ms/step\nEpoch 170/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0061 - 44ms/epoch - 9ms/step\nEpoch 171/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0063 - 43ms/epoch - 9ms/step\nEpoch 172/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0053 - 46ms/epoch - 9ms/step\nEpoch 173/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0073 - 49ms/epoch - 10ms/step\nEpoch 174/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0062 - 42ms/epoch - 8ms/step\nEpoch 175/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0055 - 42ms/epoch - 8ms/step\nEpoch 176/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0055 - 43ms/epoch - 9ms/step\nEpoch 177/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0056 - 43ms/epoch - 9ms/step\nEpoch 178/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0076 - 45ms/epoch - 9ms/step\nEpoch 179/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0054 - 54ms/epoch - 11ms/step\nEpoch 180/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0061 - 47ms/epoch - 9ms/step\nEpoch 181/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0057 - 42ms/epoch - 8ms/step\nEpoch 182/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0054 - 43ms/epoch - 9ms/step\nEpoch 183/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0060 - 43ms/epoch - 9ms/step\nEpoch 184/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0074 - 43ms/epoch - 9ms/step\nEpoch 185/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0052 - 46ms/epoch - 9ms/step\nEpoch 186/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0065 - 41ms/epoch - 8ms/step\nEpoch 187/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0052 - 43ms/epoch - 9ms/step\nEpoch 188/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0051 - 43ms/epoch - 9ms/step\nEpoch 189/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0066 - 42ms/epoch - 8ms/step\nEpoch 190/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0051 - 43ms/epoch - 9ms/step\nEpoch 191/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0053 - 44ms/epoch - 9ms/step\nEpoch 192/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0062 - 41ms/epoch - 8ms/step\nEpoch 193/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0057 - 51ms/epoch - 10ms/step\nEpoch 194/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0066 - 63ms/epoch - 13ms/step\nEpoch 195/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0052 - 60ms/epoch - 12ms/step\nEpoch 196/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0052 - 67ms/epoch - 13ms/step\nEpoch 197/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0050 - 57ms/epoch - 11ms/step\nEpoch 198/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0051 - 56ms/epoch - 11ms/step\nEpoch 199/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0053 - 43ms/epoch - 9ms/step\nEpoch 200/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0061 - 43ms/epoch - 9ms/step\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Training epoch vs errors\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n14/14 [==============================] - 2s 2ms/step\n4/4 [==============================] - 0s 2ms/step\n(418, 29, 1) (418,) (418,) (104, 29, 1) (104,) (104,)\n(418,) (418,)\n0.0061818813\n0.0066022957\nTrain MSE = 0.00618 RMSE = 0.07862\nTest MSE = 0.00660 RMSE = 0.08125\n\n\n\n                                                \n\n\n[0.547828733921051, 0.200002059340477, 0.13146454095840454, 0.0980849489569664, 0.07269972562789917, 0.05977404862642288, 0.048082444816827774, 0.044905729591846466, 0.040599796921014786, 0.03829693794250488, 0.03723759949207306, 0.03707651048898697, 0.03646749258041382, 0.035989146679639816, 0.03549237176775932, 0.0347914919257164, 0.034989070147275925, 0.03465570881962776, 0.03454133868217468, 0.03271873667836189, 0.03207872807979584, 0.031650107353925705, 0.03103453293442726, 0.03089580498635769, 0.029801348224282265, 0.029581086710095406, 0.029165759682655334, 0.028845826163887978, 0.02888757362961769, 0.027639294043183327, 0.027085967361927032, 0.02706579677760601, 0.026208968833088875, 0.02580973133444786, 0.025543401017785072, 0.025724491104483604, 0.024639425799250603, 0.024266671389341354, 0.02406599186360836, 0.023765798658132553, 0.02342335693538189, 0.02293250523507595, 0.022431431338191032, 0.022298548370599747, 0.022353723645210266, 0.021546708419919014, 0.021072128787636757, 0.02067701518535614, 0.020912667736411095, 0.020466361194849014, 0.019541390240192413, 0.01914002187550068, 0.01878763735294342, 0.018293971195816994, 0.017773602157831192, 0.017397107556462288, 0.017252890393137932, 0.01708284765481949, 0.016704125329852104, 0.01615913398563862, 0.015738271176815033, 0.015194881707429886, 0.015011039562523365, 0.014750421978533268, 0.014060243032872677, 0.014258529990911484, 0.01363588497042656, 0.013292524963617325, 0.012986279092729092, 0.013113614171743393, 0.01264180801808834, 0.01211435068398714, 0.012244374491274357, 0.011676897294819355, 0.01137862540781498, 0.011164293624460697, 0.010525200515985489, 0.010217312723398209, 0.010313945822417736, 0.01011259201914072, 0.009648445062339306, 0.00900576077401638, 0.008904468268156052, 0.008710195310413837, 0.009004049003124237, 0.008131712675094604, 0.008044864982366562, 0.00827436801046133, 0.007919395342469215, 0.007619524374604225, 0.008180283941328526, 0.007446256000548601, 0.007344976998865604, 0.0075951870530843735, 0.00827630516141653, 0.007215254940092564, 0.007030564360320568, 0.007570620160549879, 0.0074569243006408215, 0.0070255221799016, 0.007359216455370188, 0.007358872797340155, 0.007207515183836222, 0.007137522101402283, 0.006703214719891548, 0.007102213334292173, 0.006673191208392382, 0.006913843099027872, 0.006554936990141869, 0.006491891574114561, 0.0067299953661859035, 0.007004627026617527, 0.0064028301276266575, 0.0073804790154099464, 0.006479302421212196, 0.006303691770881414, 0.006265553180128336, 0.006244378164410591, 0.007079037372022867, 0.006168194115161896, 0.00627476442605257, 0.006080315448343754, 0.006501772440969944, 0.006372758187353611, 0.006745468359440565, 0.006600470747798681, 0.006157017778605223, 0.005984097719192505, 0.005960427690297365, 0.006060279905796051, 0.006066455971449614, 0.005812034942209721, 0.006044020876288414, 0.006000499706715345, 0.006381877698004246, 0.005713805090636015, 0.0056871590204536915, 0.005707080941647291, 0.0058332220651209354, 0.006077816244214773, 0.005703663919121027, 0.005607920698821545, 0.005921849049627781, 0.005620317999273539, 0.0055799358524382114, 0.005728818476200104, 0.005428491160273552, 0.005996390245854855, 0.005680794361978769, 0.00549438176676631, 0.0058236862532794476, 0.005410150624811649, 0.005529338959604502, 0.00542931305244565, 0.0053474269807338715, 0.00548589788377285, 0.005438700318336487, 0.005359894596040249, 0.0060479771345853806, 0.005358163267374039, 0.005264610983431339, 0.005192078649997711, 0.005322256591171026, 0.005346674472093582, 0.0052653718739748, 0.005648953840136528, 0.005097079556435347, 0.0052666314877569675, 0.005354780703783035, 0.005283912643790245, 0.005468647461384535, 0.005352167412638664, 0.0050476351752877235, 0.005464017391204834, 0.005613785702735186, 0.0050531961023807526, 0.005052777007222176, 0.005153801757842302, 0.005722908768802881, 0.0050082216039299965, 0.005409940145909786, 0.005061008036136627, 0.005126160103827715, 0.005265845451503992, 0.00542798824608326, 0.004915375262498856, 0.005271639209240675, 0.005012153647840023, 0.004912154749035835, 0.005744170397520065, 0.004894637037068605, 0.004857135936617851, 0.005052067805081606, 0.004918517544865608, 0.005179660394787788, 0.00488966004922986, 0.004858294501900673, 0.004809750709682703, 0.005004066973924637, 0.004895783960819244]\n\n\n<matplotlib.legend.Legend at 0x1aa3b3520>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nVisualize predictions-2\n\n\nCode\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=80)\n    #ORIGINAL DATA\n    print(X.shape,Y.shape)\n    plt.plot(Y_ind, Y,'o', label='target')\n    plt.plot(X_ind, X,'.', label='training points');     \n    plt.plot(Y_ind, train_predict,'r.', label='prediction');    \n    plt.plot(Y_ind, train_predict,'-');    \n    plt.legend()\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Sunspots scaled')\n    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n    plt.show()\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(12122,) (418,)"
  },
  {
    "objectID": "dl_discharge_copy.html#gru",
    "href": "dl_discharge_copy.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=200\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\n# max_features = 10000    #DEFINES SIZE OF VOCBULARY TO USE\n# maxlen       = 250      #CUTOFF REVIEWS maxlen 20 WORDS)\nbatch_size   = 1000\nverbose      = 1\nembed_dim    = 8        #DIMENSION OF EMBEDING SPACE (SIZE OF VECTOR FOR EACH WORD)\nlr           = 0.001    #LEARNING RATE\nvalidation_split=0.2\n\n\n\nModel with L2 regulation\n\n\nCode\n\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\n#model.add(SimpleRNN(\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_3 (GRU)                 (None, 3)                 54        \n                                                                 \n dense_10 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-28 21:14:58.870999: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:14:58.875198: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:14:58.876942: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\nCode\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\n\nEpoch 1/200\n\n\n2023-04-28 21:15:03.098977: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:15:03.100782: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:15:03.102442: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-28 21:15:04.070651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:15:04.073265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:15:04.075423: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-28 21:15:05.512623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:15:05.516172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:15:05.518139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n5/5 - 3s - loss: 0.3416 - val_loss: 0.3206 - 3s/epoch - 605ms/step\nEpoch 2/200\n5/5 - 0s - loss: 0.3151 - val_loss: 0.3022 - 78ms/epoch - 16ms/step\nEpoch 3/200\n5/5 - 0s - loss: 0.2982 - val_loss: 0.2878 - 68ms/epoch - 14ms/step\nEpoch 4/200\n5/5 - 0s - loss: 0.2846 - val_loss: 0.2766 - 70ms/epoch - 14ms/step\nEpoch 5/200\n5/5 - 0s - loss: 0.2738 - val_loss: 0.2669 - 91ms/epoch - 18ms/step\nEpoch 6/200\n5/5 - 0s - loss: 0.2643 - val_loss: 0.2591 - 126ms/epoch - 25ms/step\nEpoch 7/200\n5/5 - 0s - loss: 0.2566 - val_loss: 0.2516 - 71ms/epoch - 14ms/step\nEpoch 8/200\n5/5 - 0s - loss: 0.2493 - val_loss: 0.2443 - 72ms/epoch - 14ms/step\nEpoch 9/200\n5/5 - 0s - loss: 0.2421 - val_loss: 0.2378 - 67ms/epoch - 13ms/step\nEpoch 10/200\n5/5 - 0s - loss: 0.2356 - val_loss: 0.2312 - 67ms/epoch - 13ms/step\nEpoch 11/200\n5/5 - 0s - loss: 0.2291 - val_loss: 0.2252 - 68ms/epoch - 14ms/step\nEpoch 12/200\n5/5 - 0s - loss: 0.2232 - val_loss: 0.2195 - 70ms/epoch - 14ms/step\nEpoch 13/200\n5/5 - 0s - loss: 0.2175 - val_loss: 0.2138 - 66ms/epoch - 13ms/step\nEpoch 14/200\n5/5 - 0s - loss: 0.2119 - val_loss: 0.2082 - 67ms/epoch - 13ms/step\nEpoch 15/200\n5/5 - 0s - loss: 0.2063 - val_loss: 0.2030 - 71ms/epoch - 14ms/step\nEpoch 16/200\n5/5 - 0s - loss: 0.2010 - val_loss: 0.1976 - 77ms/epoch - 15ms/step\nEpoch 17/200\n5/5 - 0s - loss: 0.1958 - val_loss: 0.1924 - 78ms/epoch - 16ms/step\nEpoch 18/200\n5/5 - 0s - loss: 0.1906 - val_loss: 0.1875 - 70ms/epoch - 14ms/step\nEpoch 19/200\n5/5 - 0s - loss: 0.1857 - val_loss: 0.1825 - 69ms/epoch - 14ms/step\nEpoch 20/200\n5/5 - 0s - loss: 0.1807 - val_loss: 0.1777 - 73ms/epoch - 15ms/step\nEpoch 21/200\n5/5 - 0s - loss: 0.1760 - val_loss: 0.1730 - 69ms/epoch - 14ms/step\nEpoch 22/200\n5/5 - 0s - loss: 0.1713 - val_loss: 0.1684 - 70ms/epoch - 14ms/step\nEpoch 23/200\n5/5 - 0s - loss: 0.1667 - val_loss: 0.1638 - 68ms/epoch - 14ms/step\nEpoch 24/200\n5/5 - 0s - loss: 0.1622 - val_loss: 0.1594 - 67ms/epoch - 13ms/step\nEpoch 25/200\n5/5 - 0s - loss: 0.1578 - val_loss: 0.1550 - 63ms/epoch - 13ms/step\nEpoch 26/200\n5/5 - 0s - loss: 0.1535 - val_loss: 0.1507 - 64ms/epoch - 13ms/step\nEpoch 27/200\n5/5 - 0s - loss: 0.1492 - val_loss: 0.1468 - 65ms/epoch - 13ms/step\nEpoch 28/200\n5/5 - 0s - loss: 0.1452 - val_loss: 0.1427 - 65ms/epoch - 13ms/step\nEpoch 29/200\n5/5 - 0s - loss: 0.1411 - val_loss: 0.1387 - 68ms/epoch - 14ms/step\nEpoch 30/200\n5/5 - 0s - loss: 0.1372 - val_loss: 0.1349 - 71ms/epoch - 14ms/step\nEpoch 31/200\n5/5 - 0s - loss: 0.1334 - val_loss: 0.1309 - 89ms/epoch - 18ms/step\nEpoch 32/200\n5/5 - 0s - loss: 0.1295 - val_loss: 0.1273 - 104ms/epoch - 21ms/step\nEpoch 33/200\n5/5 - 0s - loss: 0.1259 - val_loss: 0.1237 - 84ms/epoch - 17ms/step\nEpoch 34/200\n5/5 - 0s - loss: 0.1223 - val_loss: 0.1203 - 97ms/epoch - 19ms/step\nEpoch 35/200\n5/5 - 0s - loss: 0.1189 - val_loss: 0.1167 - 98ms/epoch - 20ms/step\nEpoch 36/200\n5/5 - 0s - loss: 0.1154 - val_loss: 0.1133 - 64ms/epoch - 13ms/step\nEpoch 37/200\n5/5 - 0s - loss: 0.1120 - val_loss: 0.1100 - 66ms/epoch - 13ms/step\nEpoch 38/200\n5/5 - 0s - loss: 0.1087 - val_loss: 0.1068 - 66ms/epoch - 13ms/step\nEpoch 39/200\n5/5 - 0s - loss: 0.1055 - val_loss: 0.1036 - 70ms/epoch - 14ms/step\nEpoch 40/200\n5/5 - 0s - loss: 0.1024 - val_loss: 0.1005 - 69ms/epoch - 14ms/step\nEpoch 41/200\n5/5 - 0s - loss: 0.0993 - val_loss: 0.0977 - 68ms/epoch - 14ms/step\nEpoch 42/200\n5/5 - 0s - loss: 0.0963 - val_loss: 0.0946 - 68ms/epoch - 14ms/step\nEpoch 43/200\n5/5 - 0s - loss: 0.0934 - val_loss: 0.0917 - 67ms/epoch - 13ms/step\nEpoch 44/200\n5/5 - 0s - loss: 0.0905 - val_loss: 0.0889 - 66ms/epoch - 13ms/step\nEpoch 45/200\n5/5 - 0s - loss: 0.0877 - val_loss: 0.0862 - 66ms/epoch - 13ms/step\nEpoch 46/200\n5/5 - 0s - loss: 0.0850 - val_loss: 0.0834 - 69ms/epoch - 14ms/step\nEpoch 47/200\n5/5 - 0s - loss: 0.0823 - val_loss: 0.0809 - 66ms/epoch - 13ms/step\nEpoch 48/200\n5/5 - 0s - loss: 0.0798 - val_loss: 0.0783 - 67ms/epoch - 13ms/step\nEpoch 49/200\n5/5 - 0s - loss: 0.0772 - val_loss: 0.0759 - 66ms/epoch - 13ms/step\nEpoch 50/200\n5/5 - 0s - loss: 0.0749 - val_loss: 0.0735 - 122ms/epoch - 24ms/step\nEpoch 51/200\n5/5 - 0s - loss: 0.0724 - val_loss: 0.0711 - 74ms/epoch - 15ms/step\nEpoch 52/200\n5/5 - 0s - loss: 0.0700 - val_loss: 0.0688 - 69ms/epoch - 14ms/step\nEpoch 53/200\n5/5 - 0s - loss: 0.0678 - val_loss: 0.0667 - 65ms/epoch - 13ms/step\nEpoch 54/200\n5/5 - 0s - loss: 0.0656 - val_loss: 0.0644 - 67ms/epoch - 13ms/step\nEpoch 55/200\n5/5 - 0s - loss: 0.0635 - val_loss: 0.0624 - 65ms/epoch - 13ms/step\nEpoch 56/200\n5/5 - 0s - loss: 0.0614 - val_loss: 0.0602 - 66ms/epoch - 13ms/step\nEpoch 57/200\n5/5 - 0s - loss: 0.0593 - val_loss: 0.0584 - 67ms/epoch - 13ms/step\nEpoch 58/200\n5/5 - 0s - loss: 0.0574 - val_loss: 0.0563 - 69ms/epoch - 14ms/step\nEpoch 59/200\n5/5 - 0s - loss: 0.0554 - val_loss: 0.0545 - 67ms/epoch - 13ms/step\nEpoch 60/200\n5/5 - 0s - loss: 0.0536 - val_loss: 0.0526 - 66ms/epoch - 13ms/step\nEpoch 61/200\n5/5 - 0s - loss: 0.0517 - val_loss: 0.0510 - 67ms/epoch - 13ms/step\nEpoch 62/200\n5/5 - 0s - loss: 0.0500 - val_loss: 0.0491 - 64ms/epoch - 13ms/step\nEpoch 63/200\n5/5 - 0s - loss: 0.0483 - val_loss: 0.0475 - 66ms/epoch - 13ms/step\nEpoch 64/200\n5/5 - 0s - loss: 0.0467 - val_loss: 0.0458 - 66ms/epoch - 13ms/step\nEpoch 65/200\n5/5 - 0s - loss: 0.0450 - val_loss: 0.0443 - 67ms/epoch - 13ms/step\nEpoch 66/200\n5/5 - 0s - loss: 0.0435 - val_loss: 0.0430 - 68ms/epoch - 14ms/step\nEpoch 67/200\n5/5 - 0s - loss: 0.0420 - val_loss: 0.0415 - 65ms/epoch - 13ms/step\nEpoch 68/200\n5/5 - 0s - loss: 0.0406 - val_loss: 0.0400 - 63ms/epoch - 13ms/step\nEpoch 69/200\n5/5 - 0s - loss: 0.0392 - val_loss: 0.0385 - 66ms/epoch - 13ms/step\nEpoch 70/200\n5/5 - 0s - loss: 0.0378 - val_loss: 0.0374 - 64ms/epoch - 13ms/step\nEpoch 71/200\n5/5 - 0s - loss: 0.0366 - val_loss: 0.0360 - 66ms/epoch - 13ms/step\nEpoch 72/200\n5/5 - 0s - loss: 0.0352 - val_loss: 0.0348 - 68ms/epoch - 14ms/step\nEpoch 73/200\n5/5 - 0s - loss: 0.0340 - val_loss: 0.0338 - 65ms/epoch - 13ms/step\nEpoch 74/200\n5/5 - 0s - loss: 0.0328 - val_loss: 0.0325 - 66ms/epoch - 13ms/step\nEpoch 75/200\n5/5 - 0s - loss: 0.0316 - val_loss: 0.0312 - 66ms/epoch - 13ms/step\nEpoch 76/200\n5/5 - 0s - loss: 0.0305 - val_loss: 0.0302 - 65ms/epoch - 13ms/step\nEpoch 77/200\n5/5 - 0s - loss: 0.0294 - val_loss: 0.0292 - 62ms/epoch - 12ms/step\nEpoch 78/200\n5/5 - 0s - loss: 0.0283 - val_loss: 0.0282 - 73ms/epoch - 15ms/step\nEpoch 79/200\n5/5 - 0s - loss: 0.0273 - val_loss: 0.0269 - 64ms/epoch - 13ms/step\nEpoch 80/200\n5/5 - 0s - loss: 0.0262 - val_loss: 0.0261 - 67ms/epoch - 13ms/step\nEpoch 81/200\n5/5 - 0s - loss: 0.0253 - val_loss: 0.0250 - 71ms/epoch - 14ms/step\nEpoch 82/200\n5/5 - 0s - loss: 0.0243 - val_loss: 0.0241 - 74ms/epoch - 15ms/step\nEpoch 83/200\n5/5 - 0s - loss: 0.0234 - val_loss: 0.0233 - 69ms/epoch - 14ms/step\nEpoch 84/200\n5/5 - 0s - loss: 0.0225 - val_loss: 0.0225 - 64ms/epoch - 13ms/step\nEpoch 85/200\n5/5 - 0s - loss: 0.0217 - val_loss: 0.0215 - 68ms/epoch - 14ms/step\nEpoch 86/200\n5/5 - 0s - loss: 0.0209 - val_loss: 0.0207 - 66ms/epoch - 13ms/step\nEpoch 87/200\n5/5 - 0s - loss: 0.0200 - val_loss: 0.0200 - 67ms/epoch - 13ms/step\nEpoch 88/200\n5/5 - 0s - loss: 0.0192 - val_loss: 0.0192 - 66ms/epoch - 13ms/step\nEpoch 89/200\n5/5 - 0s - loss: 0.0185 - val_loss: 0.0186 - 65ms/epoch - 13ms/step\nEpoch 90/200\n5/5 - 0s - loss: 0.0178 - val_loss: 0.0177 - 66ms/epoch - 13ms/step\nEpoch 91/200\n5/5 - 0s - loss: 0.0171 - val_loss: 0.0170 - 66ms/epoch - 13ms/step\nEpoch 92/200\n5/5 - 0s - loss: 0.0164 - val_loss: 0.0164 - 65ms/epoch - 13ms/step\nEpoch 93/200\n5/5 - 0s - loss: 0.0158 - val_loss: 0.0158 - 65ms/epoch - 13ms/step\nEpoch 94/200\n5/5 - 0s - loss: 0.0152 - val_loss: 0.0151 - 64ms/epoch - 13ms/step\nEpoch 95/200\n5/5 - 0s - loss: 0.0145 - val_loss: 0.0146 - 65ms/epoch - 13ms/step\nEpoch 96/200\n5/5 - 0s - loss: 0.0139 - val_loss: 0.0140 - 66ms/epoch - 13ms/step\nEpoch 97/200\n5/5 - 0s - loss: 0.0134 - val_loss: 0.0134 - 66ms/epoch - 13ms/step\nEpoch 98/200\n5/5 - 0s - loss: 0.0128 - val_loss: 0.0129 - 66ms/epoch - 13ms/step\nEpoch 99/200\n5/5 - 0s - loss: 0.0123 - val_loss: 0.0124 - 64ms/epoch - 13ms/step\nEpoch 100/200\n5/5 - 0s - loss: 0.0117 - val_loss: 0.0119 - 66ms/epoch - 13ms/step\nEpoch 101/200\n5/5 - 0s - loss: 0.0113 - val_loss: 0.0114 - 66ms/epoch - 13ms/step\nEpoch 102/200\n5/5 - 0s - loss: 0.0108 - val_loss: 0.0112 - 67ms/epoch - 13ms/step\nEpoch 103/200\n5/5 - 0s - loss: 0.0104 - val_loss: 0.0105 - 67ms/epoch - 13ms/step\nEpoch 104/200\n5/5 - 0s - loss: 0.0099 - val_loss: 0.0102 - 65ms/epoch - 13ms/step\nEpoch 105/200\n5/5 - 0s - loss: 0.0097 - val_loss: 0.0097 - 66ms/epoch - 13ms/step\nEpoch 106/200\n5/5 - 0s - loss: 0.0091 - val_loss: 0.0094 - 65ms/epoch - 13ms/step\nEpoch 107/200\n5/5 - 0s - loss: 0.0088 - val_loss: 0.0090 - 68ms/epoch - 14ms/step\nEpoch 108/200\n5/5 - 0s - loss: 0.0084 - val_loss: 0.0088 - 64ms/epoch - 13ms/step\nEpoch 109/200\n5/5 - 0s - loss: 0.0081 - val_loss: 0.0083 - 66ms/epoch - 13ms/step\nEpoch 110/200\n5/5 - 0s - loss: 0.0077 - val_loss: 0.0082 - 66ms/epoch - 13ms/step\nEpoch 111/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0079 - 66ms/epoch - 13ms/step\nEpoch 112/200\n5/5 - 0s - loss: 0.0071 - val_loss: 0.0074 - 156ms/epoch - 31ms/step\nEpoch 113/200\n5/5 - 0s - loss: 0.0068 - val_loss: 0.0071 - 65ms/epoch - 13ms/step\nEpoch 114/200\n5/5 - 0s - loss: 0.0066 - val_loss: 0.0068 - 67ms/epoch - 13ms/step\nEpoch 115/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0066 - 66ms/epoch - 13ms/step\nEpoch 116/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0066 - 65ms/epoch - 13ms/step\nEpoch 117/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0061 - 66ms/epoch - 13ms/step\nEpoch 118/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0059 - 65ms/epoch - 13ms/step\nEpoch 119/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0061 - 67ms/epoch - 13ms/step\nEpoch 120/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0055 - 66ms/epoch - 13ms/step\nEpoch 121/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0053 - 66ms/epoch - 13ms/step\nEpoch 122/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0052 - 63ms/epoch - 13ms/step\nEpoch 123/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0051 - 66ms/epoch - 13ms/step\nEpoch 124/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0049 - 67ms/epoch - 13ms/step\nEpoch 125/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 97ms/epoch - 19ms/step\nEpoch 126/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0045 - 67ms/epoch - 13ms/step\nEpoch 127/200\n5/5 - 0s - loss: 0.0039 - val_loss: 0.0043 - 67ms/epoch - 13ms/step\nEpoch 128/200\n5/5 - 0s - loss: 0.0038 - val_loss: 0.0041 - 72ms/epoch - 14ms/step\nEpoch 129/200\n5/5 - 0s - loss: 0.0036 - val_loss: 0.0042 - 69ms/epoch - 14ms/step\nEpoch 130/200\n5/5 - 0s - loss: 0.0035 - val_loss: 0.0039 - 67ms/epoch - 13ms/step\nEpoch 131/200\n5/5 - 0s - loss: 0.0033 - val_loss: 0.0038 - 65ms/epoch - 13ms/step\nEpoch 132/200\n5/5 - 0s - loss: 0.0033 - val_loss: 0.0037 - 66ms/epoch - 13ms/step\nEpoch 133/200\n5/5 - 0s - loss: 0.0031 - val_loss: 0.0036 - 67ms/epoch - 13ms/step\nEpoch 134/200\n5/5 - 0s - loss: 0.0030 - val_loss: 0.0040 - 66ms/epoch - 13ms/step\nEpoch 135/200\n5/5 - 0s - loss: 0.0030 - val_loss: 0.0037 - 66ms/epoch - 13ms/step\nEpoch 136/200\n5/5 - 0s - loss: 0.0029 - val_loss: 0.0033 - 65ms/epoch - 13ms/step\nEpoch 137/200\n5/5 - 0s - loss: 0.0028 - val_loss: 0.0033 - 66ms/epoch - 13ms/step\nEpoch 138/200\n5/5 - 0s - loss: 0.0027 - val_loss: 0.0038 - 65ms/epoch - 13ms/step\nEpoch 139/200\n5/5 - 0s - loss: 0.0028 - val_loss: 0.0032 - 64ms/epoch - 13ms/step\nEpoch 140/200\n5/5 - 0s - loss: 0.0026 - val_loss: 0.0031 - 66ms/epoch - 13ms/step\nEpoch 141/200\n5/5 - 0s - loss: 0.0025 - val_loss: 0.0031 - 67ms/epoch - 13ms/step\nEpoch 142/200\n5/5 - 0s - loss: 0.0025 - val_loss: 0.0031 - 65ms/epoch - 13ms/step\nEpoch 143/200\n5/5 - 0s - loss: 0.0024 - val_loss: 0.0034 - 66ms/epoch - 13ms/step\nEpoch 144/200\n5/5 - 0s - loss: 0.0024 - val_loss: 0.0030 - 66ms/epoch - 13ms/step\nEpoch 145/200\n5/5 - 0s - loss: 0.0023 - val_loss: 0.0029 - 67ms/epoch - 13ms/step\nEpoch 146/200\n5/5 - 0s - loss: 0.0022 - val_loss: 0.0027 - 65ms/epoch - 13ms/step\nEpoch 147/200\n5/5 - 0s - loss: 0.0023 - val_loss: 0.0030 - 65ms/epoch - 13ms/step\nEpoch 148/200\n5/5 - 0s - loss: 0.0022 - val_loss: 0.0028 - 64ms/epoch - 13ms/step\nEpoch 149/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0030 - 65ms/epoch - 13ms/step\nEpoch 150/200\n5/5 - 0s - loss: 0.0022 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\nEpoch 151/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\nEpoch 152/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0026 - 83ms/epoch - 17ms/step\nEpoch 153/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0031 - 70ms/epoch - 14ms/step\nEpoch 154/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0026 - 69ms/epoch - 14ms/step\nEpoch 155/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 69ms/epoch - 14ms/step\nEpoch 156/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0028 - 67ms/epoch - 13ms/step\nEpoch 157/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 72ms/epoch - 14ms/step\nEpoch 158/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 68ms/epoch - 14ms/step\nEpoch 159/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0032 - 69ms/epoch - 14ms/step\nEpoch 160/200\n5/5 - 0s - loss: 0.0022 - val_loss: 0.0028 - 69ms/epoch - 14ms/step\nEpoch 161/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 79ms/epoch - 16ms/step\nEpoch 162/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 72ms/epoch - 14ms/step\nEpoch 163/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 73ms/epoch - 15ms/step\nEpoch 164/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 66ms/epoch - 13ms/step\nEpoch 165/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0028 - 67ms/epoch - 13ms/step\nEpoch 166/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 143ms/epoch - 29ms/step\nEpoch 167/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 66ms/epoch - 13ms/step\nEpoch 168/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 68ms/epoch - 14ms/step\nEpoch 169/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0025 - 64ms/epoch - 13ms/step\nEpoch 170/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 65ms/epoch - 13ms/step\nEpoch 171/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 65ms/epoch - 13ms/step\nEpoch 172/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 66ms/epoch - 13ms/step\nEpoch 173/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 67ms/epoch - 13ms/step\nEpoch 174/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 66ms/epoch - 13ms/step\nEpoch 175/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0031 - 66ms/epoch - 13ms/step\nEpoch 176/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0026 - 67ms/epoch - 13ms/step\nEpoch 177/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\nEpoch 178/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0029 - 65ms/epoch - 13ms/step\nEpoch 179/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0029 - 65ms/epoch - 13ms/step\nEpoch 180/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0027 - 67ms/epoch - 13ms/step\nEpoch 181/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 65ms/epoch - 13ms/step\nEpoch 182/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0028 - 66ms/epoch - 13ms/step\nEpoch 183/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 65ms/epoch - 13ms/step\nEpoch 184/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 63ms/epoch - 13ms/step\nEpoch 185/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 65ms/epoch - 13ms/step\nEpoch 186/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 64ms/epoch - 13ms/step\nEpoch 187/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 67ms/epoch - 13ms/step\nEpoch 188/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\nEpoch 189/200\n5/5 - 0s - loss: 0.0021 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\nEpoch 190/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 66ms/epoch - 13ms/step\nEpoch 191/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 66ms/epoch - 13ms/step\nEpoch 192/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0028 - 65ms/epoch - 13ms/step\nEpoch 193/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0027 - 65ms/epoch - 13ms/step\nEpoch 194/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 65ms/epoch - 13ms/step\nEpoch 195/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 65ms/epoch - 13ms/step\nEpoch 196/200\n5/5 - 0s - loss: 0.0019 - val_loss: 0.0028 - 66ms/epoch - 13ms/step\nEpoch 197/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0025 - 67ms/epoch - 13ms/step\nEpoch 198/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0028 - 67ms/epoch - 13ms/step\nEpoch 199/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0029 - 66ms/epoch - 13ms/step\nEpoch 200/200\n5/5 - 0s - loss: 0.0020 - val_loss: 0.0026 - 65ms/epoch - 13ms/step\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/14 [=>............................] - ETA: 5s\n\n\n2023-04-28 21:15:23.064183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:15:23.066102: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:15:23.067472: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n14/14 [==============================] - 0s 2ms/step\n4/4 [==============================] - 0s 3ms/step\n(418, 29, 1) (418,) (418,) (104, 29, 1) (104,) (104,)\n(418,) (418,)\n0.0020613042\n0.0018850414\nTrain MSE = 0.00206 RMSE = 0.04540\nTest MSE = 0.00189 RMSE = 0.04342\n\n\n\n                                                \n\n\n[0.341602087020874, 0.3150536119937897, 0.29821059107780457, 0.2845896780490875, 0.2737989127635956, 0.2642718255519867, 0.2566414177417755, 0.2492697536945343, 0.2420627325773239, 0.23562350869178772, 0.22913095355033875, 0.22318653762340546, 0.2175077497959137, 0.21186426281929016, 0.20632271468639374, 0.2010238766670227, 0.19577078521251678, 0.1906210333108902, 0.1857052594423294, 0.18074887990951538, 0.17595763504505157, 0.1712542474269867, 0.16671222448349, 0.16221798956394196, 0.15778781473636627, 0.1534547358751297, 0.14918656647205353, 0.14524242281913757, 0.14114730060100555, 0.13717453181743622, 0.133418008685112, 0.1295250803232193, 0.12589748203754425, 0.1222970113158226, 0.11891116946935654, 0.1153697520494461, 0.11199573427438736, 0.10874702781438828, 0.1055208221077919, 0.10235138982534409, 0.09925001859664917, 0.09633956104516983, 0.0933820903301239, 0.09049870073795319, 0.0877155065536499, 0.08501607179641724, 0.08229165524244308, 0.0798303410410881, 0.0772019699215889, 0.07486619055271149, 0.07242465019226074, 0.07004571706056595, 0.06778251379728317, 0.0656047984957695, 0.06346863508224487, 0.061435241252183914, 0.059300586581230164, 0.05737832933664322, 0.05538871884346008, 0.05360039696097374, 0.05171176791191101, 0.0500052273273468, 0.048286233097314835, 0.04667573794722557, 0.045047577470541, 0.04350729659199715, 0.04201197624206543, 0.04056708142161369, 0.03918718174099922, 0.03778707608580589, 0.0365750752389431, 0.035180170089006424, 0.03395770117640495, 0.0328434519469738, 0.03163935989141464, 0.030479317530989647, 0.029380880296230316, 0.028340322896838188, 0.027308084070682526, 0.02624821849167347, 0.025306707248091698, 0.02432573400437832, 0.02342129684984684, 0.02253079228103161, 0.021705947816371918, 0.020866800099611282, 0.020030220970511436, 0.01923774555325508, 0.018504410982131958, 0.017802532762289047, 0.01706051267683506, 0.016361216083168983, 0.015787633135914803, 0.015203995630145073, 0.01448579877614975, 0.013886569999158382, 0.013399815186858177, 0.012761558406054974, 0.012253150343894958, 0.01173886563628912, 0.011268947273492813, 0.010815650224685669, 0.01039650198072195, 0.009914286434650421, 0.009682276286184788, 0.009112799540162086, 0.008750717155635357, 0.008395514450967312, 0.008082153275609016, 0.007739873602986336, 0.007415142375975847, 0.0071235476061701775, 0.006818986032158136, 0.006613522302359343, 0.0062727779150009155, 0.0060026575811207294, 0.005818124860525131, 0.005534021183848381, 0.005325778387486935, 0.005172508768737316, 0.004914549179375172, 0.004743007477372885, 0.004528960213065147, 0.00439406419172883, 0.0041946908459067345, 0.004038762301206589, 0.0038845916278660297, 0.003796983975917101, 0.003604442346841097, 0.0034802507143467665, 0.0033469218760728836, 0.0032747716177254915, 0.003124433336779475, 0.0030368242878466845, 0.0030364408157765865, 0.002915782853960991, 0.0027643030043691397, 0.0026926659047603607, 0.002780607435852289, 0.002560487948358059, 0.002513288985937834, 0.0024674569722265005, 0.002398894401267171, 0.002435211557894945, 0.002315204357728362, 0.0022428573574870825, 0.0022706100717186928, 0.002212533261626959, 0.0021431557834148407, 0.002170565305277705, 0.0020865167025476694, 0.0020766390953212976, 0.0020498097874224186, 0.0021111280657351017, 0.0020475273486226797, 0.0020427394192665815, 0.0020367626566439867, 0.0020258117001503706, 0.0020380353089421988, 0.002163392724469304, 0.002030129311606288, 0.002019395586103201, 0.002039798069745302, 0.0020238959696143866, 0.0020315758883953094, 0.00202749646268785, 0.0020192009396851063, 0.0019993840251117945, 0.0020682683680206537, 0.001995167462155223, 0.0020178835839033127, 0.0019831915851682425, 0.0019955954048782587, 0.001974712125957012, 0.002017438178882003, 0.0021187711972743273, 0.002046644687652588, 0.0019675353541970253, 0.0020586710888892412, 0.0020579202100634575, 0.0019729395862668753, 0.001972590107470751, 0.0020115028601139784, 0.0019806106574833393, 0.0019812046084553003, 0.001980000175535679, 0.0019951637368649244, 0.0019599455408751965, 0.0020547108724713326, 0.0020337465684860945, 0.0019668005406856537, 0.001953614642843604, 0.002005363581702113, 0.0019648766610771418, 0.0019577944185584784, 0.001945788855664432, 0.001975767780095339, 0.001981551991775632, 0.0019664191640913486, 0.0020057689398527145]\n\n\n<matplotlib.legend.Legend at 0x1a99438b0>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nModel with no regulation\n\n\nCode\n\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n# model.add(LSTM(\n#model.add(SimpleRNN(\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\n#recurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_4 (GRU)                 (None, 3)                 54        \n                                                                 \n dense_11 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-28 21:16:20.502406: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:16:20.504697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:16:20.506431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\nCode\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\n\nEpoch 1/200\n\n\n2023-04-28 21:16:24.309141: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:16:24.312717: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:16:24.314590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-28 21:16:25.303002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:16:25.304935: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:16:25.306469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n5/5 - 4s - loss: 0.8264 - val_loss: 0.7173 - 4s/epoch - 736ms/step\nEpoch 2/200\n\n\n2023-04-28 21:16:27.345027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:16:27.347402: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:16:27.348766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n5/5 - 0s - loss: 0.7474 - val_loss: 0.6649 - 75ms/epoch - 15ms/step\nEpoch 3/200\n5/5 - 0s - loss: 0.6965 - val_loss: 0.6234 - 97ms/epoch - 19ms/step\nEpoch 4/200\n5/5 - 0s - loss: 0.6547 - val_loss: 0.5839 - 90ms/epoch - 18ms/step\nEpoch 5/200\n5/5 - 0s - loss: 0.6148 - val_loss: 0.5504 - 77ms/epoch - 15ms/step\nEpoch 6/200\n5/5 - 0s - loss: 0.5804 - val_loss: 0.5185 - 84ms/epoch - 17ms/step\nEpoch 7/200\n5/5 - 0s - loss: 0.5476 - val_loss: 0.4904 - 73ms/epoch - 15ms/step\nEpoch 8/200\n5/5 - 0s - loss: 0.5185 - val_loss: 0.4654 - 67ms/epoch - 13ms/step\nEpoch 9/200\n5/5 - 0s - loss: 0.4922 - val_loss: 0.4375 - 73ms/epoch - 15ms/step\nEpoch 10/200\n5/5 - 0s - loss: 0.4636 - val_loss: 0.4135 - 73ms/epoch - 15ms/step\nEpoch 11/200\n5/5 - 0s - loss: 0.4384 - val_loss: 0.3895 - 73ms/epoch - 15ms/step\nEpoch 12/200\n5/5 - 0s - loss: 0.4134 - val_loss: 0.3679 - 72ms/epoch - 14ms/step\nEpoch 13/200\n5/5 - 0s - loss: 0.3907 - val_loss: 0.3462 - 83ms/epoch - 17ms/step\nEpoch 14/200\n5/5 - 0s - loss: 0.3679 - val_loss: 0.3262 - 101ms/epoch - 20ms/step\nEpoch 15/200\n5/5 - 0s - loss: 0.3468 - val_loss: 0.3067 - 66ms/epoch - 13ms/step\nEpoch 16/200\n5/5 - 0s - loss: 0.3262 - val_loss: 0.2855 - 78ms/epoch - 16ms/step\nEpoch 17/200\n5/5 - 0s - loss: 0.3042 - val_loss: 0.2674 - 77ms/epoch - 15ms/step\nEpoch 18/200\n5/5 - 0s - loss: 0.2851 - val_loss: 0.2510 - 81ms/epoch - 16ms/step\nEpoch 19/200\n5/5 - 0s - loss: 0.2676 - val_loss: 0.2346 - 62ms/epoch - 12ms/step\nEpoch 20/200\n5/5 - 0s - loss: 0.2502 - val_loss: 0.2201 - 64ms/epoch - 13ms/step\nEpoch 21/200\n5/5 - 0s - loss: 0.2348 - val_loss: 0.2028 - 63ms/epoch - 13ms/step\nEpoch 22/200\n5/5 - 0s - loss: 0.2167 - val_loss: 0.1872 - 68ms/epoch - 14ms/step\nEpoch 23/200\n5/5 - 0s - loss: 0.2003 - val_loss: 0.1733 - 70ms/epoch - 14ms/step\nEpoch 24/200\n5/5 - 0s - loss: 0.1855 - val_loss: 0.1596 - 91ms/epoch - 18ms/step\nEpoch 25/200\n5/5 - 0s - loss: 0.1709 - val_loss: 0.1454 - 118ms/epoch - 24ms/step\nEpoch 26/200\n5/5 - 0s - loss: 0.1559 - val_loss: 0.1328 - 78ms/epoch - 16ms/step\nEpoch 27/200\n5/5 - 0s - loss: 0.1424 - val_loss: 0.1195 - 69ms/epoch - 14ms/step\nEpoch 28/200\n5/5 - 0s - loss: 0.1284 - val_loss: 0.1083 - 65ms/epoch - 13ms/step\nEpoch 29/200\n5/5 - 0s - loss: 0.1164 - val_loss: 0.0986 - 71ms/epoch - 14ms/step\nEpoch 30/200\n5/5 - 0s - loss: 0.1059 - val_loss: 0.0890 - 70ms/epoch - 14ms/step\nEpoch 31/200\n5/5 - 0s - loss: 0.0956 - val_loss: 0.0791 - 67ms/epoch - 13ms/step\nEpoch 32/200\n5/5 - 0s - loss: 0.0850 - val_loss: 0.0707 - 66ms/epoch - 13ms/step\nEpoch 33/200\n5/5 - 0s - loss: 0.0758 - val_loss: 0.0621 - 74ms/epoch - 15ms/step\nEpoch 34/200\n5/5 - 0s - loss: 0.0667 - val_loss: 0.0560 - 86ms/epoch - 17ms/step\nEpoch 35/200\n5/5 - 0s - loss: 0.0600 - val_loss: 0.0494 - 68ms/epoch - 14ms/step\nEpoch 36/200\n5/5 - 0s - loss: 0.0527 - val_loss: 0.0436 - 65ms/epoch - 13ms/step\nEpoch 37/200\n5/5 - 0s - loss: 0.0463 - val_loss: 0.0372 - 64ms/epoch - 13ms/step\nEpoch 38/200\n5/5 - 0s - loss: 0.0396 - val_loss: 0.0339 - 66ms/epoch - 13ms/step\nEpoch 39/200\n5/5 - 0s - loss: 0.0358 - val_loss: 0.0308 - 67ms/epoch - 13ms/step\nEpoch 40/200\n5/5 - 0s - loss: 0.0321 - val_loss: 0.0291 - 71ms/epoch - 14ms/step\nEpoch 41/200\n5/5 - 0s - loss: 0.0301 - val_loss: 0.0277 - 66ms/epoch - 13ms/step\nEpoch 42/200\n5/5 - 0s - loss: 0.0283 - val_loss: 0.0265 - 65ms/epoch - 13ms/step\nEpoch 43/200\n5/5 - 0s - loss: 0.0266 - val_loss: 0.0259 - 89ms/epoch - 18ms/step\nEpoch 44/200\n5/5 - 0s - loss: 0.0258 - val_loss: 0.0253 - 67ms/epoch - 13ms/step\nEpoch 45/200\n5/5 - 0s - loss: 0.0246 - val_loss: 0.0253 - 65ms/epoch - 13ms/step\nEpoch 46/200\n5/5 - 0s - loss: 0.0241 - val_loss: 0.0252 - 65ms/epoch - 13ms/step\nEpoch 47/200\n5/5 - 0s - loss: 0.0239 - val_loss: 0.0247 - 68ms/epoch - 14ms/step\nEpoch 48/200\n5/5 - 0s - loss: 0.0236 - val_loss: 0.0242 - 65ms/epoch - 13ms/step\nEpoch 49/200\n5/5 - 0s - loss: 0.0233 - val_loss: 0.0239 - 65ms/epoch - 13ms/step\nEpoch 50/200\n5/5 - 0s - loss: 0.0230 - val_loss: 0.0238 - 66ms/epoch - 13ms/step\nEpoch 51/200\n5/5 - 0s - loss: 0.0227 - val_loss: 0.0231 - 65ms/epoch - 13ms/step\nEpoch 52/200\n5/5 - 0s - loss: 0.0225 - val_loss: 0.0230 - 131ms/epoch - 26ms/step\nEpoch 53/200\n5/5 - 0s - loss: 0.0221 - val_loss: 0.0227 - 97ms/epoch - 19ms/step\nEpoch 54/200\n5/5 - 0s - loss: 0.0219 - val_loss: 0.0226 - 80ms/epoch - 16ms/step\nEpoch 55/200\n5/5 - 0s - loss: 0.0215 - val_loss: 0.0226 - 78ms/epoch - 16ms/step\nEpoch 56/200\n5/5 - 0s - loss: 0.0213 - val_loss: 0.0218 - 97ms/epoch - 19ms/step\nEpoch 57/200\n5/5 - 0s - loss: 0.0209 - val_loss: 0.0213 - 97ms/epoch - 19ms/step\nEpoch 58/200\n5/5 - 0s - loss: 0.0207 - val_loss: 0.0215 - 98ms/epoch - 20ms/step\nEpoch 59/200\n5/5 - 0s - loss: 0.0203 - val_loss: 0.0208 - 82ms/epoch - 16ms/step\nEpoch 60/200\n5/5 - 0s - loss: 0.0199 - val_loss: 0.0207 - 64ms/epoch - 13ms/step\nEpoch 61/200\n5/5 - 0s - loss: 0.0196 - val_loss: 0.0209 - 65ms/epoch - 13ms/step\nEpoch 62/200\n5/5 - 0s - loss: 0.0194 - val_loss: 0.0210 - 64ms/epoch - 13ms/step\nEpoch 63/200\n5/5 - 0s - loss: 0.0193 - val_loss: 0.0209 - 65ms/epoch - 13ms/step\nEpoch 64/200\n5/5 - 0s - loss: 0.0192 - val_loss: 0.0197 - 65ms/epoch - 13ms/step\nEpoch 65/200\n5/5 - 0s - loss: 0.0185 - val_loss: 0.0196 - 65ms/epoch - 13ms/step\nEpoch 66/200\n5/5 - 0s - loss: 0.0182 - val_loss: 0.0187 - 64ms/epoch - 13ms/step\nEpoch 67/200\n5/5 - 0s - loss: 0.0178 - val_loss: 0.0191 - 64ms/epoch - 13ms/step\nEpoch 68/200\n5/5 - 0s - loss: 0.0178 - val_loss: 0.0183 - 64ms/epoch - 13ms/step\nEpoch 69/200\n5/5 - 0s - loss: 0.0173 - val_loss: 0.0185 - 65ms/epoch - 13ms/step\nEpoch 70/200\n5/5 - 0s - loss: 0.0173 - val_loss: 0.0182 - 64ms/epoch - 13ms/step\nEpoch 71/200\n5/5 - 0s - loss: 0.0170 - val_loss: 0.0173 - 63ms/epoch - 13ms/step\nEpoch 72/200\n5/5 - 0s - loss: 0.0166 - val_loss: 0.0167 - 66ms/epoch - 13ms/step\nEpoch 73/200\n5/5 - 0s - loss: 0.0164 - val_loss: 0.0165 - 65ms/epoch - 13ms/step\nEpoch 74/200\n5/5 - 0s - loss: 0.0161 - val_loss: 0.0163 - 64ms/epoch - 13ms/step\nEpoch 75/200\n5/5 - 0s - loss: 0.0157 - val_loss: 0.0165 - 64ms/epoch - 13ms/step\nEpoch 76/200\n5/5 - 0s - loss: 0.0155 - val_loss: 0.0168 - 65ms/epoch - 13ms/step\nEpoch 77/200\n5/5 - 0s - loss: 0.0155 - val_loss: 0.0161 - 64ms/epoch - 13ms/step\nEpoch 78/200\n5/5 - 0s - loss: 0.0151 - val_loss: 0.0158 - 65ms/epoch - 13ms/step\nEpoch 79/200\n5/5 - 0s - loss: 0.0147 - val_loss: 0.0157 - 65ms/epoch - 13ms/step\nEpoch 80/200\n5/5 - 0s - loss: 0.0145 - val_loss: 0.0146 - 64ms/epoch - 13ms/step\nEpoch 81/200\n5/5 - 0s - loss: 0.0139 - val_loss: 0.0146 - 65ms/epoch - 13ms/step\nEpoch 82/200\n5/5 - 0s - loss: 0.0138 - val_loss: 0.0148 - 69ms/epoch - 14ms/step\nEpoch 83/200\n5/5 - 0s - loss: 0.0137 - val_loss: 0.0138 - 70ms/epoch - 14ms/step\nEpoch 84/200\n5/5 - 0s - loss: 0.0132 - val_loss: 0.0140 - 72ms/epoch - 14ms/step\nEpoch 85/200\n5/5 - 0s - loss: 0.0130 - val_loss: 0.0130 - 65ms/epoch - 13ms/step\nEpoch 86/200\n5/5 - 0s - loss: 0.0125 - val_loss: 0.0126 - 64ms/epoch - 13ms/step\nEpoch 87/200\n5/5 - 0s - loss: 0.0124 - val_loss: 0.0125 - 65ms/epoch - 13ms/step\nEpoch 88/200\n5/5 - 0s - loss: 0.0122 - val_loss: 0.0121 - 64ms/epoch - 13ms/step\nEpoch 89/200\n5/5 - 0s - loss: 0.0118 - val_loss: 0.0119 - 66ms/epoch - 13ms/step\nEpoch 90/200\n5/5 - 0s - loss: 0.0115 - val_loss: 0.0115 - 66ms/epoch - 13ms/step\nEpoch 91/200\n5/5 - 0s - loss: 0.0112 - val_loss: 0.0112 - 67ms/epoch - 13ms/step\nEpoch 92/200\n5/5 - 0s - loss: 0.0109 - val_loss: 0.0109 - 64ms/epoch - 13ms/step\nEpoch 93/200\n5/5 - 0s - loss: 0.0106 - val_loss: 0.0107 - 66ms/epoch - 13ms/step\nEpoch 94/200\n5/5 - 0s - loss: 0.0102 - val_loss: 0.0107 - 65ms/epoch - 13ms/step\nEpoch 95/200\n5/5 - 0s - loss: 0.0100 - val_loss: 0.0098 - 65ms/epoch - 13ms/step\nEpoch 96/200\n5/5 - 0s - loss: 0.0096 - val_loss: 0.0096 - 69ms/epoch - 14ms/step\nEpoch 97/200\n5/5 - 0s - loss: 0.0093 - val_loss: 0.0094 - 66ms/epoch - 13ms/step\nEpoch 98/200\n5/5 - 0s - loss: 0.0091 - val_loss: 0.0091 - 64ms/epoch - 13ms/step\nEpoch 99/200\n5/5 - 0s - loss: 0.0089 - val_loss: 0.0089 - 65ms/epoch - 13ms/step\nEpoch 100/200\n5/5 - 0s - loss: 0.0085 - val_loss: 0.0088 - 64ms/epoch - 13ms/step\nEpoch 101/200\n5/5 - 0s - loss: 0.0083 - val_loss: 0.0082 - 68ms/epoch - 14ms/step\nEpoch 102/200\n5/5 - 0s - loss: 0.0079 - val_loss: 0.0080 - 67ms/epoch - 13ms/step\nEpoch 103/200\n5/5 - 0s - loss: 0.0078 - val_loss: 0.0078 - 69ms/epoch - 14ms/step\nEpoch 104/200\n5/5 - 0s - loss: 0.0076 - val_loss: 0.0077 - 70ms/epoch - 14ms/step\nEpoch 105/200\n5/5 - 0s - loss: 0.0074 - val_loss: 0.0076 - 67ms/epoch - 13ms/step\nEpoch 106/200\n5/5 - 0s - loss: 0.0072 - val_loss: 0.0073 - 66ms/epoch - 13ms/step\nEpoch 107/200\n5/5 - 0s - loss: 0.0070 - val_loss: 0.0071 - 66ms/epoch - 13ms/step\nEpoch 108/200\n5/5 - 0s - loss: 0.0068 - val_loss: 0.0068 - 140ms/epoch - 28ms/step\nEpoch 109/200\n5/5 - 0s - loss: 0.0065 - val_loss: 0.0069 - 72ms/epoch - 14ms/step\nEpoch 110/200\n5/5 - 0s - loss: 0.0066 - val_loss: 0.0067 - 64ms/epoch - 13ms/step\nEpoch 111/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0066 - 65ms/epoch - 13ms/step\nEpoch 112/200\n5/5 - 0s - loss: 0.0062 - val_loss: 0.0064 - 65ms/epoch - 13ms/step\nEpoch 113/200\n5/5 - 0s - loss: 0.0061 - val_loss: 0.0063 - 66ms/epoch - 13ms/step\nEpoch 114/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0065 - 65ms/epoch - 13ms/step\nEpoch 115/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0061 - 67ms/epoch - 13ms/step\nEpoch 116/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0061 - 67ms/epoch - 13ms/step\nEpoch 117/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0062 - 68ms/epoch - 14ms/step\nEpoch 118/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0063 - 67ms/epoch - 13ms/step\nEpoch 119/200\n5/5 - 0s - loss: 0.0058 - val_loss: 0.0059 - 63ms/epoch - 13ms/step\nEpoch 120/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0059 - 67ms/epoch - 13ms/step\nEpoch 121/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0061 - 66ms/epoch - 13ms/step\nEpoch 122/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0059 - 65ms/epoch - 13ms/step\nEpoch 123/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0063 - 66ms/epoch - 13ms/step\nEpoch 124/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0058 - 63ms/epoch - 13ms/step\nEpoch 125/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0059 - 64ms/epoch - 13ms/step\nEpoch 126/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0060 - 66ms/epoch - 13ms/step\nEpoch 127/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0057 - 67ms/epoch - 13ms/step\nEpoch 128/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0058 - 66ms/epoch - 13ms/step\nEpoch 129/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0056 - 65ms/epoch - 13ms/step\nEpoch 130/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0058 - 66ms/epoch - 13ms/step\nEpoch 131/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0058 - 63ms/epoch - 13ms/step\nEpoch 132/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0055 - 70ms/epoch - 14ms/step\nEpoch 133/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0060 - 67ms/epoch - 13ms/step\nEpoch 134/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0062 - 67ms/epoch - 13ms/step\nEpoch 135/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0060 - 72ms/epoch - 14ms/step\nEpoch 136/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0054 - 69ms/epoch - 14ms/step\nEpoch 137/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0059 - 67ms/epoch - 13ms/step\nEpoch 138/200\n5/5 - 0s - loss: 0.0055 - val_loss: 0.0056 - 67ms/epoch - 13ms/step\nEpoch 139/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0055 - 65ms/epoch - 13ms/step\nEpoch 140/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0054 - 67ms/epoch - 13ms/step\nEpoch 141/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0055 - 67ms/epoch - 13ms/step\nEpoch 142/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0053 - 67ms/epoch - 13ms/step\nEpoch 143/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0056 - 66ms/epoch - 13ms/step\nEpoch 144/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0056 - 67ms/epoch - 13ms/step\nEpoch 145/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0058 - 67ms/epoch - 13ms/step\nEpoch 146/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0058 - 69ms/epoch - 14ms/step\nEpoch 147/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0054 - 66ms/epoch - 13ms/step\nEpoch 148/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0054 - 69ms/epoch - 14ms/step\nEpoch 149/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0055 - 65ms/epoch - 13ms/step\nEpoch 150/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0054 - 69ms/epoch - 14ms/step\nEpoch 151/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0051 - 69ms/epoch - 14ms/step\nEpoch 152/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0051 - 66ms/epoch - 13ms/step\nEpoch 153/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0051 - 66ms/epoch - 13ms/step\nEpoch 154/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0054 - 68ms/epoch - 14ms/step\nEpoch 155/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0051 - 72ms/epoch - 14ms/step\nEpoch 156/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0054 - 67ms/epoch - 13ms/step\nEpoch 157/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0050 - 68ms/epoch - 14ms/step\nEpoch 158/200\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0052 - 67ms/epoch - 13ms/step\nEpoch 159/200\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0055 - 68ms/epoch - 14ms/step\nEpoch 160/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0055 - 67ms/epoch - 13ms/step\nEpoch 161/200\n5/5 - 0s - loss: 0.0047 - val_loss: 0.0052 - 69ms/epoch - 14ms/step\nEpoch 162/200\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0049 - 68ms/epoch - 14ms/step\nEpoch 163/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0051 - 66ms/epoch - 13ms/step\nEpoch 164/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0049 - 68ms/epoch - 14ms/step\nEpoch 165/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0049 - 65ms/epoch - 13ms/step\nEpoch 166/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0055 - 69ms/epoch - 14ms/step\nEpoch 167/200\n5/5 - 0s - loss: 0.0049 - val_loss: 0.0049 - 68ms/epoch - 14ms/step\nEpoch 168/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0048 - 140ms/epoch - 28ms/step\nEpoch 169/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0051 - 78ms/epoch - 16ms/step\nEpoch 170/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0048 - 70ms/epoch - 14ms/step\nEpoch 171/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0051 - 71ms/epoch - 14ms/step\nEpoch 172/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0047 - 69ms/epoch - 14ms/step\nEpoch 173/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0051 - 70ms/epoch - 14ms/step\nEpoch 174/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0051 - 67ms/epoch - 13ms/step\nEpoch 175/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0054 - 93ms/epoch - 19ms/step\nEpoch 176/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0051 - 103ms/epoch - 21ms/step\nEpoch 177/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0047 - 78ms/epoch - 16ms/step\nEpoch 178/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0048 - 91ms/epoch - 18ms/step\nEpoch 179/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0047 - 78ms/epoch - 16ms/step\nEpoch 180/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0047 - 91ms/epoch - 18ms/step\nEpoch 181/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0049 - 77ms/epoch - 15ms/step\nEpoch 182/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0046 - 70ms/epoch - 14ms/step\nEpoch 183/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 70ms/epoch - 14ms/step\nEpoch 184/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 101ms/epoch - 20ms/step\nEpoch 185/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0045 - 71ms/epoch - 14ms/step\nEpoch 186/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0052 - 74ms/epoch - 15ms/step\nEpoch 187/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0050 - 72ms/epoch - 14ms/step\nEpoch 188/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0045 - 69ms/epoch - 14ms/step\nEpoch 189/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 73ms/epoch - 15ms/step\nEpoch 190/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0049 - 68ms/epoch - 14ms/step\nEpoch 191/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 64ms/epoch - 13ms/step\nEpoch 192/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0049 - 66ms/epoch - 13ms/step\nEpoch 193/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0044 - 78ms/epoch - 16ms/step\nEpoch 194/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 73ms/epoch - 15ms/step\nEpoch 195/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0044 - 67ms/epoch - 13ms/step\nEpoch 196/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0051 - 64ms/epoch - 13ms/step\nEpoch 197/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0044 - 65ms/epoch - 13ms/step\nEpoch 198/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0046 - 64ms/epoch - 13ms/step\nEpoch 199/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0043 - 64ms/epoch - 13ms/step\nEpoch 200/200\n5/5 - 0s - loss: 0.0039 - val_loss: 0.0044 - 63ms/epoch - 13ms/step\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n14/14 [==============================] - 0s 2ms/step\n\n\n2023-04-28 21:16:45.774576: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:16:45.776560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:16:45.778101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n4/4 [==============================] - 0s 3ms/step\n(418, 29, 1) (418,) (418,) (104, 29, 1) (104,) (104,)\n(418,) (418,)\n0.0040533487\n0.0047649574\nTrain MSE = 0.00405 RMSE = 0.06367\nTest MSE = 0.00476 RMSE = 0.06903\n\n\n\n                                                \n\n\n[0.8264123797416687, 0.7473537921905518, 0.6964675188064575, 0.6546956300735474, 0.6147596836090088, 0.5803664326667786, 0.5476018786430359, 0.5185011625289917, 0.4922313094139099, 0.46355319023132324, 0.438389390707016, 0.41344699263572693, 0.3906990587711334, 0.36786869168281555, 0.34682467579841614, 0.32622963190078735, 0.30416545271873474, 0.2850935459136963, 0.2676238417625427, 0.25023505091667175, 0.23475347459316254, 0.21673977375030518, 0.2002817690372467, 0.18551956117153168, 0.17090918123722076, 0.15592241287231445, 0.1423967182636261, 0.12844835221767426, 0.11644743382930756, 0.10585606843233109, 0.09557577967643738, 0.08495593816041946, 0.07583308219909668, 0.06673101335763931, 0.06000160053372383, 0.05272778496146202, 0.04633459821343422, 0.0396353118121624, 0.03581034392118454, 0.032126087695360184, 0.030082345008850098, 0.02826639637351036, 0.026631060987710953, 0.025788506492972374, 0.024625645950436592, 0.0241477619856596, 0.023906489834189415, 0.02363796904683113, 0.023340992629528046, 0.023029807955026627, 0.022745538502931595, 0.022515611723065376, 0.022086888551712036, 0.021866362541913986, 0.021539626643061638, 0.021266868337988853, 0.020927036181092262, 0.020685413852334023, 0.020257582888007164, 0.0198933444917202, 0.019598346203565598, 0.019402222707867622, 0.019321927800774574, 0.019185319542884827, 0.018489975482225418, 0.018243584781885147, 0.017792049795389175, 0.017751654610037804, 0.017315125092864037, 0.01725384220480919, 0.01700396090745926, 0.01662103459239006, 0.01635873317718506, 0.016074784100055695, 0.015741601586341858, 0.015511300414800644, 0.01548814494162798, 0.015088015235960484, 0.014717437326908112, 0.014484710060060024, 0.013912294991314411, 0.013750686310231686, 0.01369375642389059, 0.013166711665689945, 0.013015265576541424, 0.012495830655097961, 0.012375591322779655, 0.012249563820660114, 0.011847666464745998, 0.011525923386216164, 0.011211237870156765, 0.010866479948163033, 0.01063199806958437, 0.010171754285693169, 0.009998105466365814, 0.009614219889044762, 0.009341157972812653, 0.009141121059656143, 0.008860575966536999, 0.008482248522341251, 0.008283928036689758, 0.007949603721499443, 0.00778112793341279, 0.007587751839309931, 0.0074206767603755, 0.0072478377260267735, 0.006986833643168211, 0.006819666363298893, 0.006528655998408794, 0.006564987823367119, 0.006312387529760599, 0.006241753231734037, 0.006124440580606461, 0.006029142066836357, 0.006007288116961718, 0.005834977608174086, 0.005779854487627745, 0.0057967305183410645, 0.005831551738083363, 0.005627606995403767, 0.005556488409638405, 0.0056740352883934975, 0.005517027340829372, 0.0056387921795248985, 0.005431257653981447, 0.005389971658587456, 0.005510515067726374, 0.0052810958586633205, 0.005379716865718365, 0.005236443597823381, 0.0052237543277442455, 0.005226000212132931, 0.0051589421927928925, 0.005431542173027992, 0.005628985818475485, 0.005468133371323347, 0.005059248302131891, 0.005460753571242094, 0.0051376293413341045, 0.00499914214015007, 0.004960326012223959, 0.005034377332776785, 0.00490289693698287, 0.005016281269490719, 0.004939297214150429, 0.004972412716597319, 0.005021581891924143, 0.004805450327694416, 0.00486867455765605, 0.004825117532163858, 0.00499397749081254, 0.004724792204797268, 0.00470547704026103, 0.004668277222663164, 0.004744323436170816, 0.004652902018278837, 0.004677037242799997, 0.004617241211235523, 0.004630201030522585, 0.004689253866672516, 0.004747733939439058, 0.0045646256767213345, 0.004521363414824009, 0.004492087755352259, 0.004471680149435997, 0.004506739322096109, 0.004947861656546593, 0.004395178519189358, 0.00439399154856801, 0.0044204541482031345, 0.004361461382359266, 0.004377076867967844, 0.004339203704148531, 0.004374202806502581, 0.004387748893350363, 0.004487558268010616, 0.004357867408543825, 0.004239394795149565, 0.004339923616498709, 0.004247516393661499, 0.004364547319710255, 0.004466005600988865, 0.004168464336544275, 0.004154007416218519, 0.004184297285974026, 0.00422166520729661, 0.004309412091970444, 0.004252600017935038, 0.004084717016667128, 0.004103183746337891, 0.004160366486757994, 0.0040698363445699215, 0.004136969801038504, 0.004053852055221796, 0.003994070924818516, 0.003981219604611397, 0.0041887699626386166, 0.0039596594870090485, 0.004154846537858248, 0.003925359807908535]\n\n\n<matplotlib.legend.Legend at 0x1a95d5a20>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "dl_discharge_copy.html#lstm",
    "href": "dl_discharge_copy.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nBuild model\n\n\nCode\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_2 (LSTM)               (None, 3)                 60        \n                                                                 \n dense_12 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-28 21:17:04.291525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:04.293569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:04.295116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nModel and training parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=200\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCreate model\n\n\nCode\n\n#CREATE MODEL\nmodel = Sequential()\n#COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(SimpleRNN(\n# model.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(1e-1),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_13\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 3)                 60        \n                                                                 \n dense_13 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-28 21:17:25.571230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:25.573533: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:25.575180: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n\n\nTrain model\n\n\nCode\n\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, \nbatch_size=int(f_batch*trainX.shape[0]), \nvalidation_split=validation_split,  # BEING \"SLOPPY WITH CROSS VALIDATION\" HERE FOR TIME-SERIES\nverbose=2)\n\n\nEpoch 1/200\n\n\n2023-04-28 21:17:33.623419: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:33.625970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:33.627764: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-28 21:17:34.509654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:34.512374: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:34.514921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n2023-04-28 21:17:35.942484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:35.944753: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:35.949908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n5/5 - 3s - loss: 0.4771 - val_loss: 0.4350 - 3s/epoch - 573ms/step\nEpoch 2/200\n5/5 - 0s - loss: 0.4334 - val_loss: 0.4043 - 71ms/epoch - 14ms/step\nEpoch 3/200\n5/5 - 0s - loss: 0.4039 - val_loss: 0.3768 - 101ms/epoch - 20ms/step\nEpoch 4/200\n5/5 - 0s - loss: 0.3772 - val_loss: 0.3558 - 104ms/epoch - 21ms/step\nEpoch 5/200\n5/5 - 0s - loss: 0.3561 - val_loss: 0.3365 - 103ms/epoch - 21ms/step\nEpoch 6/200\n5/5 - 0s - loss: 0.3367 - val_loss: 0.3191 - 98ms/epoch - 20ms/step\nEpoch 7/200\n5/5 - 0s - loss: 0.3191 - val_loss: 0.3017 - 72ms/epoch - 14ms/step\nEpoch 8/200\n5/5 - 0s - loss: 0.3017 - val_loss: 0.2867 - 66ms/epoch - 13ms/step\nEpoch 9/200\n5/5 - 0s - loss: 0.2865 - val_loss: 0.2728 - 78ms/epoch - 16ms/step\nEpoch 10/200\n5/5 - 0s - loss: 0.2725 - val_loss: 0.2597 - 61ms/epoch - 12ms/step\nEpoch 11/200\n5/5 - 0s - loss: 0.2593 - val_loss: 0.2478 - 64ms/epoch - 13ms/step\nEpoch 12/200\n5/5 - 0s - loss: 0.2474 - val_loss: 0.2371 - 64ms/epoch - 13ms/step\nEpoch 13/200\n5/5 - 0s - loss: 0.2365 - val_loss: 0.2278 - 61ms/epoch - 12ms/step\nEpoch 14/200\n5/5 - 0s - loss: 0.2271 - val_loss: 0.2193 - 62ms/epoch - 12ms/step\nEpoch 15/200\n5/5 - 0s - loss: 0.2186 - val_loss: 0.2103 - 63ms/epoch - 13ms/step\nEpoch 16/200\n5/5 - 0s - loss: 0.2095 - val_loss: 0.2031 - 67ms/epoch - 13ms/step\nEpoch 17/200\n5/5 - 0s - loss: 0.2022 - val_loss: 0.1959 - 63ms/epoch - 13ms/step\nEpoch 18/200\n5/5 - 0s - loss: 0.1949 - val_loss: 0.1891 - 69ms/epoch - 14ms/step\nEpoch 19/200\n5/5 - 0s - loss: 0.1880 - val_loss: 0.1832 - 69ms/epoch - 14ms/step\nEpoch 20/200\n5/5 - 0s - loss: 0.1821 - val_loss: 0.1773 - 70ms/epoch - 14ms/step\nEpoch 21/200\n5/5 - 0s - loss: 0.1760 - val_loss: 0.1720 - 69ms/epoch - 14ms/step\nEpoch 22/200\n5/5 - 0s - loss: 0.1705 - val_loss: 0.1666 - 63ms/epoch - 13ms/step\nEpoch 23/200\n5/5 - 0s - loss: 0.1652 - val_loss: 0.1614 - 61ms/epoch - 12ms/step\nEpoch 24/200\n5/5 - 0s - loss: 0.1601 - val_loss: 0.1563 - 59ms/epoch - 12ms/step\nEpoch 25/200\n5/5 - 0s - loss: 0.1551 - val_loss: 0.1514 - 60ms/epoch - 12ms/step\nEpoch 26/200\n5/5 - 0s - loss: 0.1503 - val_loss: 0.1466 - 62ms/epoch - 12ms/step\nEpoch 27/200\n5/5 - 0s - loss: 0.1454 - val_loss: 0.1420 - 61ms/epoch - 12ms/step\nEpoch 28/200\n5/5 - 0s - loss: 0.1407 - val_loss: 0.1375 - 59ms/epoch - 12ms/step\nEpoch 29/200\n5/5 - 0s - loss: 0.1362 - val_loss: 0.1330 - 70ms/epoch - 14ms/step\nEpoch 30/200\n5/5 - 0s - loss: 0.1318 - val_loss: 0.1287 - 64ms/epoch - 13ms/step\nEpoch 31/200\n5/5 - 0s - loss: 0.1277 - val_loss: 0.1248 - 65ms/epoch - 13ms/step\nEpoch 32/200\n5/5 - 0s - loss: 0.1239 - val_loss: 0.1209 - 60ms/epoch - 12ms/step\nEpoch 33/200\n5/5 - 0s - loss: 0.1199 - val_loss: 0.1172 - 62ms/epoch - 12ms/step\nEpoch 34/200\n5/5 - 0s - loss: 0.1161 - val_loss: 0.1134 - 61ms/epoch - 12ms/step\nEpoch 35/200\n5/5 - 0s - loss: 0.1125 - val_loss: 0.1099 - 59ms/epoch - 12ms/step\nEpoch 36/200\n5/5 - 0s - loss: 0.1090 - val_loss: 0.1066 - 61ms/epoch - 12ms/step\nEpoch 37/200\n5/5 - 0s - loss: 0.1055 - val_loss: 0.1035 - 65ms/epoch - 13ms/step\nEpoch 38/200\n5/5 - 0s - loss: 0.1023 - val_loss: 0.1000 - 65ms/epoch - 13ms/step\nEpoch 39/200\n5/5 - 0s - loss: 0.0989 - val_loss: 0.0966 - 65ms/epoch - 13ms/step\nEpoch 40/200\n5/5 - 0s - loss: 0.0957 - val_loss: 0.0937 - 74ms/epoch - 15ms/step\nEpoch 41/200\n5/5 - 0s - loss: 0.0927 - val_loss: 0.0906 - 82ms/epoch - 16ms/step\nEpoch 42/200\n5/5 - 0s - loss: 0.0897 - val_loss: 0.0877 - 82ms/epoch - 16ms/step\nEpoch 43/200\n5/5 - 0s - loss: 0.0869 - val_loss: 0.0850 - 61ms/epoch - 12ms/step\nEpoch 44/200\n5/5 - 0s - loss: 0.0842 - val_loss: 0.0823 - 65ms/epoch - 13ms/step\nEpoch 45/200\n5/5 - 0s - loss: 0.0816 - val_loss: 0.0798 - 65ms/epoch - 13ms/step\nEpoch 46/200\n5/5 - 0s - loss: 0.0789 - val_loss: 0.0771 - 63ms/epoch - 13ms/step\nEpoch 47/200\n5/5 - 0s - loss: 0.0764 - val_loss: 0.0747 - 144ms/epoch - 29ms/step\nEpoch 48/200\n5/5 - 0s - loss: 0.0741 - val_loss: 0.0723 - 68ms/epoch - 14ms/step\nEpoch 49/200\n5/5 - 0s - loss: 0.0717 - val_loss: 0.0704 - 68ms/epoch - 14ms/step\nEpoch 50/200\n5/5 - 0s - loss: 0.0695 - val_loss: 0.0679 - 73ms/epoch - 15ms/step\nEpoch 51/200\n5/5 - 0s - loss: 0.0673 - val_loss: 0.0658 - 66ms/epoch - 13ms/step\nEpoch 52/200\n5/5 - 0s - loss: 0.0652 - val_loss: 0.0639 - 62ms/epoch - 12ms/step\nEpoch 53/200\n5/5 - 0s - loss: 0.0632 - val_loss: 0.0618 - 68ms/epoch - 14ms/step\nEpoch 54/200\n5/5 - 0s - loss: 0.0613 - val_loss: 0.0599 - 61ms/epoch - 12ms/step\nEpoch 55/200\n5/5 - 0s - loss: 0.0593 - val_loss: 0.0580 - 61ms/epoch - 12ms/step\nEpoch 56/200\n5/5 - 0s - loss: 0.0575 - val_loss: 0.0562 - 58ms/epoch - 12ms/step\nEpoch 57/200\n5/5 - 0s - loss: 0.0558 - val_loss: 0.0545 - 60ms/epoch - 12ms/step\nEpoch 58/200\n5/5 - 0s - loss: 0.0539 - val_loss: 0.0529 - 59ms/epoch - 12ms/step\nEpoch 59/200\n5/5 - 0s - loss: 0.0523 - val_loss: 0.0513 - 59ms/epoch - 12ms/step\nEpoch 60/200\n5/5 - 0s - loss: 0.0508 - val_loss: 0.0499 - 60ms/epoch - 12ms/step\nEpoch 61/200\n5/5 - 0s - loss: 0.0494 - val_loss: 0.0482 - 62ms/epoch - 12ms/step\nEpoch 62/200\n5/5 - 0s - loss: 0.0476 - val_loss: 0.0467 - 60ms/epoch - 12ms/step\nEpoch 63/200\n5/5 - 0s - loss: 0.0462 - val_loss: 0.0454 - 59ms/epoch - 12ms/step\nEpoch 64/200\n5/5 - 0s - loss: 0.0449 - val_loss: 0.0440 - 58ms/epoch - 12ms/step\nEpoch 65/200\n5/5 - 0s - loss: 0.0435 - val_loss: 0.0427 - 59ms/epoch - 12ms/step\nEpoch 66/200\n5/5 - 0s - loss: 0.0422 - val_loss: 0.0414 - 59ms/epoch - 12ms/step\nEpoch 67/200\n5/5 - 0s - loss: 0.0409 - val_loss: 0.0403 - 60ms/epoch - 12ms/step\nEpoch 68/200\n5/5 - 0s - loss: 0.0398 - val_loss: 0.0390 - 58ms/epoch - 12ms/step\nEpoch 69/200\n5/5 - 0s - loss: 0.0385 - val_loss: 0.0378 - 60ms/epoch - 12ms/step\nEpoch 70/200\n5/5 - 0s - loss: 0.0374 - val_loss: 0.0367 - 64ms/epoch - 13ms/step\nEpoch 71/200\n5/5 - 0s - loss: 0.0362 - val_loss: 0.0355 - 65ms/epoch - 13ms/step\nEpoch 72/200\n5/5 - 0s - loss: 0.0351 - val_loss: 0.0345 - 62ms/epoch - 12ms/step\nEpoch 73/200\n5/5 - 0s - loss: 0.0341 - val_loss: 0.0338 - 61ms/epoch - 12ms/step\nEpoch 74/200\n5/5 - 0s - loss: 0.0332 - val_loss: 0.0325 - 60ms/epoch - 12ms/step\nEpoch 75/200\n5/5 - 0s - loss: 0.0321 - val_loss: 0.0315 - 62ms/epoch - 12ms/step\nEpoch 76/200\n5/5 - 0s - loss: 0.0311 - val_loss: 0.0306 - 59ms/epoch - 12ms/step\nEpoch 77/200\n5/5 - 0s - loss: 0.0302 - val_loss: 0.0299 - 60ms/epoch - 12ms/step\nEpoch 78/200\n5/5 - 0s - loss: 0.0294 - val_loss: 0.0290 - 60ms/epoch - 12ms/step\nEpoch 79/200\n5/5 - 0s - loss: 0.0285 - val_loss: 0.0281 - 74ms/epoch - 15ms/step\nEpoch 80/200\n5/5 - 0s - loss: 0.0277 - val_loss: 0.0271 - 70ms/epoch - 14ms/step\nEpoch 81/200\n5/5 - 0s - loss: 0.0268 - val_loss: 0.0263 - 67ms/epoch - 13ms/step\nEpoch 82/200\n5/5 - 0s - loss: 0.0259 - val_loss: 0.0256 - 60ms/epoch - 12ms/step\nEpoch 83/200\n5/5 - 0s - loss: 0.0253 - val_loss: 0.0247 - 58ms/epoch - 12ms/step\nEpoch 84/200\n5/5 - 0s - loss: 0.0244 - val_loss: 0.0240 - 60ms/epoch - 12ms/step\nEpoch 85/200\n5/5 - 0s - loss: 0.0236 - val_loss: 0.0234 - 60ms/epoch - 12ms/step\nEpoch 86/200\n5/5 - 0s - loss: 0.0230 - val_loss: 0.0226 - 60ms/epoch - 12ms/step\nEpoch 87/200\n5/5 - 0s - loss: 0.0223 - val_loss: 0.0221 - 60ms/epoch - 12ms/step\nEpoch 88/200\n5/5 - 0s - loss: 0.0216 - val_loss: 0.0212 - 58ms/epoch - 12ms/step\nEpoch 89/200\n5/5 - 0s - loss: 0.0209 - val_loss: 0.0206 - 59ms/epoch - 12ms/step\nEpoch 90/200\n5/5 - 0s - loss: 0.0203 - val_loss: 0.0200 - 59ms/epoch - 12ms/step\nEpoch 91/200\n5/5 - 0s - loss: 0.0196 - val_loss: 0.0195 - 58ms/epoch - 12ms/step\nEpoch 92/200\n5/5 - 0s - loss: 0.0191 - val_loss: 0.0188 - 65ms/epoch - 13ms/step\nEpoch 93/200\n5/5 - 0s - loss: 0.0184 - val_loss: 0.0185 - 58ms/epoch - 12ms/step\nEpoch 94/200\n5/5 - 0s - loss: 0.0180 - val_loss: 0.0177 - 58ms/epoch - 12ms/step\nEpoch 95/200\n5/5 - 0s - loss: 0.0174 - val_loss: 0.0171 - 60ms/epoch - 12ms/step\nEpoch 96/200\n5/5 - 0s - loss: 0.0168 - val_loss: 0.0166 - 59ms/epoch - 12ms/step\nEpoch 97/200\n5/5 - 0s - loss: 0.0163 - val_loss: 0.0163 - 58ms/epoch - 12ms/step\nEpoch 98/200\n5/5 - 0s - loss: 0.0159 - val_loss: 0.0161 - 60ms/epoch - 12ms/step\nEpoch 99/200\n5/5 - 0s - loss: 0.0155 - val_loss: 0.0152 - 58ms/epoch - 12ms/step\nEpoch 100/200\n5/5 - 0s - loss: 0.0149 - val_loss: 0.0149 - 57ms/epoch - 11ms/step\nEpoch 101/200\n5/5 - 0s - loss: 0.0145 - val_loss: 0.0144 - 58ms/epoch - 12ms/step\nEpoch 102/200\n5/5 - 0s - loss: 0.0141 - val_loss: 0.0142 - 60ms/epoch - 12ms/step\nEpoch 103/200\n5/5 - 0s - loss: 0.0137 - val_loss: 0.0139 - 61ms/epoch - 12ms/step\nEpoch 104/200\n5/5 - 0s - loss: 0.0134 - val_loss: 0.0136 - 119ms/epoch - 24ms/step\nEpoch 105/200\n5/5 - 0s - loss: 0.0130 - val_loss: 0.0128 - 81ms/epoch - 16ms/step\nEpoch 106/200\n5/5 - 0s - loss: 0.0125 - val_loss: 0.0125 - 63ms/epoch - 13ms/step\nEpoch 107/200\n5/5 - 0s - loss: 0.0122 - val_loss: 0.0121 - 62ms/epoch - 12ms/step\nEpoch 108/200\n5/5 - 0s - loss: 0.0118 - val_loss: 0.0119 - 60ms/epoch - 12ms/step\nEpoch 109/200\n5/5 - 0s - loss: 0.0115 - val_loss: 0.0115 - 60ms/epoch - 12ms/step\nEpoch 110/200\n5/5 - 0s - loss: 0.0112 - val_loss: 0.0113 - 60ms/epoch - 12ms/step\nEpoch 111/200\n5/5 - 0s - loss: 0.0109 - val_loss: 0.0111 - 57ms/epoch - 11ms/step\nEpoch 112/200\n5/5 - 0s - loss: 0.0107 - val_loss: 0.0108 - 58ms/epoch - 12ms/step\nEpoch 113/200\n5/5 - 0s - loss: 0.0103 - val_loss: 0.0104 - 59ms/epoch - 12ms/step\nEpoch 114/200\n5/5 - 0s - loss: 0.0101 - val_loss: 0.0101 - 61ms/epoch - 12ms/step\nEpoch 115/200\n5/5 - 0s - loss: 0.0097 - val_loss: 0.0098 - 59ms/epoch - 12ms/step\nEpoch 116/200\n5/5 - 0s - loss: 0.0095 - val_loss: 0.0099 - 57ms/epoch - 11ms/step\nEpoch 117/200\n5/5 - 0s - loss: 0.0093 - val_loss: 0.0095 - 60ms/epoch - 12ms/step\nEpoch 118/200\n5/5 - 0s - loss: 0.0091 - val_loss: 0.0091 - 59ms/epoch - 12ms/step\nEpoch 119/200\n5/5 - 0s - loss: 0.0087 - val_loss: 0.0091 - 59ms/epoch - 12ms/step\nEpoch 120/200\n5/5 - 0s - loss: 0.0086 - val_loss: 0.0087 - 59ms/epoch - 12ms/step\nEpoch 121/200\n5/5 - 0s - loss: 0.0083 - val_loss: 0.0085 - 60ms/epoch - 12ms/step\nEpoch 122/200\n5/5 - 0s - loss: 0.0082 - val_loss: 0.0083 - 58ms/epoch - 12ms/step\nEpoch 123/200\n5/5 - 0s - loss: 0.0079 - val_loss: 0.0081 - 57ms/epoch - 11ms/step\nEpoch 124/200\n5/5 - 0s - loss: 0.0077 - val_loss: 0.0081 - 59ms/epoch - 12ms/step\nEpoch 125/200\n5/5 - 0s - loss: 0.0077 - val_loss: 0.0077 - 61ms/epoch - 12ms/step\nEpoch 126/200\n5/5 - 0s - loss: 0.0073 - val_loss: 0.0076 - 57ms/epoch - 11ms/step\nEpoch 127/200\n5/5 - 0s - loss: 0.0072 - val_loss: 0.0076 - 61ms/epoch - 12ms/step\nEpoch 128/200\n5/5 - 0s - loss: 0.0071 - val_loss: 0.0074 - 61ms/epoch - 12ms/step\nEpoch 129/200\n5/5 - 0s - loss: 0.0069 - val_loss: 0.0071 - 60ms/epoch - 12ms/step\nEpoch 130/200\n5/5 - 0s - loss: 0.0067 - val_loss: 0.0070 - 60ms/epoch - 12ms/step\nEpoch 131/200\n5/5 - 0s - loss: 0.0066 - val_loss: 0.0068 - 60ms/epoch - 12ms/step\nEpoch 132/200\n5/5 - 0s - loss: 0.0064 - val_loss: 0.0068 - 62ms/epoch - 12ms/step\nEpoch 133/200\n5/5 - 0s - loss: 0.0063 - val_loss: 0.0067 - 60ms/epoch - 12ms/step\nEpoch 134/200\n5/5 - 0s - loss: 0.0062 - val_loss: 0.0063 - 60ms/epoch - 12ms/step\nEpoch 135/200\n5/5 - 0s - loss: 0.0060 - val_loss: 0.0065 - 63ms/epoch - 13ms/step\nEpoch 136/200\n5/5 - 0s - loss: 0.0059 - val_loss: 0.0065 - 62ms/epoch - 12ms/step\nEpoch 137/200\n5/5 - 0s - loss: 0.0059 - val_loss: 0.0063 - 61ms/epoch - 12ms/step\nEpoch 138/200\n5/5 - 0s - loss: 0.0057 - val_loss: 0.0060 - 63ms/epoch - 13ms/step\nEpoch 139/200\n5/5 - 0s - loss: 0.0056 - val_loss: 0.0059 - 62ms/epoch - 12ms/step\nEpoch 140/200\n5/5 - 0s - loss: 0.0054 - val_loss: 0.0058 - 61ms/epoch - 12ms/step\nEpoch 141/200\n5/5 - 0s - loss: 0.0053 - val_loss: 0.0057 - 60ms/epoch - 12ms/step\nEpoch 142/200\n5/5 - 0s - loss: 0.0052 - val_loss: 0.0056 - 62ms/epoch - 12ms/step\nEpoch 143/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0055 - 60ms/epoch - 12ms/step\nEpoch 144/200\n5/5 - 0s - loss: 0.0051 - val_loss: 0.0055 - 60ms/epoch - 12ms/step\nEpoch 145/200\n5/5 - 0s - loss: 0.0050 - val_loss: 0.0053 - 60ms/epoch - 12ms/step\nEpoch 146/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0051 - 60ms/epoch - 12ms/step\nEpoch 147/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0054 - 62ms/epoch - 12ms/step\nEpoch 148/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0054 - 61ms/epoch - 12ms/step\nEpoch 149/200\n5/5 - 0s - loss: 0.0048 - val_loss: 0.0050 - 62ms/epoch - 12ms/step\nEpoch 150/200\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0050 - 61ms/epoch - 12ms/step\nEpoch 151/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0048 - 61ms/epoch - 12ms/step\nEpoch 152/200\n5/5 - 0s - loss: 0.0045 - val_loss: 0.0048 - 58ms/epoch - 12ms/step\nEpoch 153/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0047 - 59ms/epoch - 12ms/step\nEpoch 154/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0051 - 60ms/epoch - 12ms/step\nEpoch 155/200\n5/5 - 0s - loss: 0.0046 - val_loss: 0.0048 - 61ms/epoch - 12ms/step\nEpoch 156/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0046 - 61ms/epoch - 12ms/step\nEpoch 157/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0048 - 59ms/epoch - 12ms/step\nEpoch 158/200\n5/5 - 0s - loss: 0.0044 - val_loss: 0.0046 - 59ms/epoch - 12ms/step\nEpoch 159/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 61ms/epoch - 12ms/step\nEpoch 160/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 100ms/epoch - 20ms/step\nEpoch 161/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 82ms/epoch - 16ms/step\nEpoch 162/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0047 - 62ms/epoch - 12ms/step\nEpoch 163/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 62ms/epoch - 12ms/step\nEpoch 164/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0047 - 63ms/epoch - 13ms/step\nEpoch 165/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0047 - 60ms/epoch - 12ms/step\nEpoch 166/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0047 - 61ms/epoch - 12ms/step\nEpoch 167/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0045 - 62ms/epoch - 12ms/step\nEpoch 168/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0048 - 60ms/epoch - 12ms/step\nEpoch 169/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0050 - 66ms/epoch - 13ms/step\nEpoch 170/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0045 - 59ms/epoch - 12ms/step\nEpoch 171/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 63ms/epoch - 13ms/step\nEpoch 172/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 61ms/epoch - 12ms/step\nEpoch 173/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0047 - 62ms/epoch - 12ms/step\nEpoch 174/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 60ms/epoch - 12ms/step\nEpoch 175/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 61ms/epoch - 12ms/step\nEpoch 176/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0051 - 61ms/epoch - 12ms/step\nEpoch 177/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 61ms/epoch - 12ms/step\nEpoch 178/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0045 - 62ms/epoch - 12ms/step\nEpoch 179/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 62ms/epoch - 12ms/step\nEpoch 180/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0047 - 61ms/epoch - 12ms/step\nEpoch 181/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 61ms/epoch - 12ms/step\nEpoch 182/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0045 - 60ms/epoch - 12ms/step\nEpoch 183/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0046 - 63ms/epoch - 13ms/step\nEpoch 184/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 59ms/epoch - 12ms/step\nEpoch 185/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 63ms/epoch - 13ms/step\nEpoch 186/200\n5/5 - 0s - loss: 0.0042 - val_loss: 0.0045 - 60ms/epoch - 12ms/step\nEpoch 187/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 59ms/epoch - 12ms/step\nEpoch 188/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0047 - 59ms/epoch - 12ms/step\nEpoch 189/200\n5/5 - 0s - loss: 0.0043 - val_loss: 0.0045 - 60ms/epoch - 12ms/step\nEpoch 190/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0045 - 62ms/epoch - 12ms/step\nEpoch 191/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0046 - 60ms/epoch - 12ms/step\nEpoch 192/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 59ms/epoch - 12ms/step\nEpoch 193/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0047 - 60ms/epoch - 12ms/step\nEpoch 194/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 64ms/epoch - 13ms/step\nEpoch 195/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 62ms/epoch - 12ms/step\nEpoch 196/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 60ms/epoch - 12ms/step\nEpoch 197/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0048 - 60ms/epoch - 12ms/step\nEpoch 198/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0046 - 62ms/epoch - 12ms/step\nEpoch 199/200\n5/5 - 0s - loss: 0.0040 - val_loss: 0.0046 - 59ms/epoch - 12ms/step\nEpoch 200/200\n5/5 - 0s - loss: 0.0041 - val_loss: 0.0045 - 58ms/epoch - 12ms/step\n\n\n\n\nVisualize fitting history\n\n\nCode\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n\n#COMPUTE RMSE\nprint(trainY.shape, train_predict.shape)\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\nprint(np.mean((trainY-train_predict)**2.0))\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))    \n\n# PLOTLY PLOT\nplotly_line_plot([epochs_steps,epochs_steps],[history.history['loss'],history.history['val_loss']],title=\"Sunspots per month since 1749-01\",x_label=\"training epochs\",y_label=\"loss (MSE)\")\n\nprint(history.history['loss'])\n# SIMPLE PLOT \nplt.figure()\nplt.plot(epochs_steps, history.history['loss'], 'bo', label='Training loss')\nplt.plot(epochs_steps, history.history['val_loss'], 'b', label='Validation loss')\nplt.legend()\n\n\n 1/14 [=>............................] - ETA: 5s\n\n\n2023-04-28 21:17:52.175979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n     [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n2023-04-28 21:17:52.177722: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n     [[{{node gradients/split_grad/concat/split/split_dim}}]]\n2023-04-28 21:17:52.179108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n     [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n\n\n14/14 [==============================] - 0s 3ms/step\n4/4 [==============================] - 0s 3ms/step\n(418, 29, 1) (418,) (418,) (104, 29, 1) (104,) (104,)\n(418,) (418,)\n0.004183721\n0.0047373553\nTrain MSE = 0.00418 RMSE = 0.06468\nTest MSE = 0.00474 RMSE = 0.06883\n\n\n\n                                                \n\n\n[0.47707828879356384, 0.43339022994041443, 0.4038979113101959, 0.37722280621528625, 0.3561413586139679, 0.33670005202293396, 0.3190544843673706, 0.30165591835975647, 0.2865275740623474, 0.27250680327415466, 0.2593110501766205, 0.24736914038658142, 0.23654693365097046, 0.22713933885097504, 0.21855060756206512, 0.20953640341758728, 0.20224346220493317, 0.19492486119270325, 0.18802693486213684, 0.18208780884742737, 0.17601479589939117, 0.17054474353790283, 0.16524788737297058, 0.16014008224010468, 0.1551070362329483, 0.15034565329551697, 0.14541414380073547, 0.14074817299842834, 0.1362224668264389, 0.13181577622890472, 0.12770943343639374, 0.12388191372156143, 0.11994922906160355, 0.11608196049928665, 0.11245965957641602, 0.10897630453109741, 0.10550908744335175, 0.10231121629476547, 0.09888777136802673, 0.09571108222007751, 0.0927400216460228, 0.08974623680114746, 0.08694097399711609, 0.08415905386209488, 0.08158525824546814, 0.07889511436223984, 0.07640904933214188, 0.0740690603852272, 0.07169277220964432, 0.06951157748699188, 0.0673222616314888, 0.0651693269610405, 0.06315726786851883, 0.06128856912255287, 0.059302918612957, 0.05750473961234093, 0.05577145144343376, 0.05390476807951927, 0.052330467849969864, 0.050796035677194595, 0.049353621900081635, 0.047647200524806976, 0.046237725764513016, 0.04490064084529877, 0.04352109134197235, 0.04219312220811844, 0.04093460366129875, 0.039773210883140564, 0.03854208067059517, 0.037363868206739426, 0.0362069346010685, 0.035119663923978806, 0.03410874679684639, 0.03317806497216225, 0.03208936005830765, 0.03114534169435501, 0.030199427157640457, 0.029406538233160973, 0.028513923287391663, 0.027719970792531967, 0.026772184297442436, 0.025928286835551262, 0.025283340364694595, 0.02436806820333004, 0.023613858968019485, 0.0229518860578537, 0.02225423790514469, 0.021627970039844513, 0.02087664231657982, 0.02025170437991619, 0.019621964544057846, 0.019104603677988052, 0.01842612959444523, 0.017992591485381126, 0.01736980862915516, 0.01683294214308262, 0.016283707693219185, 0.0158852469176054, 0.015516385436058044, 0.014937502332031727, 0.014524899423122406, 0.01405747327953577, 0.013729599304497242, 0.013401182368397713, 0.013041311874985695, 0.012514223344624043, 0.012153706513345242, 0.011800681240856647, 0.011548630893230438, 0.011198760010302067, 0.010877415537834167, 0.010650956071913242, 0.010337860323488712, 0.010055430233478546, 0.009724182076752186, 0.009465524926781654, 0.009301611222326756, 0.009102992713451385, 0.008726612664759159, 0.008649925701320171, 0.008299916051328182, 0.008168957196176052, 0.007911705411970615, 0.0077062551863491535, 0.0076733301393687725, 0.007344536483287811, 0.007169098127633333, 0.007114797364920378, 0.006870925892144442, 0.006714366842061281, 0.0065574925392866135, 0.006410124246031046, 0.006331034004688263, 0.006240360904484987, 0.00598220806568861, 0.005930766928941011, 0.005853984039276838, 0.005680092144757509, 0.005552077665925026, 0.005414981860667467, 0.005314849317073822, 0.005220714490860701, 0.005097897257655859, 0.005065748002380133, 0.004963231272995472, 0.004831227939575911, 0.00481724226847291, 0.00476105185225606, 0.004755574278533459, 0.004559893161058426, 0.004522360395640135, 0.0044500636868178844, 0.00442884024232626, 0.004359432961791754, 0.004590090364217758, 0.0043037510477006435, 0.004253766965121031, 0.004428091924637556, 0.004213002044707537, 0.004191297106444836, 0.004152263049036264, 0.004204768221825361, 0.0041533359326422215, 0.004217435140162706, 0.004148735199123621, 0.004273321945220232, 0.004295204766094685, 0.004098236095160246, 0.004236858803778887, 0.004275082610547543, 0.0041121975518763065, 0.004091714043170214, 0.00410504313185811, 0.004126902669668198, 0.004083028994500637, 0.004085531458258629, 0.004230589605867863, 0.004225912503898144, 0.004115298856049776, 0.004135805182158947, 0.004113730043172836, 0.004159292206168175, 0.004161519929766655, 0.004108143970370293, 0.004084911663085222, 0.004218204878270626, 0.004050259944051504, 0.004053658805787563, 0.004273586440831423, 0.004048397298902273, 0.00404812628403306, 0.0040818569250404835, 0.004058740567415953, 0.004094185773283243, 0.004051898140460253, 0.004052458330988884, 0.00407516909763217, 0.00409622211009264, 0.004035555757582188, 0.004064596723765135]\n\n\n<matplotlib.legend.Legend at 0x1ab082140>\n\n\n\n\n\n\n\nVisualize parity plot\n\n\nCode\n# GET DATA\n# GENERATE PLOTLY FIGURE\n\nfig = px.scatter(x=trainY,y=train_predict,height=600,width=800)\nfig.add_scatter(x=testY,y=test_predict,mode=\"markers\")\nfig.add_scatter(x=trainY,y=trainY, mode='lines')\n\nfig.update_layout(\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "dl_discharge.html#conclusion",
    "href": "dl_discharge.html#conclusion",
    "title": "Deep Learning for TS",
    "section": "Conclusion",
    "text": "Conclusion\n\nComparison among ANNs\nThen analyze and discuss the following points. How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power\n\n\nTable 1: Comparison among RNN, GRU, and LSTM\n\n\n\n\n\n\n\n\n\n\nTrain MSE\nTrain RMSE\nTest MSE\nTest RMSE\n\n\n\n\nRNN with L2 regularization\n0.00175\n0.04179\n0.00157\n0.03967\n\n\nRNN with no regularization\n0.00319\n0.05651\n0.00318\n0.05642\n\n\nGRU with L2 regularization\n0.00216\n0.04647\n0.00205\n0.04527\n\n\nGRU with no regularization\n0.00393\n0.06267\n0.00441\n0.06637\n\n\nLSTM with L2 regularization\n0.00527\n0.07257\n0.00613\n0.07831\n\n\n\n\nFrom Table 1, we can see the model with the lowest training MSE, RMSE and testing MSE, RMSE is RNN with L2 regularization. The models with L2 regularization perform better than the model without any regularization. The parity plots also confirm this. We can see that the testing errors are higher than the training error, but still can predict decent discharge. Thus, the deep learning model can accurately predict the future as long as the historical data are good represent of the majority of the time series data patterns. \n\n\nComparison among ANNs and ARMA/ARIMA\nAs to compare with time-series ARMA/ARIMA models from HW-3, it really depends on our criteria. The deep learning models took more time to training compares to the traditional time series model such as ARMA/ARIMA due to more parameters in the deep learning models. However, deep learning model parameters setting are more like a black box. To tuning the parameters, cross-validation method is extensively used. ARMA/ARIMA model used ACF and PACF plots to obtain the distribution of the data, which make the models’ interpretability better than the deep leaning models.\nFrom Table 2, we can see that the RMSEs of ANN models are significantly lower than the traditional time series models. The traditional time series model with lowest RMSE is the SARIMA(2,1,1,0,1,1) model. The RMSE is around 0.7, while the ANN model with lowest RMSE is RNN with L2 regularization with RMSE of 0.04.\nWhen choosing traditional time series models, we used AIC, while for the deep learning models, we use MSE/RMSE to chose the best model. This may also slightly affect that in terms of RMSE, traditional time series models perform not good. a In terms of MSE/RMSE, deep learning modes perform better than the ARMA/ARIMA models. We can also draw this conclusion from the prediction in ARIMAX/SARIMAX/VAR Models, ARIMAX/SARIMAX/VAR Models and the parity plots above.\n\n\nTable 2: Comparison of RMSE among ARIMA, SARIMA, VAR(3), RNN, GRU, and LSTM.\n\n\n\nTrain RMSE\nTest RMSE\n\n\n\n\nARIMA(5,1,1)\n0.8049785\nN/A\n\n\nSARIMA(2,1,1,0,1,1)\n0.7150316\nN/A\n\n\nVAR(3)\n0.8009\nN/A\n\n\nRNN with L2 regularization\n0.04179\n0.03967\n\n\nGRU with L2 regularization\n0.04647\n0.04527\n\n\nLSTM with L2 regularization\n0.07257\n0.07831"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Water and Economy Xiaojing Ni",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, D.C. Master of Science in Data Science and Analytics | Aug 2021 - Dec 2023\nMississippi State University | Mississippi State, MS PhD in Biological Engineering | Aug 2014 - Aug 2018"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Water and Economy Xiaojing Ni",
    "section": "Experience",
    "text": "Experience\nCSET Georgetown University | Research Assistant | Jan 2022 - Now\nU.S.EPA | Postdoctoral Research Fellow | Oct 2018 - Oct 2019"
  },
  {
    "objectID": "intro.html#initial-questions",
    "href": "intro.html#initial-questions",
    "title": "Water quality time series analysis — a time series case study of Toledo Ohio, U.S.",
    "section": "Initial questions",
    "text": "Initial questions\nInitially, I want to answer following questions:\n\nWhat are the stream water quantity and quality distribution, including streamflow, sediment, phosphorus, near the outlet from Toledo, Ohio, U.S. to Lake Erie?\nHave the stream water quantity and quality changed overtime?\nIf the stream water quantity and quality changed overtime, how? Any particular events?\nHow about the economy, representing by population, house price, agriculture and recreation output distributions, in Toledo Ohio, US overtime?\nHave the economy indicators have similar or various trends?\nHow and why their trends similar or different?\nAny similarity or difference among water quality indicators and economy indicators?\nHow and why of the question 7?\nCan we predict water quality with time series model with factors including climate and economic features?\nCan we do the other way around of question 9?\n\nWith more and more research on this topic, I found that due to the different periodicity of economy and water quality evaluation, i.e. economy normally is reported quarterly, while water quality index are measured monthly, it is hard to incorporate these two time series data, especially the water quality data are usually limited to less than 10 years. My focus has shifted to water quality time series analysis, with a stand along analysis on the “Financial time series model”."
  },
  {
    "objectID": "intro.html#objective",
    "href": "intro.html#objective",
    "title": "Water quality time series analysis — a time series case study of Toledo Ohio, U.S.",
    "section": "Objective",
    "text": "Objective\nThe main objective of this study is to investigate the water quality index, such as stream flow discharge, turbidity, and sediment, as time series data. More specifically shown in Figure 3, various time series models were used to simulate stream flow discharge, i.e. ARMA, ARIMA, SARIMA, VAR, turbidity, i.e. SARIMAX, and sediment, i.e. VAR. In addition, the climate data, such as precipitation and temperature were also used to predict the water quality index. In the “Financial time series model” session, GARCH model are used to analyze water treatment company, Xylem, stock prices.\n\n\n\nFigure 3: Big picture"
  },
  {
    "objectID": "intro.html#initial-questions-and-incentives",
    "href": "intro.html#initial-questions-and-incentives",
    "title": "Water quality time series analysis — a time series case study of Toledo Ohio, U.S.",
    "section": "Initial questions and incentives",
    "text": "Initial questions and incentives\nInitially, I want to answer following questions:\n\nWhat are the stream water quantity and quality distribution, including streamflow, sediment, phosphorus, near the outlet from Toledo, Ohio, U.S. to Lake Erie?\nHave the stream water quantity and quality changed overtime?\nIf the stream water quantity and quality changed overtime, how? Any particular events?\nHow about the economy, representing by population, house price, agriculture and recreation output distributions, in Toledo Ohio, US overtime?\nHave the economy indicators have similar or various trends?\nHow and why their trends similar or different?\nAny similarity or difference among water quality indicators and economy indicators?\nHow and why of the question 7?\nCan we predict water quality with time series model with factors including climate and economic features?\nCan we do the other way around of question 9?\n\nWith more and more research on this topic, I found that due to the different periodicity of economy and water quality evaluation, i.e. economy normally is reported quarterly, while water quality index are measured monthly, it is hard to incorporate these two time series data, especially the water quality data are usually limited to less than 10 years. My focus has shifted to water quality time series analysis, with a stand along analysis on the “Financial time series model”."
  },
  {
    "objectID": "data_sources.html#stock-price",
    "href": "data_sources.html#stock-price",
    "title": "Data Sources",
    "section": "Stock price",
    "text": "Stock price\nThe stock price is from Yahoo finance using getSymbols() function of quantmod package in R. The data includes open price, highest price, lowest price, closing price, volume, and adjusted closing price."
  },
  {
    "objectID": "data_viz.html#water-quality",
    "href": "data_viz.html#water-quality",
    "title": "Data Visualization",
    "section": "Water quality",
    "text": "Water quality\n\nStream discharge\n\n\nCode\n## import and processing data\ndischarge <- read.csv(\"./data/discharge.csv\")\ndischarge <- discharge[discharge$X.!= \"#\",]\ndischarge <- discharge[3:dim(discharge)[1],]\ncolnames(discharge) <- c(\"prefix\", \"station\",\"date\",\"discharge\",\"flag\")\ndischarge = discharge[c(\"prefix\", \"station\",\"date\",\"discharge\",\"flag\")]\ndischarge$date<-as.Date(discharge$date,\"%m/%d/%y\")\ndischarge$date <- as.Date(ifelse(discharge$date > Sys.Date(), \n  format(discharge$date, \"19%y-%m-%d\"), \n  format(discharge$date)))\n# head(discharge)\n# dim(discharge)\n# str(discharge)\n\n# select data after 1970\ndischarge = discharge[discharge$date >= \"1970-01-01\", ]\n# head(discharge)\n# dim(discharge)\n# str(discharge)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\ndischarge$discharge <- na_ma(discharge$discharge, k = 4, weighting = \"exponential\")\n\n## sanity check\n# str(discharge)\n# sum(is.na(discharge))\n\n\n\n\nCode\np <- plot_ly(discharge, type = 'scatter', mode = 'lines')%>%\n  add_trace(x = ~date, y = ~discharge, name=\"discharge\")%>%\n  layout(title='Stream discharge at Waterville, OH station',\n         xaxis = list(rangeslider = list(visible = T),\n                      rangeselector=list(\n                        buttons=list(\n                          list(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n                          list(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                          list(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n                          list(count=16, label=\"YTD\", step=\"year\", stepmode=\"todate\")\n                          )\n                        )))\n\np <- p %>%\n  layout(\n    xaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff',\n                 title = \"Date\"),\n    yaxis = list(zerolinecolor = '#ffff',\n                 zerolinewidth = 2,\n                 gridcolor = 'ffff',\n                 title = \"Discharge, ft<sup>3</sup>/s\"),\n    plot_bgcolor='#e5ecf6', margin = 0.2, width = 900)\np\n\n\n\n\n\nFigure 3: Stream discharge at Waterville, OH station\n\n\n\nStream flow has clear periodicity according to Figure 3. There are some high value year, for example 2015. This is caused by the 2014–2016 El Niño event.\n\n\nTurbidity\n\n\nCode\n## read data\nturb <- read.csv(\"./data/turbidity.csv\", header = F, comment.char = \"#\")\nturb <- turb[3:nrow(turb),1:9]\ncolnames(turb) <- c(\"prefix\", \"station\",\"date\",\"max\",\"flag1\",\"min\",\"flag2\",\"mean\",\"flag3\")\nturb$date<-as.Date(turb$date,\"%m/%d/%y\")\n\n## extract daily max\nturb_max<- turb[c(\"date\",\"max\")]\nturb_max$date<-as.Date(turb_max$date,\"%m/%d/%y\")\nturb_max$max = as.numeric(turb_max$max) \n## interpolate the missing data using moving average\nturb_max$max <- na_ma(turb_max$max, k = 4, weighting = \"exponential\")\n\n# min(turb_max$date);max(turb_max$date)\n\n## extract to 12/31/2022 as the climate max date\nturb_max = turb_max[turb_max$date <= \"2022-12-31\", ]\n# head(turb_max)\n# min(turb_max$date);max(turb_max$date)\n## sanity check\n# str(turb_max)\n# sum(is.na(turb_max))\n\nturb_df <- data.frame(turb_max$date,turb_max$max)\ncolnames(turb_df) <- c(\"Date\",\"max\")\n\n\n\n## prepare data for candlestick plot\nprcp_summary <- turb_df %>%                               \n  group_by(Date) %>% \n  summarize(min = min(max),\n            q1 = quantile(max, 0.25),\n            median = median(max),\n            mean = mean(max),\n            q3 = quantile(max, 0.75),\n            max = max(max))\n\np2 <- plot_ly(prcp_summary,\n              x = ~Date,\n              y = ~mean,\n              type = 'scatter', mode = 'lines')\np2 <- p2 %>% layout(title = \"Maximum turbidity in studied stream\",\n      xaxis = list(rangeslider = list(visible = T)),\n       yaxis = list(title = 'Turbidity, NTU'))\np2\n\n\n\n\n\nFigure 4: Maximum turbidity at studied stream"
  },
  {
    "objectID": "data_viz.html#xylem-stock",
    "href": "data_viz.html#xylem-stock",
    "title": "Data Visualization",
    "section": "Xylem stock",
    "text": "Xylem stock\nFigure 5 shows the plot of Xylem stock prices from 2011 to 2023. We can see the price has increased more than triple since 2014.\n\n\nCode\n## read data since 2011\nxyl <- getSymbols(\"XYL\",auto.assign = FALSE, from = \"2011-10-14\",src=\"yahoo\")\nxyl$date<-as.Date(xyl$date,\"%Y-%m-%d\")\n\nchartSeries(xyl, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color)\n\n\nxyl.close<- Ad(xyl)\n\n\n\n\n\nFigure 5: Xylem Inc. stock price 2011-2023"
  },
  {
    "objectID": "data_viz.html#sediment",
    "href": "data_viz.html#sediment",
    "title": "Data Visualization",
    "section": "Sediment",
    "text": "Sediment\n\n\nCode\n## import and processing data\nsed <- read.csv(\"./data/sediment.csv\", header = F, comment.char = \"#\")\nsed <- sed[3:nrow(sed),3:4]\ncolnames(sed) <- c(\"date\",\"sed\")\nsed$date<-as.Date(sed$date,\"%m/%d/%y\")\nsed$date <- as.Date(ifelse(sed$date > Sys.Date(), \n  format(sed$date, \"19%y-%m-%d\"), \n  format(sed$date)))\n\nsed$sed <- as.numeric(sed$sed)\n\n## interpolate the missing data using moving average\nlibrary(imputeTS)\nsed$sed <- na_ma(sed$sed, k = 4, weighting = \"exponential\")\n# min(sed$date);max(sed$date)\n\n## extract from 1983-9-29 to 2003-09-29, 20-years of data\nsed = sed[sed$date >= \"1983-9-29\", ]\n\nsed_df <- data.frame(sed$date,sed$sed)\ncolnames(sed_df) <- c(\"Date\",\"sediment\")\n\n## prepare data for candlestick plot\nprcp_summary <- sed_df %>%                               \n  group_by(Date) %>% \n  summarize(min = min(sediment),\n            q1 = quantile(sediment, 0.25),\n            median = median(sediment),\n            mean = mean(sediment),\n            q3 = quantile(sediment),\n            max = max(sediment))\n\np2 <- plot_ly(prcp_summary,\n              x = ~Date,\n              y = ~mean,\n              type = 'scatter', mode = 'lines')\np2 <- p2 %>% layout(title = \"Average sediment in studied stream\",\n      xaxis = list(rangeslider = list(visible = T)),\n       yaxis = list(title = 'Sediment'))\np2"
  },
  {
    "objectID": "conclusions.html#stream-discharge-simulation",
    "href": "conclusions.html#stream-discharge-simulation",
    "title": "Conclusions",
    "section": "Stream discharge simulation",
    "text": "Stream discharge simulation\n\nThis study compares six models (see comparison) predicting stream flow discharge. All of them have ability to simulate stream flow discharge. The deep learning models perform much better than the traditional time series models. However, the interpretability of the traditional time series models is one of the advantages of those models. For example, the SARIMA model have better performance compares to ARIMA and VAR models, which indicates seasonality of stream flow discharge is an important factor for prediction. The importance of stream flow seasonality was also discussed in other literature (Rango, Salomonson, and Foster (1977),Dettinger and Diaz (2000), Siqueira et al. (2014)). Due to the time and data limitation of this project, only climate factors were considered in the VAR model. To improve the model prediction ability, more data could be included in the VAR model, such as ground water factors (Pechlivanidis et al. (2020)) and land cover type (Harr, Fredriksen, and Rothacher (1979), Pechlivanidis et al. (2020)). Those factors affect the stream flow discharge by altering the behaviors of water after falling off as precipitation."
  },
  {
    "objectID": "conclusions.html#turbidity-and-sediment-simulation",
    "href": "conclusions.html#turbidity-and-sediment-simulation",
    "title": "Conclusions",
    "section": "Turbidity and sediment simulation",
    "text": "Turbidity and sediment simulation\nIn this study, simulating the change of turbidity and sediment in stream water were also performed. Theoretically, Due to the water cycle effect, turbidity and sediment is an indication of soil erosion (De Vente et al. (2013)) and can cause later higher stream discharge. However, this phenomenon wasn’t detected by traditional time series model, such as VAR model.\nThe same happens to the stream flow discharge and precipitation. The VAR models show that the long term correlation caused by water cycle is not significant or cannot be detected. The precipitation is not affected by discharge or sediment. The discharge is not affected by sediment. The strong relationships are still one way. Thus, SARIMAX model may be more suitable for these variables with sediment as dependent variables, and precipitation and discharge plus temperature as the independent variables.\n source: https://www.noaa.gov/education/resource-collections/freshwater/water-cycle\nThe reason that VAR model didn’t detect the water cycle impact may caused by the complexity of the water cycle. By including other factor, such as groundwater flow, may improve the results. Unfortunately, the data of groundwater are usually limited. As an alternative, the process-based model simulating the behavior of water cycle may be more suitable for the analysis."
  },
  {
    "objectID": "conclusions.html#water-treatment-company-stock-price",
    "href": "conclusions.html#water-treatment-company-stock-price",
    "title": "Conclusions",
    "section": "Water treatment company stock price",
    "text": "Water treatment company stock price\nDue to the climate change and human activities, extreme climate and environment events, such as the water crisis caused by eutrophication in City of Toledo, OH, occur more frequently (Climate change indicators), and may be regularly. This phenomenon brings opportunities for environmental companies such as water treatment firm. Xylem Inc. is a large American water technology provider, in public utility, residential, commercial, agricultural and industrial settings. The company was founded in 2011 and does business in more than 150 countries (Wiki). In this study, the stock of Xylem was modeled using financial time series models, such as ARMA+GARCH to forecast the volatility of future returns.The company thrives after 2014, which can be caused by both the economy thriving and thriving of the environment industry. More and more policies focus on regulating water pollution."
  }
]